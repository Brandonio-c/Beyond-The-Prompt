TY  - JOUR
TI  - Combining prompt learning with contextual semantics for inductive relation prediction
AU  - Xie, S. R.
AU  - Pan, Q. F.
AU  - Wang, X. Z.
AU  - Luo, X. F.
AU  - Sugumaran, V.
PY  - 2024
PD  - 
N2  - Inductive relation prediction for knowledge graphs aims to predict missing relations between two new entities. Most previous studies on relation prediction are limited to the transductive setting and could not be applied to it. Recently, some inductive methods have been proposed to handle it by learning the topological semantics. However, they solely rely on structural information, disregarding the role of prior knowledge. In cases of sparse structures, this limitation is magnified, thereby hindering the inductive ability. Prior knowledge can not only filter out invalid topological structures but also complement the topological semantics. To this end, We propose a novel inductive model, PLCS, which incorporates prompt learning with contextual semantics to fully exploit prior knowledge. To filter out irrelevant topological structures, we innovatively employ hard prompts to mine prior knowledge in pre-trained language models (PLMs) as the basis for subgraph extraction. Additionally, we enhance semantic representation by integrating relation text descriptions into relation embeddings during initialization, supplementing topological semantics. The experimental results on four benchmark datasets show the superiority of PLCS over existing state-of-the-art methods.
JO  - Expert Syst. Appl.
PB  - 
CY  - 
VL  - 238
IS  - 
PG  - 12
SP  - 12
EP  - 
AN  - WOS:001098693800001
DO  - 10.1016/j.eswa.2023.121669
UR  - <Go to ISI>://WOS:001098693800001
NS  - 
N1  - Ishan Tamrakar (2024-12-06 02:24:44)(Screen): external; 
ER  - 

TY  - CONF
TI  - (COMET-)ATOMIC<sub>20</sub><SUP>20</SUP>): On Symbolic and Neural Commonsense Knowledge Graphs
AU  - Hwang, J. D.
AU  - Bhagavatula, C.
AU  - Le Bras, R.
AU  - Da, J.
AU  - Sakaguchi, K.
AU  - Bosselut, A.
AU  - Choi, Y. J.
AU  - Assoc Advancement Artificial, Intelligence
PY  - 2021
PD  - 
N2  - Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC(20)(20)) , a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC(20)(20)) is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains similar to 12 absolute points lower than a BART-based knowledge model trained on ATOMIC(20)(20) despite using over 430x fewer parameters.
JO  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Electr Network
VL  - 35
IS  - 
PG  - 6384-6392
SP  - 6384
EP  - 6392
AN  - WOS:000680423506056
DO  - 
UR  - <Go to ISI>://WOS:000680423506056
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Computers' Interpretations of Knowledge Representation Using Pre-Conceptual Schemas: An Approach Based on the BERT and Llama 2-Chat Models
AU  - Insuasti, J.
AU  - Roa, F.
AU  - Zapata-Jaramillo, C. M.
PY  - 2023
PD  - 
N2  - Pre-conceptual schemas are a straightforward way to represent knowledge using controlled language regardless of context. Despite the benefits of using pre-conceptual schemas by humans, they present challenges when interpreted by computers. We propose an approach to making computers able to interpret the basic pre-conceptual schemas made by humans. To do that, the construction of a linguistic corpus is required to work with large language models-LLM. The linguistic corpus was mainly fed using Master's and doctoral theses from the digital repository of the University of Narino to produce a training dataset for re-training the BERT model; in addition, we complement this by explaining the elicited sentences in triads from the pre-conceptual schemas using one of the cutting-edge large language models in natural language processing: Llama 2-Chat by Meta AI. The diverse topics covered in these theses allowed us to expand the spectrum of linguistic use in the BERT model and empower the generative capabilities using the fine-tuned Llama 2-Chat model and the proposed solution. As a result, the first version of a computational solution was built to consume the language models based on BERT and Llama 2-Chat and thus automatically interpret pre-conceptual schemas by computers via natural language processing, adding, at the same time, generative capabilities. The validation of the computational solution was performed in two phases: the first one for detecting sentences and interacting with pre-conceptual schemas with students in the Formal Languages and Automata Theory course-the seventh semester of the systems engineering undergraduate program at the University of Narino's Tumaco campus. The second phase was for exploring the generative capabilities based on pre-conceptual schemas; this second phase was performed with students in the Object-oriented Design course-the second semester of the systems engineering undergraduate program at the University of Narino's Tumaco campus. This validation yielded favorable results in implementing natural language processing using the BERT and Llama 2-Chat models. In this way, some bases were laid for future developments related to this research topic.
JO  - Big Data Cogn. Comput.
PB  - 
CY  - 
VL  - 7
IS  - 4
PG  - 22
SP  - 22
EP  - 
AN  - WOS:001137855700001
DO  - 10.3390/bdcc7040182
UR  - <Go to ISI>://WOS:001137855700001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - CoRTEx: contrastive learning for representing terms via explanations with applications on constructing biomedical knowledge graphs
AU  - Ying, H. Y.
AU  - Zhao, Z. Y.
AU  - Zhao, Y.
AU  - Zeng, S. H.
AU  - Yu, S.
PY  - 2024
PD  - 
N2  - Objectives Biomedical Knowledge Graphs play a pivotal role in various biomedical research domains. Concurrently, term clustering emerges as a crucial step in constructing these knowledge graphs, aiming to identify synonymous terms. Due to a lack of knowledge, previous contrastive learning models trained with Unified Medical Language System (UMLS) synonyms struggle at clustering difficult terms and do not generalize well beyond UMLS terms. In this work, we leverage the world knowledge from large language models (LLMs) and propose Contrastive Learning for Representing Terms via Explanations (CoRTEx) to enhance term representation and significantly improves term clustering.Materials and Methods The model training involves generating explanations for a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning, considering term and explanation embeddings simultaneously, and progressively introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH algorithm is designed for efficient clustering of a new ontology.Results We established a clustering test set and a hard negative test set, where our model consistently achieves the highest F1 score. With CoRTEx embeddings and the modified BIRCH algorithm, we grouped 35 580 932 terms from the Biomedical Informatics Ontology System (BIOS) into 22 104 559 clusters with O(N) queries to ChatGPT. Case studies highlight the model's efficacy in handling challenging samples, aided by information from explanations.Conclusion By aligning terms to their explanations, CoRTEx demonstrates superior accuracy over benchmark models and robustness beyond its training set, and it is suitable for clustering terms for large-scale biomedical ontologies.
JO  - J. Am. Med. Inf. Assoc.
PB  - 
CY  - 
VL  - 31
IS  - 9
PG  - 10
SP  - 10
EP  - 
AN  - WOS:001228915700001
DO  - 10.1093/jamia/ocae115
UR  - <Go to ISI>://WOS:001228915700001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Crawling The Internal Knowledge-Base of Language Models
AU  - Cohen, R.
AU  - Geva, M.
AU  - Berant, J.
AU  - Globerson, A.
PY  - 2023
PD  - 
N2  - Language models are trained on large volumes of text, and as a result their parameters might contain a significant body of factual knowledge. Any downstream task performed by these models implicitly builds on these facts, and thus it is highly desirable to have means for representing this body of knowledge in an interpretable way. However, there is currently no mechanism for such a representation. Here, we propose to address this goal by extracting a knowledge-graph of facts from a given language model. We describe a procedure for "crawling" the internal knowledge-base of a language model. Specifically, given a seed entity, we expand a knowledge-graph around it. The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision (i.e., that no wrong facts are generated) and recall (i.e., the number of facts generated). We evaluate our approach on graphs crawled starting from dozens of seed entities, and show it yields high precision graphs (82-92%), while emitting a reasonable number of facts per entity.
JO  - 17th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL)
PB  - Assoc Computational Linguistics-Acl
CY  - Dubrovnik, CROATIA
VL  - 
IS  - 
PG  - 1856-1869
SP  - 1856
EP  - 1869
AN  - WOS:001181085100137
DO  - 
UR  - <Go to ISI>://WOS:001181085100137
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Deep Learning-Based Knowledge Graph Generation for COVID-19
AU  - Kim, T.
AU  - Yun, Y.
AU  - Kim, N.
PY  - 2021
PD  - 
N2  - Many attempts have been made to construct new domain-specific knowledge graphs using the existing knowledge base of various domains. However, traditional "dictionary-based" or "supervised" knowledge graph building methods rely on predefined human-annotated resources of entities and their relationships. The cost of creating human-annotated resources is high in terms of both time and effort. This means that relying on human-annotated resources will not allow rapid adaptability in describing new knowledge when domain-specific information is added or updated very frequently, such as with the recent coronavirus disease-19 (COVID-19) pandemic situation. Therefore, in this study, we propose an Open Information Extraction (OpenIE) system based on unsupervised learning without a pre-built dataset. The proposed method obtains knowledge from a vast amount of text documents about COVID-19 rather than a general knowledge base and add this to the existing knowledge graph. First, we constructed a COVID-19 entity dictionary, and then we scraped a large text dataset related to COVID-19. Next, we constructed a COVID-19 perspective language model by fine-tuning the bidirectional encoder representations from transformer (BERT) pre-trained language model. Finally, we defined a new COVID-19-specific knowledge base by extracting connecting words between COVID-19 entities using the BERT self-attention weight from COVID-19 sentences. Experimental results demonstrated that the proposed Co-BERT model outperforms the original BERT in terms of mask prediction accuracy and metric for evaluation of translation with explicit ordering (METEOR) score.
JO  - Sustainability
PB  - 
CY  - 
VL  - 13
IS  - 4
PG  - 19
SP  - 19
EP  - 
AN  - WOS:000624819300001
DO  - 10.3390/su13042276
UR  - <Go to ISI>://WOS:000624819300001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - DeNERT-KG: Named Entity and Relation Extraction Model Using DQN, Knowledge Graph, and BERT
AU  - Yang, S.
AU  - Yoo, S.
AU  - Jeong, O.
PY  - 2020
PD  - 
N2  - Along with studies on artificial intelligence technology, research is also being carried out actively in the field of natural language processing to understand and process people's language, in other words, natural language. For computers to learn on their own, the skill of understanding natural language is very important. There are a wide variety of tasks involved in the field of natural language processing, but we would like to focus on the named entity registration and relation extraction task, which is considered to be the most important in understanding sentences. We propose DeNERT-KG, a model that can extract subject, object, and relationships, to grasp the meaning inherent in a sentence. Based on the BERT language model and Deep Q-Network, the named entity recognition (NER) model for extracting subject and object is established, and a knowledge graph is applied for relation extraction. Using the DeNERT-KG model, it is possible to extract the subject, type of subject, object, type of object, and relationship from a sentence, and verify this model through experiments.
JO  - Appl. Sci.-Basel
PB  - 
CY  - 
VL  - 10
IS  - 18
PG  - 15
SP  - 15
EP  - 
AN  - WOS:000581394800001
DO  - 10.3390/app10186429
UR  - <Go to ISI>://WOS:000581394800001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering
AU  - Wang, Y. J.
AU  - Zhang, H.
AU  - Liang, J. Y.
AU  - Li, R.
PY  - 2023
PD  - 
N2  - Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and knowledge representation learning (DHLK), which constructs a heterogeneous knowledge graph (HKG) based on multiple knowledge sources and optimizes the structure and knowledge representation of the HKG using a two-stage pruning strategy and knowledge representation learning (KRL). It then performs joint reasoning by LMs and Relation Mask Self-Attention (RMSA). Specifically, DHLK filters key entities based on the dictionary vocabulary to achieve the first-stage pruning while incorporating the paraphrases in the dictionary into the subgraph to construct the HKG. Then, DHLK encodes and fuses the QA context and HKG using LM, and dynamically removes irrelevant KG entities based on the attention weights of LM for the second-stage pruning. Finally, DHLK introduces KRL to optimize the knowledge representation and perform answer reasoning on the HKG by RMSA. We evaluate DHLK at CommonsenseQA and OpenBookQA, and show its improvement on existing LM and LM+KG methods.
JO  - 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)
PB  - Assoc Computational Linguistics-Acl
CY  - Toronto, CANADA
VL  - 
IS  - 
PG  - 14048-14063
SP  - 14048
EP  - 14063
AN  - WOS:001190962505047
DO  - 
UR  - <Go to ISI>://WOS:001190962505047
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Empowering Graph Neural Network-Based Computational Drug Repositioning with Large Language Model-Inferred Knowledge Representation
AU  - Gu, Y. W.
AU  - Xu, Z. D.
AU  - Yang, C. R.
PY  - 2024
PD  - 
N2  - Computational drug repositioning, through predicting drug-disease associations (DDA), offers significant potential for discovering new drug indications. Current methods incorporate graph neural networks (GNN) on drug-disease heterogeneous networks to predict DDAs, achieving notable performances compared to traditional machine learning and matrix factorization approaches. However, these methods depend heavily on network topology, hampered by incomplete and noisy network data, and overlook the wealth of biomedical knowledge available. Correspondingly, large language models (LLMs) excel in graph search and relational reasoning, which can possibly enhance the integration of comprehensive biomedical knowledge into drug and disease profiles. In this study, we first investigate the contribution of LLM-inferred knowledge representation in drug repositioning and DDA prediction. A zero-shot prompting template was designed for LLM to extract high-quality knowledge descriptions for drug and disease entities, followed by embedding generation from language models to transform the discrete text to continual numerical representation. Then, we proposed LLM-DDA with three different model architectures (LLM-DDANode Feat, LLM-DDADual GNN, LLM-DDAGNN-AE) to investigate the best fusion mode for LLM-based embeddings. Extensive experiments on four DDA benchmarks show that, LLM-DDAGNN-AE achieved the optimal performance compared to 11 baselines with the overall relative improvement in AUPR of 23.22%, F1-Score of 17.20%, and precision of 25.35%. Meanwhile, selected case studies of involving Prednisone and Allergic Rhinitis highlighted the model's capability to identify reliable DDAs and knowledge descriptions, supported by existing literature. This study showcases the utility of LLMs in drug repositioning with its generality and applicability in other biomedical relation prediction tasks.
JO  - Interdiscip. Sci.
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 18
SP  - 18
EP  - 
AN  - WOS:001320091000001
DO  - 10.1007/s12539-024-00654-7
UR  - <Go to ISI>://WOS:001320091000001
NS  - 
N1  - Ishan Tamrakar (2024-12-29 03:09:10)(Screen): Does include aspects of KE; 
ER  - 

TY  - JOUR
TI  - Enhancing Abstractive Summarization with Extracted Knowledge Graphs and Multi-Source Transformers
AU  - Chen, T.
AU  - Wang, X. W.
AU  - Yue, T. W.
AU  - Bai, X. Y.
AU  - Le, C. X.
AU  - Wang, W. P.
PY  - 2023
PD  - 
N2  - As the popularity of large language models (LLMs) has risen over the course of the last year, led by GPT-3/4 and especially its productization as ChatGPT, we have witnessed the extensive application of LLMs to text summarization. However, LLMs do not intrinsically have the power to verify the correctness of the information they supply and generate. This research introduces a novel approach to abstractive summarization, aiming to address the limitations of LLMs in that they struggle to understand the truth. The proposed method leverages extracted knowledge graph information and structured semantics as a guide for summarization. Building upon BART, one of the state-of-the-art sequence-to-sequence pre-trained LLMs, multi-source transformer modules are developed as an encoder, which are capable of processing textual and graphical inputs. Decoding is performed based on this enriched encoding to enhance the summary quality. The Wiki-Sum dataset, derived from Wikipedia text dumps, is introduced for evaluation purposes. Comparative experiments with baseline models demonstrate the strengths of the proposed approach in generating informative and relevant summaries. We conclude by presenting our insights into utilizing LLMs with graph external information, which will become a powerful aid towards the goal of factually correct and verified LLMs.
JO  - Appl. Sci.-Basel
PB  - 
CY  - 
VL  - 13
IS  - 13
PG  - 14
SP  - 14
EP  - 
AN  - WOS:001028381400001
DO  - 10.3390/app13137753
UR  - <Go to ISI>://WOS:001028381400001
NS  - 
N1  - Ishan Tamrakar (2025-01-09 11:40:09)(Screen): don't know where the "extracted knowledge graph information" is coming from ; 
ER  - 

TY  - JOUR
TI  - Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement
AU  - Yang, R.
AU  - Zhu, J. H.
AU  - Man, J. P.
AU  - Fang, L.
AU  - Zhou, Y.
PY  - 2024
PD  - 
N2  - The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and contrastive learning to enhance KGC models. The effectiveness of text-based methods largely hinges on the quality and richness of the training data. Large language models (LLMs) can utilize straightforward prompts to alter text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs typically demand substantial computational resources. To address these issues, we introduce a framework termed constrained prompts for KGC (CP-KGC). This CP-KGC framework designs prompts that adapt to different datasets to enhance semantic richness. Additionally, CP-KGC employs a context constraint strategy to effectively identify polysemous entities within KGC datasets. Through extensive experimentation, we have verified the effectiveness of this framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances the performance of text-based KGC methods.1 1 This study extends the performance limits of existing models and promotes further integration of KGC with LLMs.
JO  - Knowledge-Based Syst.
PB  - 
CY  - 
VL  - 300
IS  - 
PG  - 9
SP  - 9
EP  - 
AN  - WOS:001282476600001
DO  - 10.1016/j.knosys.2024.112155
UR  - <Go to ISI>://WOS:001282476600001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Enriching contextualized language model from knowledge graph for biomedical information extraction
AU  - Fei, H.
AU  - Ren, Y. F.
AU  - Zhang, Y.
AU  - Ji, D. H.
AU  - Liang, X. H.
PY  - 2021
PD  - 
N2  - Biomedical information extraction (BioIE) is an important task. The aim is to analyze biomedical texts and extract structured information such as named entities and semantic relations between them. In recent years, pre-trained language models have largely improved the performance of BioIE. However, they neglect to incorporate external structural knowledge, which can provide rich factual information to support the underlying understanding and reasoning for biomedical information extraction. In this paper, we first evaluate current extraction methods, including vanilla neural networks, general language models and pre-trained contextualized language models on biomedical information extraction tasks, including named entity recognition, relation extraction and event extraction. We then propose to enrich a contextualized language model by integrating a large scale of biomedical knowledge graphs (namely, BioKGLM). In order to effectively encode knowledge, we explore a three-stage training procedure and introduce different fusion strategies to facilitate knowledge injection. Experimental results on multiple tasks show that BioKGLM consistently outperforms state-of-the-art extraction models. A further analysis proves that BioKGLM can capture the underlying relations between biomedical knowledge concepts, which are crucial for BioIE.
JO  - Brief. Bioinform.
PB  - 
CY  - 
VL  - 22
IS  - 3
PG  - 14
SP  - 14
EP  - 
AN  - WOS:000709461300056
DO  - 10.1093/bib/bbaa110
UR  - <Go to ISI>://WOS:000709461300056
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - From Static to Dynamic: Knowledge Metabolism for Large Language Models
AU  - Du, M. Z.
AU  - Luu, A. T.
AU  - Ji, B.
AU  - Ng, S. K.
PY  - 2024
PD  - 
N2  - The immense parameter space of Large Language Models (LLMs) endows them with superior knowledge retention capabilities, allowing them to excel in a variety of natural language processing tasks. However, it also instigates difficulties in consistently tuning LLMs to incorporate the most recent knowledge, which may further lead LLMs to produce inaccurate and fabricated content. To alleviate this issue, we propose a knowledge metabolism framework for LLMs, which proactively sustains the credibility of knowledge through an auxiliary memory component and directly delivers pertinent knowledge for LLM inference, thereby suppressing hallucinations caused by obsolete internal knowledge during the LLM inference process. Benchmark experiments demonstrate DynaMind's effectiveness in overcoming this challenge. The code and demo of DynaMind are available at: https://github.com/Elfsong/DynaMind.
JO  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Vancouver, CANADA
VL  - 
IS  - 
PG  - 23784-23786
SP  - 23784
EP  - 23786
AN  - WOS:001239989100260
DO  - 
UR  - <Go to ISI>://WOS:001239989100260
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Generative Multi-Modal Knowledge Retrieval with Large Language Models
AU  - Long, X. W.
AU  - Zeng, J. L.
AU  - Meng, F. D.
AU  - Ma, Z. Y.
AU  - Zhang, K. Y.
AU  - Zhou, B. W.
AU  - Zhou, J.
PY  - 2024
PD  - 
N2  - Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications. However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries. In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval. Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data. We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue. In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning. Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions. Subsequently, we construct instruction data with a unified format for model training. Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues. Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines.
JO  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Vancouver, CANADA
VL  - 
IS  - 
PG  - 18733-18741
SP  - 18733
EP  - 18741
AN  - WOS:001239407300027
DO  - 
UR  - <Go to ISI>://WOS:001239407300027
NS  - 
N1  - Ishan Tamrakar (2025-01-09 05:37:36)(Screen): KE; 
ER  - 

TY  - CONF
TI  - Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis
AU  - Kirk, J. R.
AU  - Wray, R. E.
AU  - Lindes, P.
AU  - Laird, J. E.
PY  - 2024
PD  - 
N2  - Large language models (LLMs) offer significant promise as a knowledge source for task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks. We describe a cognitive-agent approach, STARS, that extends and complements prompt engineering, mitigating its limitations and thus enabling an agent to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The STARS approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous agent, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how an agent, by retrieving and evaluating a breadth of responses from the LLM, can achieve 77- 94% task completion in one-shot learning without user oversight. The approach achieves 100% task completion when human oversight (such as an indication of preference) is provided. Further, the type of oversight largely shifts from explicit, natural language instruction to simple confirmation/discomfirmation of high-quality responses that have been vetted by the agent before presentation to a user.
JO  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Vancouver, CANADA
VL  - 
IS  - 
PG  - 18390-18398
SP  - 18390
EP  - 18398
AN  - WOS:001239323500122
DO  - 
UR  - <Go to ISI>://WOS:001239323500122
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Improving Relation Extraction by Knowledge Representation Learning
AU  - Hong, W. X.
AU  - Li, S. Y.
AU  - Hu, Z. Q.
AU  - Rasool, A.
AU  - Jiang, Q. S.
AU  - Weng, Y.
AU  - Soc, Ieee Comp
PY  - 2021
PD  - 
N2  - Relation extraction is an important NLP task to extract the semantic relationship between two entities. Recently, large-scale pre-training language models have achieved excellent performance in many NLP applications. Most of the existing relation extraction models mainly rely on context information, but entity information is also very important for relation extraction, especially domain knowledge of entity and the direction between entity pairs. In this paper, based on the pre-trained BERT model, we propose a multi-task joint relation extraction model incorporating knowledge representation learning(KRL). The experimental results on the SemEval 2010 task 8 dataset and the KBP37 dataset show that our proposed model outperforms most of state-of-the-art methods. The results on the larger dataset FewRe180 refined from FewRel also indicate that increasing the knowledge representation learning as an auxiliary objective is helpful for the relation extraction task.
JO  - IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)
PB  - Ieee Computer Soc
CY  - Electr Network
VL  - 
IS  - 
PG  - 1211-1215
SP  - 1211
EP  - 1215
AN  - WOS:000747482300183
DO  - 10.1109/ictai52525.2021.00191
UR  - <Go to ISI>://WOS:000747482300183
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Inductive Reasoning in Minds and Machines
AU  - Bhatia, S.
PY  - 2023
PD  - 
N2  - Induction-the ability to generalize from existing knowledge-is the cornerstone of intelligence. Cognitive models of human induction are largely limited to toy problems and cannot make quantitative predictions for the thousands of different induction arguments that have been studied by researchers, or to the countless induction arguments that could be encountered in everyday life. Leading large language models (LLMs) go beyond toy problems but fail to mimic observed patterns of human induction. In this article, we combine rich knowledge representations obtained from LLMs with theories of human inductive reasoning developed by cognitive psychologists. We show that this integrative approach can capture several benchmark empirical findings on human induction and generate human-like responses to natural language arguments with thousands of common categories and properties. These findings shed light on the cognitive mechanisms at play in human induction and show how existing theories in psychology and cognitive science can be integrated with new methods in artificial intelligence, to successfully model high-level human cognition.
JO  - Psychol. Rev.
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 20
SP  - 20
EP  - 
AN  - WOS:001109511000001
DO  - 10.1037/rev0000446
UR  - <Go to ISI>://WOS:001109511000001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Information Extraction of Aviation Accident Causation Knowledge Graph: An LLM-Based Approach
AU  - Chen, L.
AU  - Xu, J. H.
AU  - Wu, T. Y.
AU  - Liu, J.
PY  - 2024
PD  - 
N2  - Summarizing the causation of aviation accidents is conducive to enhancing aviation safety. The knowledge graph of aviation accident causation, constructed based on aviation accident reports, can assist in analyzing the causes of aviation accidents. With the continuous development of artificial intelligence technology, leveraging large language models for information extraction and knowledge graph construction has demonstrated significant advantages. This paper proposes an information extraction method for aviation accident causation based on Claude-prompt, which relies on the large-scale pre-trained language model Claude 3.5. Through prompt engineering, combined with a few-shot learning strategy and a self-judgment mechanism, this method achieves automatic extraction of accident-cause entities and their relationships. Experimental results indicate that this approach effectively improves the accuracy of information extraction, overcoming the limitations of traditional methods in terms of accuracy and efficiency in processing complex texts. It provides strong support for subsequently constructing a structured knowledge graph of aviation accident causation and conducting causation analysis of aviation accidents.
JO  - Electronics
PB  - 
CY  - 
VL  - 13
IS  - 19
PG  - 21
SP  - 21
EP  - 
AN  - WOS:001334218600001
DO  - 10.3390/electronics13193936
UR  - <Go to ISI>://WOS:001334218600001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Intermediate representations to improve the semantic parsing of building regulations
AU  - Fuchs, S.
AU  - Dimyadi, J.
AU  - Witbrock, M.
AU  - Amor, R.
PY  - 2024
PD  - 
N2  - Recent developments show that large transformer-based language models have the capability to generate coherent text and source code in response to user prompts. This capability can be used in the construction domain to interpret building regulations and convert them into a formal representation usable for automated compliance checking. While base-size models can already be taught to perform semantic parsing with decent quality, this paper shows how Intermediate Representations (IRs) can be used to improve the semantic parsing quality. With reversible IRs, the training time was reduced to almost a quarter of the initial duration, and through adding a hierarchical parsing step, improvements of up to 6.6% on F1 scores were reached. Furthermore, intermediate representations provide a novel and interpretable method towards a human-in-the-loop approach for translating building regulations into a formal representation.
JO  - Adv. Eng. Inform.
PB  - 
CY  - 
VL  - 62
IS  - 
PG  - 15
SP  - 15
EP  - 
AN  - WOS:001287710200001
DO  - 10.1016/j.aei.2024.102735
UR  - <Go to ISI>://WOS:001287710200001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - JAKET: Joint Pre-training of Knowledge Graph and Language Understanding
AU  - Yu, D. H.
AU  - Zhu, C. G.
AU  - Yang, Y. M.
AU  - Zeng, M.
AU  - Assoc Advancement Artificial, Intelligence
PY  - 2022
PD  - 
N2  - Knowledge graphs (KGs) contain rich information about world knowledge, entities, and relations. Thus, they can be great supplements to existing pre-trained language models. However, it remains a challenge to efficiently integrate information from KG into language modeling. And the understanding of a knowledge graph requires related context. We propose a novel joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embeddings for entities and relations in the graph. Our design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. Experiment results on several knowledge-aware NLP tasks show that our proposed framework achieves superior performance by effectively leveraging knowledge in language understanding.
JO  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Electr Network
VL  - 
IS  - 
PG  - 11630-11638
SP  - 11630
EP  - 11638
AN  - WOS:000893639104072
DO  - 
UR  - <Go to ISI>://WOS:000893639104072
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons
AU  - Chen, Y. H.
AU  - Cao, P. F.
AU  - Chen, Y. B.
AU  - Liu, K.
AU  - Zhao, J.
PY  - 2024
PD  - 
N2  - Pre-trained language models (PLMs) contain vast amounts of factual knowledge, but how the knowledge is stored in the parameters remains unclear. This paper delves into the complex task of understanding how factual knowledge is stored in multilingual PLMs, and introduces the Architecture-adapted Multilingual Integrated Gradients method, which successfully localizes knowledge neurons more precisely compared to current methods, and is more universal across various architectures and languages. Moreover, we conduct an in-depth exploration of knowledge neurons, leading to the following two important discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which store factual knowledge in a form that transcends language. We design cross-lingual knowledge editing experiments, demonstrating that the PLMs can accomplish this task based on language-independent neurons; (2) The discovery of Degenerate Knowledge Neurons, a novel type of neuron showing that different knowledge neurons can store the same fact. Its property of functional overlap endows the PLMs with a robust mastery of factual knowledge. We design fact-checking experiments, proving that the degenerate knowledge neurons can help the PLMs to detect wrong facts. Experiments corroborate these findings, shedding light on the mechanisms of factual knowledge storage in multilingual PLMs, and contribute valuable insights to the field. The code is available at https://github.com/heng840/AMIG.
JO  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Vancouver, CANADA
VL  - 
IS  - 
PG  - 17817-17825
SP  - 17817
EP  - 17825
AN  - WOS:001239323500060
DO  - 
UR  - <Go to ISI>://WOS:001239323500060
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Knowledge graph empowerment from knowledge learning to graduation requirements achievement
AU  - Yang, Y. R.
AU  - Chen, S. S.
AU  - Zhu, Y. P.
AU  - Zhu, H.
AU  - Chen, Z. G.
PY  - 2023
PD  - 
N2  - A deep understanding of the relationship between the knowledge acquired and the graduation requirements is essential for students to precisely meet the graduation requirements and to become human resources with specific knowledge, skills and professionalism. In this paper, we define the ontology layer of the knowledge graph by deeply analyzing the relationship between graduation requirement, course and knowledge. Based on the implementation of the concept of Outcome Based Education, we use Knowledge extraction, fusion, reasoning techniques to construct a hierarchical knowledge graph with the main line of "knowledge-course-graduation requirements. In the process of knowledge extraction, in order to alleviate the huge labor overhead brought by traditional extraction methods, this paper adopts a transfer learning method to extract triadic knowledge using the multi-task framework EERJE, Finally, knowledge reasoning was also performed with the help of LLM to further expand the knowledge scope. The comprehensiveness, correctness and relatedness of the data were evaluated through the experiment, and the F1 value of the ternary group extraction was 87.76%, the accuracy rate of entity classification was 85.42%, the data coverage was more comprehensive, and the results showed that the data quality was better, and the knowledge graph constructed in this way can fully optimize the organization and management of teaching resources, help students intuitively and comprehensively grasp the correlation and difference between graduation requirements and various knowledge points, and let the Students can carry out personalized independent learning through the navigation mode of knowledge graph, strengthen their weak links, and complete the relevant graduation requirements, which effectively improves the degree of students' graduation requirements achievement. This new paradigm of knowledge graph enabled teaching is of reference significance for engineering education majors to improve the degree of graduation requirements achievement.
JO  - PLoS One
PB  - 
CY  - 
VL  - 18
IS  - 10
PG  - 18
SP  - 18
EP  - 
AN  - WOS:001092548300091
DO  - 10.1371/journal.pone.0292903
UR  - <Go to ISI>://WOS:001092548300091
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust
AU  - Roy, K.
AU  - Garg, T.
AU  - Palit, V.
AU  - Ieee
PY  - 2023
PD  - 
N2  - A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the selfattention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.
JO  - IEEE Conference on Artificial Intelligence (IEEE CAI)
PB  - Ieee Computer Soc
CY  - Santa Clara, CA
VL  - 
IS  - 
PG  - 234-236
SP  - 234
EP  - 236
AN  - WOS:001046447800098
DO  - 10.1109/cai54212.2023.00108
UR  - <Go to ISI>://WOS:001046447800098
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Graph-Based Credibility Evaluation Method for Electric Grid Large Language Model Knowledge Question-Answering
AU  - Li, W. Q.
AU  - Qi, X. M.
AU  - Zhao, Q.
AU  - Wang, C.
AU  - Wu, Q. Y.
AU  - Tang, X. S.
AU  - Assoc Computing, Machinery
PY  - 2023
PD  - 
N2  - In the field of electricity, specialized terminology is often intricate and complex, making it challenging for non-experts to comprehend. However, with the advancement of artificial intelligence technology, the emergence of large language models provides a new technological solution to address this issue. Large language models, based on deep learning techniques, have the capability to quickly understand and interpret specialized terminology in the electricity domain through learning from a vast corpus of professional literature and data. They can then be applied to various domains, including question-answering systems. However, existing large language models still face issues of unreliable outputs, necessitating a method to evaluate their results and improve the quality of their applications. We propose a knowledge graph-based credibility evaluation method for electric grid large language model knowledge question-answering. This method aligns the answers generated by large language models with the knowledge graph of a local knowledge base and calculates their cosine similarity and Pearson correlation coefficient. We batch-process the answers from the large language model into an electricity dataset and validate them using this method. Experimental results demonstrate that this method can accurately and efficiently reflect the relevance between texts, providing a reliable scoring basis for question-answering by large models in vertical domains. Future research can focus on exploring other embedding methods that can better extract semantic relationships between texts and validating the feasibility of this method in vertical domains other than electricity.
JO  - 7th International Conference on Electronic Information Technology and Computer Engineering (EITCE)
PB  - Assoc Computing Machinery
CY  - Xiamen, PEOPLES R CHINA
VL  - 
IS  - 
PG  - 754-759
SP  - 754
EP  - 759
AN  - WOS:001283896700126
DO  - 10.1145/3650400.3650526
UR  - <Go to ISI>://WOS:001283896700126
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Neurons in Pretrained Transformers
AU  - Dai, D. M.
AU  - Dong, L.
AU  - Hao, Y. R.
AU  - Sui, Z. F.
AU  - Chang, B. B.
AU  - Wei, F. R.
AU  - Assoc Computat, Linguist
PY  - 2022
PD  - 
N2  - Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at https://github.com/Hunter-DDM/knowledge-neurons.
JO  - 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)
PB  - Assoc Computational Linguistics-Acl
CY  - Dublin, IRELAND
VL  - 
IS  - 
PG  - 8493-8502
SP  - 8493
EP  - 8502
AN  - WOS:000828702308038
DO  - 
UR  - <Go to ISI>://WOS:000828702308038
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - KPT: Keyword-Guided Pre-training for Grounded Dialog Generation
AU  - Zhu, Q.
AU  - Mi, F.
AU  - Zhang, Z.
AU  - Wang, Y. S.
AU  - Li, Y. T.
AU  - Jiang, X.
AU  - Liu, Q.
AU  - Zhu, X. Y.
AU  - Huang, M. L.
PY  - 2023
PD  - 
N2  - Incorporating external knowledge into the response generation process is essential to building more helpful and reliable dialog agents. However, collecting knowledge-grounded conversations is often costly, calling for a better pre-trained model for grounded dialog generation that generalizes well w.r.t. different types of knowledge. In this work, we propose KPT (Keyword-guided Pre-Training), a novel self-supervised pre-training method for grounded dialog genera-tion without relying on extra knowledge annotation. Specifically, we use a pre-trained language model to extract the most uncertain tokens in the dialog as keywords. With these keywords, we construct two kinds of knowledge and pre-train a knowledge-grounded response generation model, aiming at handling two different scenarios: (1) the knowledge should be faithfully grounded; (2) it can be selectively used. For the former, the grounding knowledge consists of keywords extracted from the response. For the latter, the grounding knowledge is additionally augmented with keywords extracted from other utterances in the same dialog. Since the knowledge is extracted from the dialog itself, KPT can be easily performed on a large volume and variety of dialogue data. We considered three data sources (open-domain, task-oriented, conversational QA) with a total of 2.5M dialogues. We conduct extensive experiments on various few-shot knowledge-grounded generation tasks, including grounding on dialog acts, knowledge graphs, persona descriptions, and Wikipedia passages. Our comprehensive experiments and analyses demonstrate that KPT consistently outperforms state-of-the-art methods on these tasks with diverse grounding knowledge.
JO  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Washington, DC
VL  - 
IS  - 
PG  - 14065-14073
SP  - 14065
EP  - 14073
AN  - WOS:001243753000172
DO  - 
UR  - <Go to ISI>://WOS:001243753000172
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Language Models Fine-Tuning for Automatic Format Reconstruction of SEC Financial Filings
AU  - Lombardo, G.
AU  - Trimigno, G.
AU  - Pellegrino, M.
AU  - Cagnoni, S.
PY  - 2024
PD  - 
N2  - The analysis of financial reports is a crucial task for investors and regulators, especially the mandatory annual reports (10-K) required by the SEC (Securities and Exchange Commission) that provide crucial information about a public company in the American stock market. Although SEC suggests a specific document format to standardize and simplify the analysis, in recent years, several companies have introduced their own format and organization of the contents, making human-based and automatic knowledge extraction inherently more difficult. In this research work, we investigate different Neural language models based on Transformer networks (Bidirectional recurrence-based, Autoregressive-based, and Autoencoders-based approaches) to automatically reconstruct an SEC-like format of the documents as a multi-class classification task with 18 classes at the sentence level. In particular, we propose a Bidirectional fine-tuning procedure to specialize pre-trained language models on this task. We propose and make the resulting novel transformer model, named SEC-former, publicly available to deal with this task. We evaluate SEC-former in three different scenarios: 1) in terms of topic detection performances; 2) in terms of document similarity (TF-IDF Bag-of-words and Doc2Vec) achieved with respect to original and trustable financial reports since this operation is leveraged for portfolio optimization tasks; and 3) testing the model in a real use-case scenario related to a public company that does not respect the SEC format but provides a human-supervised reference to reconstruct it.
JO  - IEEE Access
PB  - 
CY  - 
VL  - 12
IS  - 
PG  - 31249-31261
SP  - 31249
EP  - 31261
AN  - WOS:001176095400001
DO  - 10.1109/access.2024.3370444
UR  - <Go to ISI>://WOS:001176095400001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Learning to Rank Query Graphs for Complex Question Answering over Knowledge Graphs
AU  - Maheshwari, G.
AU  - Trivedi, P.
AU  - Lukovnikov, D.
AU  - Chakraborty, N.
AU  - Fischer, A.
AU  - Lehmann, J.
PY  - 2019
PD  - 
N2  - In this paper, we conduct an empirical investigation of neural query graph ranking approaches for the task of complex question answering over knowledge graphs. We propose a novel self-attention based slot matching model which exploits the inherent structure of query graphs, our logical form of choice. Our proposed model generally outperforms other ranking models on two QA datasets over the DBpedia knowledge graph, evaluated in different settings. We also show that domain adaption and pre-trained language model based transfer learning yield improvements, effectively offsetting the general lack of training data.
JO  - 18th International Semantic Web Conference (ISWC)
PB  - Springer International Publishing Ag
CY  - Auckland, NEW ZEALAND
VL  - 11778
IS  - 
PG  - 487-504
SP  - 487
EP  - 504
AN  - WOS:000521413100028
DO  - 10.1007/978-3-030-30793-6_28
UR  - <Go to ISI>://WOS:000521413100028
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Meta-path reasoning of knowledge graph for commonsense question answering
AU  - Zhang, M.
AU  - He, T. T.
AU  - Dong, M.
PY  - 2024
PD  - 
N2  - Commonsense question answering (CQA) requires understanding and reasoning over QA context and related commonsense knowledge, such as a structured Knowledge Graph (KG). Existing studies combine language models and graph neural networks to model inference. However, traditional knowledge graph are mostly concept-based, ignoring direct path evidence necessary for accurate reasoning. In this paper, we propose MRGNN (Meta-path Reasoning Graph Neural Network), a novel model that comprehensively captures sequential semantic information from concepts and paths. In MRGNN, meta-paths are introduced as direct inference evidence and an original graph neural network is adopted to aggregate features from both concepts and paths simultaneously. We conduct sufficient experiments on the CommonsenceQA and OpenBookQA datasets, showing the effectiveness of MRGNN. Also, we conduct further ablation experiments and explain the reasoning behavior through the case study.
JO  - Front.. Comput. Sci.
PB  - 
CY  - 
VL  - 18
IS  - 1
PG  - 11
SP  - 11
EP  - 
AN  - WOS:001048565800007
DO  - 10.1007/s11704-022-2336-6
UR  - <Go to ISI>://WOS:001048565800007
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Military Chain: Construction of Domain Knowledge Graph of Kill Chain Based on Natural Language Model
AU  - Wang, Y. F.
AU  - Wang, T.
AU  - Wang, J. H.
AU  - Zhou, X.
AU  - Gao, M.
AU  - Liu, R. M.
PY  - 2022
PD  - 
N2  - With the advent of the Big Data era, the specialized data in the kill chain domain has increased dramatically, and the engine-based method of retrieving information can hardly meet the users' need for more accurate answers. The kill chain domain includes four components: control equipment, sensor equipment, strike equipment (weapon and platform), and evaluator equipment, as well as related data which contain a large amount of valuable information such as the parameter information contained in each component. If these fragmented and confusing data are integrated and effective query methods are established, they can help professionals complete the military kill chain knowledge system. The knowledge system constructed in this paper is based on the Neo4j graph database and the US Command simulation system to establish a target-oriented knowledge map of kill chain, aiming to provide data support for the Q&A system. Secondly, in order to facilitate the query, this paper establishes entity and relationship/attribute mining based on the continuous bag-of-words (CBOW) encoding model, bidirectional long short-term memory-conditional random field (BiLSTM-CRF) named entity model, and bidirectional gated recurrent neural network (BiGRU) intent recognition model for Chinese kill chain question and answer; returns the corresponding entity or attribute values in combination with the knowledge graph triad form; and finally constructs the answer return. The constructed knowledge map of the kill chain contains 2767 items (including sea, land, and air), and the number of parameters involved is 30124. The number of model parameters of the deep learning network is 27.9 M for the Q&A system built this time, and the accuracy rate is 85.5% after 200 simulated queries.
JO  - Mob. Inf. Syst.
PB  - 
CY  - 
VL  - 2022
IS  - 
PG  - 11
SP  - 11
EP  - 
AN  - WOS:000831948000004
DO  - 10.1155/2022/7097385
UR  - <Go to ISI>://WOS:000831948000004
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Multivariate graph neural networks on enhancing syntactic and semantic for aspect-based sentiment analysis
AU  - Wang, H. Y.
AU  - Qiu, X. H.
AU  - Tan, X. Y.
PY  - 2024
PD  - 
N2  - Aspect-based sentiment analysis (ABSA) aims to predict sentiment orientations towards textual aspects by extracting insights from user comments. While pretrained large language models (LLMs) demonstrate proficiency in sentiment analysis, incorporating syntactic and semantic features into ABSA remains a challenge. Additionally, employing LLMs for sentiment analysis often requires significant computational resources, rendering them impractical for use by individuals or small-scale entities. To address this, we propose the semiotic signal integration network (SSIN), which effectively combines syntactic and semantic features. The core syncretic information network leverages isomorphism and syntax to enhance knowledge acquisition. The semantically guided syntactic attention module further enables integrated semiotic representations via sophisticated attention mechanisms. Experiments on the publicly available SemEval dataset show that SSIN performs better than existing state-of-the-art ABSA baselines and LLMs such as Llama and Alpaca with high accuracy and macro-F1 scores. Moreover, our model demonstrates exceptional interpretability and the ability to discern both positive and negative sentiments, which is vitally important for real-world applications such as social media monitoring, health care, and customer service. Code is available at https://github.com/AmbitYuki/SSIN.
JO  - Appl. Intell.
PB  - 
CY  - 
VL  - 54
IS  - 22
PG  - 11672-11689
SP  - 11672
EP  - 11689
AN  - WOS:001302333000003
DO  - 10.1007/s10489-024-05802-6
UR  - <Go to ISI>://WOS:001302333000003
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Neural memory plasticity for medical anomaly detection
AU  - Fernando, T.
AU  - Denman, S.
AU  - Ahmedt-Aristizabal, D.
AU  - Sridharan, S.
AU  - Laurens, K. R.
AU  - Johnston, P.
AU  - Fookes, C.
PY  - 2020
PD  - 
N2  - In the domain of machine learning, Neural Memory Networks (NMNs) have recently achieved impressive results in a variety of application areas including visual question answering, trajectory prediction, object tracking, and language modelling. However, we observe that the attention based knowledge retrieval mechanisms used in current NMNs restrict them from achieving their full potential as the attention process retrieves information based on a set of static connection weights. This is suboptimal in a setting where there are vast differences among samples in the data domain; such as anomaly detection where there is no consistent criteria for what constitutes an anomaly. In this paper, we propose a plastic neural memory access mechanism which exploits both static and dynamic connection weights in the memory read, write and output generation procedures. We demonstrate the effectiveness and flexibility of the proposed memory model in three challenging anomaly detection tasks in the medical domain: abnormal EEG identification, MRI tumour type classification and schizophrenia risk detection in children. In all settings, the proposed approach outperforms the current state-of-the-art. Furthermore, we perform an in-depth analysis demonstrating the utility of neural plasticity for the knowledge retrieval process and provide evidence on how the proposed memory model generates sparse yet informative memory outputs. (C) 2020 Elsevier Ltd. All rights reserved.
JO  - Neural Netw.
PB  - 
CY  - 
VL  - 127
IS  - 
PG  - 67-81
SP  - 67
EP  - 81
AN  - WOS:000536453100008
DO  - 10.1016/j.neunet.2020.04.011
UR  - <Go to ISI>://WOS:000536453100008
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - A Novel Named Entity Recognition Algorithm for Hot Strip Rolling Based on BERT-Imseq2seq-CRF Model
AU  - Jing, F. W.
AU  - Zhang, M. Y.
AU  - Li, J.
AU  - Xu, G. Z.
AU  - Wang, J.
PY  - 2022
PD  - 
N2  - Named entity recognition is not only the first step of text information extraction, but also the key process of constructing domain knowledge graphs. In view of the large amount of text data, complex process flow and urgent application needs in the hot strip rolling process, a novel named entity recognition algorithm based on BERT-Imseq2seq-CRF model is proposed in this paper. Firstly, the algorithm uses the BERT preprocessing language model to mine the dependencies in the domain text and obtain the corresponding representation vector. Then, the representation vector is sent to the encoder layer, and the output vector is input to the decoder at the same time, on the premise that the original model only considers the semantic vector. The Teacher-Forcing mechanism is integrated into the decoder layer to randomly modify the labeling results, and error accumulation is avoided to guarantee the sequence recognition effect. Finally, the validity of the labeling results is checked according to the conditional random field constraints, and the overall labeling quality of the algorithm is improved. The experimental results show that this model can efficiently and accurately predict the physical label of hot strip rolling, and the model performance index is better than other models, with the F1-Score reaching 91.47%. This model further provides technical support for information extraction and domain knowledge graph construction of hot strip rolling.
JO  - Appl. Sci.-Basel
PB  - 
CY  - 
VL  - 12
IS  - 22
PG  - 13
SP  - 13
EP  - 
AN  - WOS:000887150700001
DO  - 10.3390/app122211418
UR  - <Go to ISI>://WOS:000887150700001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - On Contrasting YAGO with GPT-J: An Experiment for Person-Related Attributes
AU  - Martin-Moncunill, D.
AU  - Sicilia, M. A.
AU  - Gonzlez, L.
AU  - Rodrguez, D.
PY  - 2022
PD  - 
N2  - Language models (LMs) trained or large text corpora have demonstrated their superior performance in different language related tasks in the last years. These models automatically implicitly incorporate factual knowledge that can be used to complement existing Knowledge Graphs (KGs) that in most cases are structured from human curated databases. Here we report an experiment that attempts to gain insights about the extent to which LMs can generate factual information as that present in KGs. Concretely, we have tested such process using the English Wikipedia subset of YAGO and the GPT-J model for attributes related to individuals. Results show that the generation of correct factual information depends on the generation parameters of the model and are unevenly balanced across diverse individuals. Further, the LM can be used to populate further factual information, but it requires intermediate parsing to correctly map to KG attributes.
JO  - 4th Iberoamerican Conference and 3rd Indo-American Conference Knowledge Graphs and Semantic Web Conference (KGSWC)
PB  - Springer International Publishing Ag
CY  - Madrid, SPAIN
VL  - 1686
IS  - 
PG  - 234-245
SP  - 234
EP  - 245
AN  - WOS:000921164700017
DO  - 10.1007/978-3-031-21422-6_17
UR  - <Go to ISI>://WOS:000921164700017
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Ontology-Based BERT Model for Automated Information Extraction from Geological Hazard Reports
AU  - Ma, K.
AU  - Tian, M.
AU  - Tan, Y. J.
AU  - Qiu, Q. J.
AU  - Xie, Z.
AU  - Huang, R.
PY  - 2023
PD  - 
N2  - Geological knowledge can provide support for knowledge discovery, knowledge inference and mineralization predictions of geological big data. Entity identification and relationship extraction from geological data description text are the key links for constructing knowledge graphs. Given the lack of publicly annotated datasets in the geology domain, this paper illustrates the construction process of geological entity datasets, defines the types of entities and interconceptual relationships by using the geological entity concept system, and completes the construction of the geological corpus. To address the shortcomings of existing language models (such as Word2vec and Glove) that cannot solve polysemous words and have a poor ability to fuse contexts, we propose a geological named entity recognition and relationship extraction model jointly with Bidirectional Encoder Representation from Transformers (BERT) pretrained language model. To effectively represent the text features, we construct a BERT- bidirectional gated recurrent unit network (BiGRU)-conditional random field (CRF)-based architecture to extract the named entities and the BERT-BiGRU-Attention-based architecture to extract the entity relations. The results show that the F1-score of the BERT-BiGRU-CRF named entity recognition model is 0.91 and the F1-score of the BERT-BiGRU-Attention relationship extraction model is 0.84, which are significant performance improvements when compared to classic language models (e.g., word2vec and Embedding from Language Models (ELMo)).
JO  - J. Earth Sci.
PB  - 
CY  - 
VL  - 34
IS  - 5
PG  - 1390-1405
SP  - 1390
EP  - 1405
AN  - WOS:001087592500007
DO  - 10.1007/s12583-022-1724-z
UR  - <Go to ISI>://WOS:001087592500007
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - PMET: Precise Model Editing in a Transformer
AU  - Li, X. P.
AU  - Li, S. S.
AU  - Song, S. Z.
AU  - Yang, J.
AU  - Ma, J.
AU  - Yu, J.
PY  - 2024
PD  - 
N2  - Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we introduce PMET, which simultaneously optimizes Transformer Component (TC, namely MHSA and FFN) hidden states, while only using the optimized TC hidden states of FFN to precisely update FFN weights. Our experiments demonstrate that PMET exhibits state-of-the-art performance on both the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the effectiveness of our enhancements, further reinforcing the finding that the MHSA encodes certain general knowledge extraction patterns and indicating its storage of a small amount of factual knowledge. Our code is available at \url{https://github.com/xpq-tech/PMET}.
JO  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
PB  - Assoc Advancement Artificial Intelligence
CY  - Vancouver, CANADA
VL  - 
IS  - 
PG  - 18564-18572
SP  - 18564
EP  - 18572
AN  - WOS:001239407300008
DO  - 
UR  - <Go to ISI>://WOS:001239407300008
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Pre-training of Graph Augmented Transformers for Medication Recommendation
AU  - Shang, J. Y.
AU  - Ma, T. F.
AU  - Xiao, C.
AU  - Sun, J. M.
PY  - 2019
PD  - 
N2  - Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.
JO  - 28th International Joint Conference on Artificial Intelligence
PB  - Ijcai-Int Joint Conf Artif Intell
CY  - Macao, PEOPLES R CHINA
VL  - 
IS  - 
PG  - 5953-5959
SP  - 5953
EP  - 5959
AN  - WOS:000761735106014
DO  - 
UR  - <Go to ISI>://WOS:000761735106014
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Probing Pretrained Language Models for Lexical Semantics
AU  - Vulie, I.
AU  - Ponti, E. M.
AU  - Litschko, R.
AU  - Glava, G.
AU  - Korhonen, A.
AU  - Assoc Computat, Linguist
PY  - 2020
PD  - 
N2  - The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.
JO  - Conference on Empirical Methods in Natural Language Processing (EMNLP)
PB  - Assoc Computational Linguistics-Acl
CY  - Electr Network
VL  - 
IS  - 
PG  - 7222-7240
SP  - 7222
EP  - 7240
AN  - WOS:000855160707033
DO  - 
UR  - <Go to ISI>://WOS:000855160707033
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Prompt-based Zero-shot Text Classification with Conceptual Knowledge
AU  - Wang, Y. Q.
AU  - Wang, W.
AU  - Chen, Q.
AU  - Huang, K. Z.
AU  - Nguyen, A.
AU  - De, S.
PY  - 2023
PD  - 
N2  - In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings.
JO  - 61st Annual Meeting of the Association-for-Computational-Linguistics / Student Research Workshop (ACL-SRW)
PB  - Assoc Computational Linguistics-Acl
CY  - Toronto, CANADA
VL  - 
IS  - 
PG  - 30-38
SP  - 30
EP  - 38
AN  - WOS:001181053700003
DO  - 
UR  - <Go to ISI>://WOS:001181053700003
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language
AU  - Faghihi, H. R.
AU  - Nafar, A.
AU  - Uszok, A.
AU  - Karimian, H.
AU  - Kordjamshidi, P.
PY  - 2024
PD  - 
N2  - This paper presents a conversational pipeline for crafting domain knowledge for complex neuro-symbolic models through natural language prompts. It leverages large language models to generate declarative programs in the DomiKnowS framework. The programs in this framework express concepts and their relationships as a graph in addition to logical constraints between them. The graph, later, can be connected to trainable neural models according to those specifications. Our proposed pipeline utilizes techniques like dynamic in-context demonstration retrieval, model refinement based on feedback from a symbolic parser, visualization, and user interaction to generate the tasks' structure and formal knowledge representation. This approach empowers domain experts, even those not well-versed in ML/AI, to formally declare their knowledge to be incorporated in customized neural models in the DomiKnowS framework.
JO  - 18th International Conference on Neural-Symbolic Learning and Reasoning (NeSy)
PB  - Springer International Publishing Ag
CY  - Barcelona, SPAIN
VL  - 14980
IS  - 
PG  - 315-327
SP  - 315
EP  - 327
AN  - WOS:001329993800025
DO  - 10.1007/978-3-031-71170-1_25
UR  - <Go to ISI>://WOS:001329993800025
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering
AU  - Yasunaga, M.
AU  - Ren, H. Y.
AU  - Bosselut, A.
AU  - Liang, P.
AU  - Leskovec, J.
AU  - Assoc Computat, Linguist
PY  - 2021
PD  - 
N2  - The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and Open BookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correct iv handling negation in questions.
JO  - Conference of the North-American-Chapter of the Association-for-Computational-Linguistics - Human Language Technologies (NAACL-HLT)
PB  - Assoc Computational Linguistics-Acl
CY  - Electr Network
VL  - 
IS  - 
PG  - 535-546
SP  - 535
EP  - 546
AN  - WOS:000895685600043
DO  - 
UR  - <Go to ISI>://WOS:000895685600043
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Relational World Knowledge Representation in Contextual Language Models: A Review
AU  - Safavi, T.
AU  - Koutra, D.
AU  - Assoc Computat, Linguist
PY  - 2021
PD  - 
N2  - Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.
JO  - Conference on Empirical Methods in Natural Language Processing (EMNLP)
PB  - Assoc Computational Linguistics-Acl
CY  - Punta Cana, DOMINICAN REP
VL  - 
IS  - 
PG  - 1053-1067
SP  - 1053
EP  - 1067
AN  - WOS:000855966301014
DO  - 
UR  - <Go to ISI>://WOS:000855966301014
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding
AU  - Zhang, D.
AU  - Rong, Z.
AU  - Xue, C. Y.
AU  - Li, G. Y.
PY  - 2024
PD  - 
N2  - Knowledge graphs serve as a pivotal framework for the structured representation of information regarding entities and relations. However, in the real world, these knowledge graphs are often incomplete and harboring missing facts. Knowledge graph completion (KGC) has emerged as a central research focus, entailing the automated prediction of these missing facts and garnering substantial scholarly attention in recent years. Text -based knowledge graph embedding methods have demonstrated considerable potential for tackling the challenges associated with KGC by employing pre -trained language models. However, their limitation lies in the lack of logical features, which constrains the efficacy of capturing intricate patterns within knowledge graphs. This paper proposed SimRE, a straightforward contrastive learning framework augmented with soft logic rules. SimRE introduces a self -supervised framework that leverages the input rule bodies to predict the corresponding rule heads through a contrastive objective. We introduced two rule sampling techniques to enhance the efficiency and accuracy of the model: in -batch rule negatives and pre -batch rule negatives. SimRE employs a simple method for integrating logical features with the text -based model. The experimental results on benchmark datasets demonstrate that the proposed approach outperforms state-of-the-art methods.
JO  - Inf. Sci.
PB  - 
CY  - 
VL  - 661
IS  - 
PG  - 14
SP  - 14
EP  - 
AN  - WOS:001173850500001
DO  - 10.1016/j.ins.2023.120069
UR  - <Go to ISI>://WOS:001173850500001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - SPOT: Knowledge-Enhanced Language Representations for Information Extraction
AU  - Li, J. C.
AU  - Katsis, Y.
AU  - Baldwin, T.
AU  - Kim, H. C.
AU  - Bartko, A.
AU  - McAuley, J.
AU  - Hsu, C. N.
AU  - Acm
PY  - 2022
PD  - 
N2  - Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e., relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.
JO  - 31st ACM International Conference on Information and Knowledge Management (CIKM)
PB  - Assoc Computing Machinery
CY  - Atlanta, GA
VL  - 
IS  - 
PG  - 1124-1134
SP  - 1124
EP  - 1134
AN  - WOS:001074639601012
DO  - 10.1145/3511808.3557459
UR  - <Go to ISI>://WOS:001074639601012
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations
AU  - Sinclair, A.
AU  - Jumelet, J.
AU  - Zuidema, W.
AU  - Fernndez, R.
PY  - 2022
PD  - 
N2  - We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model's internal states.(1)
JO  - Trans. Assoc. Comput. Linguist.
PB  - 
CY  - 
VL  - 10
IS  - 
PG  - 1031-1050
SP  - 1031
EP  - 1050
AN  - WOS:000923422400003
DO  - 10.1162/tacl_a_00504
UR  - <Go to ISI>://WOS:000923422400003
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Surgicberta: a pre-trained language model for procedural surgical language
AU  - Bombieri, M.
AU  - Rospocher, M.
AU  - Ponzetto, S. P.
AU  - Fiorini, P.
PY  - 2024
PD  - 
N2  - Pre-trained language models are now ubiquitous in natural language processing, being successfully applied for many different tasks and in several real-world applications. However, even though there is a wealth of high-quality written materials on surgery, and the scientific community has shown a growing interest in the application of natural language processing techniques in surgery, a pre-trained language model specific to the surgical domain is still missing. The creation and public release of such a model would serve numerous useful clinical applications. For example, it could enhance existing surgical knowledge bases employed for task automation, or assist medical students in summarizing complex surgical descriptions. For this reason, in this paper, we introduce SurgicBERTa, a pre-trained language model specific for the English surgical language, i.e., the language used in the surgical domain. SurgicBERTa has been obtained from RoBERTa through continued pre-training with the Masked language modeling objective on 300 k sentences taken from English surgical books and papers, for a total of 7 million words. By publicly releasing SurgicBERTa, we make available a resource built from the content collected in many high-quality surgical books, online textual resources, and academic papers. We performed several assessments in order to evaluate SurgicBERTa, comparing it with the general domain RoBERTa. First, we intrinsically assessed the model in terms of perplexity, accuracy, and evaluation loss resulting from the continual training according to the masked language modeling task. Then, we extrinsically evaluated SurgicBERTa on several downstream tasks, namely (i) procedural sentence detection, (ii) procedural knowledge extraction, (iii) ontological information discovery, and (iv) surgical terminology acquisition. Finally, we conducted some qualitative analysis on SurgicBERTa, showing that it contains a lot of surgical knowledge that could be useful to enrich existing state-of-the-art surgical knowledge bases or to extract surgical knowledge. All the assessments show that SurgicBERTa better deals with surgical language than a general-purpose pre-trained language model such as RoBERTa, and therefore can be effectively exploited in many computer-assisted applications in the surgical domain.
JO  - Int. J, Data Sci. Anal.
PB  - 
CY  - 
VL  - 18
IS  - 1
PG  - 69-81
SP  - 69
EP  - 81
AN  - WOS:001050945700001
DO  - 10.1007/s41060-023-00433-5
UR  - <Go to ISI>://WOS:001050945700001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Utilizing Language Models to Expand Vision-Based Commonsense Knowledge Graphs
AU  - Rezaei, N.
AU  - Reformat, M. Z.
PY  - 2022
PD  - 
N2  - The introduction and ever-growing size of the transformer deep-learning architecture have had a tremendous impact not only in the field of natural language processing but also in other fields. The transformer-based language models have contributed to a renewed interest in commonsense knowledge due to the abilities of deep learning models. Recent literature has focused on analyzing commonsense embedded within the pre-trained parameters of these models and embedding missing commonsense using knowledge graphs and fine-tuning. We base our current work on the empirically proven language understanding of very large transformer-based language models to expand a limited commonsense knowledge graph, initially generated only on visual data. The few-shot-prompted pre-trained language models can learn the context of an initial knowledge graph with less bias than language models fine-tuned on a large initial corpus. It is also shown that these models can offer new concepts that are added to the vision-based knowledge graph. This two-step approach of vision mining and language model prompts results in the auto-generation of a commonsense knowledge graph well equipped with physical commonsense, which is human commonsense gained by interacting with the physical world. To prompt the language models, we adapted the chain-of-thought method of prompting. To the best of our knowledge, it is a novel contribution to the domain of the generation of commonsense knowledge, which can result in a five-fold cost reduction compared to the state-of-the-art. Another contribution is assigning fuzzy linguistic terms to the generated triples. The process is end to end in the context of knowledge graphs. It means the triples are verbalized to natural language, and after being processed, the results are converted back to triples and added to the commonsense knowledge graph.
JO  - Symmetry-Basel
PB  - 
CY  - 
VL  - 14
IS  - 8
PG  - 20
SP  - 20
EP  - 
AN  - WOS:000845176200001
DO  - 10.3390/sym14081715
UR  - <Go to ISI>://WOS:000845176200001
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - WERECE: An Unsupervised Method for Educational Concept Extraction Based on Word Embedding Refinement
AU  - Huang, J. X.
AU  - Ding, R. F.
AU  - Wu, X. M.
AU  - Chen, S. M.
AU  - Zhang, J. L.
AU  - Liu, L. X.
AU  - Zheng, Y. X.
PY  - 2023
PD  - 
N2  - The era of educational big data has sparked growing interest in extracting and organizing educational concepts from massive amounts of information. Outcomes are of the utmost importance for artificial intelligence-empowered teaching and learning. Unsupervised educational concept extraction methods based on pre-trained models continue to proliferate due to ongoing advances in semantic representation. However, it remains challenging to directly apply pre-trained large language models to extract educational concepts; pre-trained models are built on extensive corpora and do not necessarily cover all subject-specific concepts. To address this gap, we propose a novel unsupervised method for educational concept extraction based on word embedding refinement (i.e., word embedding refinement-based educational concept extraction (WERECE)). It integrates a manifold learning algorithm to adapt a pre-trained model for extracting educational concepts while accounting for the geometric information in semantic computation. We further devise a discriminant function based on semantic clustering and Box-Cox transformation to enhance WERECE's accuracy and reliability. We evaluate its performance on two newly constructed datasets, EDU-DT and EDUTECH-DT. Experimental results show that WERECE achieves an average precision up to 85.9%, recall up to 87.0%, and F1 scores up to 86.4%, which significantly outperforms baselines (TextRank, term frequency-inverse document frequency, isolation forest, K-means, and one-class support vector machine) on educational concept extraction. Notably, when WERECE is implemented with different parameter settings, its precision and recall sensitivity remain robust. WERECE also holds broad application prospects as a foundational technology, such as for building discipline-oriented knowledge graphs, enhancing learning assessment and feedback, predicting learning interests, and recommending learning resources.
JO  - Appl. Sci.-Basel
PB  - 
CY  - 
VL  - 13
IS  - 22
PG  - 20
SP  - 20
EP  - 
AN  - WOS:001118360300001
DO  - 10.3390/app132212307
UR  - <Go to ISI>://WOS:001118360300001
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models
AU  - Hao, S.
AU  - Tan, B.
AU  - Tang, K.
AU  - Ni, B.
AU  - Shao, X.
AU  - Zhang, H.
AU  - Xing, E. P.
AU  - Hu, Z.
PY  - 2023
PD  - 
N2  - It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations from different LMs. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., "A is capable of but not good at B"). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities.  2023 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 5000-5015
SP  - 5000
EP  - 5015
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171861045&partnerID=40&md5=eb1171194b6978bd9b807ac86ac88d32
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Beyond Classification: Financial Reasoning in State-of-the-Art Language Models
AU  - Son, G.
AU  - Jung, H.
AU  - Hahm, M.
AU  - Jin, S.
AU  - Na, K.
PY  - 2023
PD  - 
N2  - Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes. By analyzing the results, we reveal that the ability to generate coherent financial reasoning first emerges at 6B parameters, and continues to improve with better instruction-tuning or larger datasets. Additionally, the study provides a publicly accessible dataset named sFIOG (Synthetic-Financial Investment Opinion Generation), consisting of 11,802 synthetic investment thesis samples, to support further research in the field of financial reasoning. Overall, this research seeks to contribute to the understanding of the efficacy of language models in the field of finance, with a particular emphasis on their ability to engage in sophisticated reasoning and analysis within the context of investment decision-making. We release our models, dataset, and code1  FinNLP-Muffin 2023 - Joint Workshop of the 5th Financial Technology and Natural Language Processing and 2nd Multimodal AI For Financial Forecasting, in conjunction with IJCAI 2023 - Proceedings.
JO  - FinNLP-Muffin 2023 - Joint Workshop of the 5th Financial Technology and Natural Language Processing and 2nd Multimodal AI For Financial Forecasting, in conjunction with IJCAI 2023 - Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 34-44
SP  - 34
EP  - 44
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808998&partnerID=40&md5=be798f6afba112ccf8a55a2935b76e92
NS  - 
N1  - Ishan Tamrakar (2024-12-29 04:02:09)(Screen): financial knowledge boundary ; 
ER  - 

TY  - CONF
TI  - Building knowledge graph using pre-trained language model for learning entity-aware relationships
AU  - Kumar, A.
AU  - Pandey, A.
AU  - Gadia, R.
AU  - Mishra, M.
PY  - 2020
PD  - 
N2  - Relations exhibited among entities from textual content can be a potential source of information for any business domain. This paper encompasses a wholesome approach to mine entity-relation and building knowledge graph from textual documents. The paper concentrates on two approaches to classify directional entity relations. We build on extending pretrained language model i.e. BERT for text classification along-side providing entity and directionality information as input making it entity-aware BERT classifier. We also did ablation studies of presented model in terms of various ways of providing entity information on the learning capabilities of model. We demonstrate the end to end pipeline for building an entity-relation extraction system in a business application. The techniques proposed in the paper are also evaluated against SemEval-2010 Task 8, a popular relation classification dataset. The experimental results demonstrate that learning entity-aware relations through language models outperforms almost all the previous state-of-the-art (SOTA) models.  2020 IEEE.
JO  - 2020 IEEE International Conference on Computing, Power and Communication Technologies, GUCON 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 310-315
SP  - 310
EP  - 315
AN  - 
DO  - 10.1109/GUCON48875.2020.9231227
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096601581&doi=10.1109%2fGUCON48875.2020.9231227&partnerID=40&md5=bf7d199155bd226e4a030188be0c9d66
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - C5: toward better conversation comprehension and contextual continuity for ChatGPT
AU  - Liang, P.
AU  - Ye, D.
AU  - Zhu, Z.
AU  - Wang, Y.
AU  - Xia, W.
AU  - Liang, R.
AU  - Sun, G.
PY  - 2024
PD  - 
N2  - Large language models (LLMs), such as ChatGPT, have demonstrated outstanding performance in various fields, particularly in natural language understanding and generation tasks. In complex application scenarios, users tend to engage in multi-turn conversations with ChatGPT to keep contextual information and obtain comprehensive responses. However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users conversation comprehension and contextual continuity for ChatGPT. To address these challenges, we propose an interactive conversation visualization system called C5, which includes Global View, Topic View, and Context-associated Q&A View. The Global View uses the GitLog diagram metaphor to represent the conversation structure, presenting the trend of conversation evolution and supporting the exploration of locally salient features. The Topic View is designed to display all the question and answer nodes and their relationships within a topic using the structure of a knowledge graph, thereby display the relevance and evolution of conversations. The Context-associated Q&A View consists of three linked views, which allow users to explore individual conversations deeply while providing specific contextual information when posing questions. The usefulness and effectiveness of C5 were evaluated through a case study and a user study.  The Visualization Society of Japan 2024.
JO  - J. Visual.
PB  - 
CY  - 
VL  - 27
IS  - 4
PG  - 713-730
SP  - 713
EP  - 730
AN  - 
DO  - 10.1007/s12650-024-00980-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189615043&doi=10.1007%2fs12650-024-00980-4&partnerID=40&md5=c6ad6e6301dce0517ef30beddca4a31f
NS  - 
N1  - Ishan Tamrakar (2025-01-09 05:35:21)(Screen): KR through KGs; 
ER  - 

TY  - CONF
TI  - Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks
AU  - Qiu, Y.
AU  - Hu, J.
AU  - Zhang, Q.
AU  - Yin, H.
PY  - 2024
PD  - 
N2  - Recent advancements in large language models (LLMs) have exhibited promising capabilities in addressing various tasks such as defect detection and program repair. Despite their prevalence, LLMs still face limitations in effectively handling these tasks. Common strategies to adapt them and improve their performance for specific tasks involve fine-tuning models based on user data or employing in-context learning with examples of desired inputs and outputs. However, they pose challenges for practical adoption due to the need for extensive computational resources, high-quality data, and continuous maintenance. Furthermore, neither strategy can explain or reason about the deficiencies of LLMs in the given tasks. We propose Calico to address the high cost of fine-tuning, eliminate the necessity for task-specific examples, and provide explanations of LLM deficiency. At the heart of Calico is an evolutionary approach that interleaves knowledge calibration and AI deficiency diagnosis. The key essence of Calico is as follows. First, it focuses on identifying knowledge gaps in LLMs' program comprehension. Second, it conducts automated code refactoring to integrate the overlooked knowledge into the source code for mitigating those gaps. Third, it employs what-if analysis and counterfactual reasoning to determine a minimum set of overlooked knowledge necessary to improve the performance of LLMs in code tasks. We have extensively evaluated Calico over 8,938 programs on three most commonly seen code tasks. Our experimental results show that vanilla ChatGPT cannot fully understand code structures. With knowledge calibration, Calico improves it by 20% and exhibits comparable proficiency compared to fine-tuned LLMs. Deficiency diagnosis contributes to 8% reduction in program sizes while ensuring performance. These impressive results demonstrate the feasibility of utilizing a vanilla LLM for automated software engineering (SE) tasks, thereby avoiding the high computational costs associated with a fine-tuned model.  2024 Owner/Author.
JO  - ISSTA 2024 - Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis
PB  - Association for Computing Machinery, Inc
CY  - 
VL  - 
IS  - 
PG  - 1785-1797
SP  - 1785
EP  - 1797
AN  - 
DO  - 10.1145/3650212.3680399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205539673&doi=10.1145%2f3650212.3680399&partnerID=40&md5=d6253fd11b8f055e88aa8e31fa56ae98
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models
AU  - Wang, C.
AU  - Li, J.
AU  - Chen, Y.
AU  - Liu, K.
AU  - Zhao, J.
PY  - 2022
PD  - 
N2  - Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language models has recently received attention, yet it has not been explored in non-English languages. In this paper, we propose a large-scale Chinese CKG generated from multilingual PLMs, named as CN-AutoMIC, aiming to fill the research gap of non-English CKGs. To improve the efficiency, we propose generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we develop cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping iteration process to reuse generated results. Finally, we conduct detailed analyses on CN-AutoMIC from different aspects. Empirical results show the proposed CKG has high quality and diversity, surpassing the direct translation version of similar English CKGs. We also find some interesting deficiency patterns and differences between relations, which reveal pending problems in commonsense knowledge generation. We share the resources and related models for further study.  2022 Association for Computational Linguistics.
JO  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 9253-9265
SP  - 9253
EP  - 9265
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149442496&partnerID=40&md5=64842f8e676c773c8796f0be985b0743
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models
AU  - Lv, Q.
AU  - Wang, J.
AU  - Chen, H.
AU  - Li, B.
AU  - Zhang, Y.
AU  - Wu, F.
PY  - 2024
PD  - 
N2  - Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM)-which enhances models with up-to-date knowledge-emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel COarse-to-Fine highlighTing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: recaller, scorer, and selector. First, recaller applies a knowledge graph to extract potential key entities in a given context. Second, scorer measures the importance of each entity by calculating its contextual weight. Finally, selector selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on the knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over 30% in the F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering. Copyright 2024 by the author(s)
JO  - Proceedings of Machine Learning Research
PB  - ML Research Press
CY  - 
VL  - 235
IS  - 
PG  - 33594-33623
SP  - 33594
EP  - 33623
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203842300&partnerID=40&md5=f92b546fdd0759297c19814d978aa5ba
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Constructing Knowledge Graph for Electricity Keywords Based on Large Language Model
AU  - Zhao, H.
AU  - Jiang, W.
AU  - Deng, J.
AU  - Ren, Q.
AU  - Zhang, L.
PY  - 2023
PD  - 
N2  - In the information age, the electric power industry, as a crucial pillar of modern society, has accumulated a wealth of valuable research literature. Knowledge graph technology offers the potential to tap into this knowledge repository, providing a better understanding of research outcomes in the electric power domain. However, due to the diversity and complexity of knowledge in the power industry, it is difficult to build a comprehensive and complete knowledge graph of power keywords. In recent years, large language models (LLMs) have made significant advancements. This paper harnesses LLM technology along with text similarity analysis and co-occurrence frequency analysis to establish a comprehensive framework for processing keyword knowledge in the field of electric power. Within this framework, various forms of information found in electric power research can be processed. This includes creating a thesaurus of electric power domain keywords; obtaining the individual attributes implied by the information keywords in this thesaurus and their interconnections; and generating a knowledge graph of electric power domain keywords. This knowledge graph includes attributes, interpretations, relationships, and associated literature for keywords. It serves as a valuable reference for the effective utilization of research outcomes in the electric power domain.  2023 IEEE.
JO  - 2023 IEEE 7th Conference on Energy Internet and Energy System Integration, EI2 2023
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 4844-4849
SP  - 4844
EP  - 4849
AN  - 
DO  - 10.1109/EI259745.2023.10512525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194156503&doi=10.1109%2fEI259745.2023.10512525&partnerID=40&md5=2e88a32adee5f9484979c3da86f1a9b3
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Constructing Knowledge Graph from Cyber Threat Intelligence Using Large Language Model
AU  - Liu, J.
AU  - Zhan, J.
PY  - 2023
PD  - 
N2  - Cyber Threat Intelligence (CTI) reports are valuable resources in various applications but manually extracting information from them is time-consuming. Existing approaches for automating extraction require specialized models trained on a substantial corpus. In this paper, we present an efficient methodology for constructing knowledge graphs from CTI by leveraging the Large Language Model (LLM), using ChatGPT for instance. Our approach automatically extracts attack-related entities and their relationships, organizing them within a CTI knowledge graph. We evaluate our approach on 13 CTIs, demonstrating better performance compared to AttacKG and REBEL while requiring less manual intervention and computational resources. This proves the feasibility and suitability of our method in low-resource scenarios, specifically within the domain of cyber threat intelligence.  2023 IEEE.
JO  - Proceedings - 2023 IEEE International Conference on Big Data, BigData 2023
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 516-521
SP  - 516
EP  - 521
AN  - 
DO  - 10.1109/BigData59044.2023.10386611
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184980754&doi=10.1109%2fBigData59044.2023.10386611&partnerID=40&md5=4def110f5ab4d68c8fc274d50133d9b2
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Construction of Diabetes Knowledge Graph Based on Deep Learning
AU  - Lu, Y.
AU  - Zhao, R.
AU  - Huang, S.
AU  - Liu, R.
PY  - 2021
PD  - 
N2  - To integrate medical data which is scattered over the internet, natural language processing (NLP) is widely used in medical text mining. BERT (Bidirectional Encoder Representations from Transformers) is outstanding among many other representation models and vector representation based on Bert pre-Training language model can help the target task learn more semantic information. The knowledge graph intuitively reveals the relationship between entities and helps explore deeper semantic connections between entities. There are three important parts in the construction of a knowledge graph, including entity extraction, relation extraction, and graph generation. Based on these methods this paper proposes a Bert-based named entities identification model Bert-BiLSTM-CRF and it is outperforming the established methods. In the relation extraction part, use the BERT-Softmax to improve the semantic expression and its F1-value increased by 12 percent compared with the traditional entity relation extraction model. Based on the above redefined the entities of diabetes and their relationships to enrich the semantics of the knowledge graph. Finally, the Neo4j graph database was used to realize the visualization of the diabetes knowledge map.  2021 IEEE.
JO  - Proceedings - 2021 7th Annual International Conference on Network and Information Systems for Computers, ICNISC 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 966-970
SP  - 966
EP  - 970
AN  - 
DO  - 10.1109/ICNISC54316.2021.00181
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123162432&doi=10.1109%2fICNISC54316.2021.00181&partnerID=40&md5=c5c75329ba7336a74cf08fc4ee1cd750
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - COPEN: Probing Conceptual Knowledge in Pre-trained Language Models
AU  - Peng, H.
AU  - Wang, X.
AU  - Hu, S.
AU  - Jin, H.
AU  - Hou, L.
AU  - Li, J.
AU  - Liu, Z.
AU  - Liu, Q.
PY  - 2022
PD  - 
N2  - Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.  2022 Association for Computational Linguistics.
JO  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 5015-5035
SP  - 5015
EP  - 5035
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149436076&partnerID=40&md5=bf5d714486226b7b6f1558adc1dd4ecb
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion
AU  - Li, J.
AU  - Yu, H.
AU  - Luo, X.
AU  - Liu, Q.
PY  - 2024
PD  - 
N2  - Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance. 2024 Association for Computational Linguistics.
JO  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 1
IS  - 
PG  - 1669-1682
SP  - 1669
EP  - 1682
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199650503&partnerID=40&md5=939803ae362bee98b254e4c65ed6349b
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation
AU  - Fang, Y.
AU  - Zhang, Y.
PY  - 2022
PD  - 
N2  - Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor and fine-tuning it with 20% training data can improve the quality and accuracy of the generated explanations over multiple evaluation metrics. Compared with conventional methods that search concepts over knowledge graphs, our method does not require the preparation and training models to search through knowledge graphs. To better understand the results from pre-trained language models, we also designed a metric to evaluate the retrieved concepts. Through analysis and experiments, we show the correlation between this metric and the performance of the generators, and we also show the importance of attaching concepts for generating high-quality sentences.  2022 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2022
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 5912-5922
SP  - 5912
EP  - 5922
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149835618&partnerID=40&md5=7893f118859ad2a5e8397368699b6195
NS  - 
N1  - Ishan Tamrakar (2024-12-25 04:11:45)(Screen): some kind of KE is happening; 
ER  - 

TY  - CONF
TI  - Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs
AU  - Shiri, F.
AU  - Moghimifar, F.
AU  - Haffari, R.
AU  - Li, Y. F.
AU  - Nguyen, V.
AU  - Yoo, J.
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method's superior performance compared to baseline approaches.  2024 ISIF.
JO  - FUSION 2024 - 27th International Conference on Information Fusion
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 10.23919/FUSION59988.2024.10706385
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207692961&doi=10.23919%2fFUSION59988.2024.10706385&partnerID=40&md5=0c09281439668b007aa8c4613043f86c
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models
AU  - Wu, X.
AU  - Li, J.
AU  - Xu, M.
AU  - Dong, W.
AU  - Wu, S.
AU  - Bian, C.
AU  - Xiong, D.
PY  - 2023
PD  - 
N2  - Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach. 2023 Association for Computational Linguistics.
JO  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 2875-2886
SP  - 2875
EP  - 2886
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182223735&partnerID=40&md5=9e88daf6ebc67993017261a9543f0f79
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Dissecting Recall of Factual Associations in Auto-Regressive Language Models
AU  - Geva, M.
AU  - Bastings, J.
AU  - Filippova, K.
AU  - Globerson, A.
PY  - 2023
PD  - 
N2  - Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation queries the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing. 2023 Association for Computational Linguistics.
JO  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 12216-12235
SP  - 12216
EP  - 12235
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183356144&partnerID=40&md5=800ab1dde40e0332cec94ac1919aee15
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression
AU  - Bayat, F. F.
AU  - Liu, X.
AU  - Jagadish, H. V.
AU  - Wang, L.
PY  - 2024
PD  - 
N2  - Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the truthful directions previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model's internal knowledge only when it is confident. Our code is available at https://github.com/launchnlp/LITO.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 12388-12400
SP  - 12388
EP  - 12400
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205317192&partnerID=40&md5=74544429c6bc0e2e40a079de08dc6380
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Evaluating Prompt-Based Question Answering forObject Prediction intheOpen Research Knowledge Graph
AU  - DSouza, J.
AU  - Hrou, M.
AU  - Auer, S.
PY  - 2023
PD  - 
N2  - Recent investigations have explored prompt-based training of transformer language models for new text genres in low-resource settings. This approach has proven effective in transferring pre-trained or fine-tuned models to resource-scarce environments. This work presents the first results on applying prompt-based training to transformers for scholarly knowledge graph object prediction. Methodologically, it stands out in two main ways: 1) it deviates from previous studies that propose entity and relation extraction pipelines, and 2) it tests the method in a significantly different domain, scholarly knowledge, evaluating linguistic, probabilistic, and factual generalizability of large-scale transformer models. Our findings demonstrate that: i) out-of-the-box transformer models underperform on the new scholarly domain, ii) prompt-based training improves performance by up to 40% in relaxed evaluation, and iii) tests of the models in a distinct domain reveals a gap in capturing domain knowledge, highlighting the need for increased attention and resources in the scholarly domain for transformer models.  The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
PB  - Springer Science and Business Media Deutschland GmbH
CY  - 
VL  - 14146 LNCS
IS  - 
PG  - 508-515
SP  - 508
EP  - 515
AN  - 
DO  - 10.1007/978-3-031-39847-6_40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174710939&doi=10.1007%2f978-3-031-39847-6_40&partnerID=40&md5=438467ddc816151f60bcade994dbc82d
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text
AU  - Khorashadizadeh, H.
AU  - Mihindukulasooriya, N.
AU  - Tiwari, S.
AU  - Groppe, J.
AU  - Groppe, S.
PY  - 2023
PD  - 
N2  - Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text. In this paper, grounded by several research questions, we explore the capabilities of foundation models such as ChatGPT to generate knowledge graphs from the knowledge it captured during pre-training as well as the new text provided to it in the prompt. The paper provides a qualitative analysis of a set of example outputs generated by a foundation model with the aim of knowledge graph construction and completion. The results demonstrate promising capabilities. Furthermore, we discuss the challenges and next steps for this research work.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3447
IS  - 
PG  - 132-153
SP  - 132
EP  - 153
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168818600&partnerID=40&md5=f1d79f7173a4f5005baba416c9acc899
NS  - 
N1  - Ishan Tamrakar (2024-12-25 03:08:40)(Screen): External; 
ER  - 

TY  - CONF
TI  - Extracting Geographic Knowledge from Large Language Models: An Experiment
AU  - Salmas, K.
AU  - Pantazi, D. A.
AU  - Koubarakis, M.
PY  - 2023
PD  - 
N2  - We perform an experimental analysis of how the inner architecture of large language models behaves whilst extracting geographic knowledge. Our aim is to conclude on whether models actually incorporate geospatial information or simply follow statistical patterns in the data; hence to contribute to the research area of creating knowledge graphs from large language models. To achieve this, we study one specific geospatial relation and explore different techniques that leverage the masked language modeling abilities of BERT and RoBERTa. Our study should be construed as a stepping stone to the general study of the ways large language models encapsulate geospatial knowledge. In addition, it has allowed us to observe important points one should focus on when querying language models, which we discuss in detail.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3577
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179546460&partnerID=40&md5=0fd90a6d6edc446515886a7202960882
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Foundation Model for Biomedical Graphs: Integrating Knowledge Graphs and Protein Structures to Large Language Models
AU  - Kim, Y.
PY  - 2024
PD  - 
N2  - Transformer model has been a de-facto standard in natural language processing. Its adaptations in other fields such as computer vision showed promising results that this architecture is a powerful neural network in representation learning regardless of the data type. This recent success has led to research in multimodal Large Language Model (LLM), which enabled us to new types of tasks and applications with multiple data types. However, multimodal LLM in the biomedical domain is primarily limited to images, text, and/or sequence data. Here I propose to work on multimodal LLM architecture for biomedical graphs such as protein structure and chemical molecules. The research hypothesis is based on the fact that clinicians and researchers in computational biology and clinical research take advantage of various information for their decision-making process. Therefore, an AI model being able to handle multiple data types should boost its ability to use diverse knowledge for improved performances in clinical applications. 2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 4
IS  - 
PG  - 346-355
SP  - 346
EP  - 355
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204046637&partnerID=40&md5=708cbd849841b3f34bdebeca53329e3c
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - From Unstructured Data to Knowledge Graphs: An Application for Compliance Checking Problem
AU  - Karmakar, A.
AU  - Patel, C.
AU  - Kumar Delhi, V. S.
PY  - 2024
PD  - 
N2  - The rule requirements of a building code are frequently violated to create financially viable designs. These deviations are subjected to condonation by the municipal commissioner if recognizable hardships are faced. The historical concession applications for similar cases are stored in an unstructured manner, creating a barrier to knowledge transfer. The subjective statements given by applicants are composed of logical structure, language, and embedded knowledge that requires years of experience from the domain expert to decipher. A knowledge graph (KG) representation of the problem can capture concepts and represent them visually, which is easy for novice stakeholders to understand. A Large Language Model (LLM)-based method is used in this study for ontology extraction in the form of concepts and relationships. Also, unstructured input preprocessing and entity disambiguation were performed to evaluate the applicability of KG in this domain. The performance of the proposed method was checked qualitatively in a case study from real-life project examples. The limitations and scopes for improvements were also highlighted. The outcome of this study indicates KG as a potential candidate for knowledge generation from the unstructured archival data of compliance checking. The target audience for this application can be the new architects, reviewers, and programmers working on developing the end-to-end automated compliance checking systems. Finally, applying these Artificial Intelligence (AI)-based knowledge transfer mechanisms can ignite future research on automated concession applications and approvals, laying a path to the digital transformation of the industry.  2024 ISARC. All Rights Reserved.
JO  - Proceedings of the International Symposium on Automation and Robotics in Construction
PB  - International Association for Automation and Robotics in Construction (IAARC)
CY  - 
VL  - 
IS  - 
PG  - 863-871
SP  - 863
EP  - 871
AN  - 
DO  - 10.22260/ISARC2024/0112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199652264&doi=10.22260%2fISARC2024%2f0112&partnerID=40&md5=7feaa09bffdd81b1709a4cf4d3cd9b08
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Geotechnical Named Entity Recognition Based on BERT-BiGRU-CRF Model
AU  - Quanyu, W.
AU  - Li, Z.
AU  - Tu, Z.
AU  - Chen, G.
AU  - Hu, J.
AU  - Chen, J.
AU  - Lv, G.
PY  - 2023
PD  - 
N2  - Geotechnical engineering named entity recognition is an important prerequisite and the work foundation for geotechnical information mining and knowledge Graph. Aiming at the recognition and classification of named entities in geotechnical texts, this article first designs and constructs a named entity corpus of geotechnical engineering according to Standard for Fundamental Terms of Geotechnical Engineering (GB/T 50279-2014) and other national industry standards; and based on deep learning technologies, a named entity recognition and classification deep learning model GENER is proposed for geotechnical engineering text. In GENER, the distributed representation learning of geotechnical engineering text features is realized based on the BERT pretrained language model; the geotechnical engineering text context feature encoding is achieved based on the BiGRU context coding layer; and based on the label decoding layer of CRF, the context features are decoded to generate the label sequence of geotechnical engineering named entity. Finally, based on the geotechnical engineering corpus, the GENER model is experimentally analyzed. comparing with other deep learning models for named entity recognition based on pretrained language models, the GENER model has better performance. The precision reaches 90.94%, the recall reaches 92.88%, the F1 - score reaches 91.89%and model training speed increased by 4.735% respectively.Experiments show that compared with BiLSTM-CRF and CNN-BiLSTM-CRF models, this model is more effective in small-scale corpus geotechnical engineering entity recognition.  2023 China University of Geosciences. All rights reserved.
JO  - Diqiu Kexue Zhongguo Dizhi Daxue Xuebao
PB  - 
CY  - 
VL  - 48
IS  - 8
PG  - 3137-3150
SP  - 3137
EP  - 3150
AN  - 
DO  - 10.3799/dqkx.2022.462
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171573554&doi=10.3799%2fdqkx.2022.462&partnerID=40&md5=ae31e07e0eea89251253c5316d95c62f
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions
AU  - Zeng, Z.
AU  - Cheng, K. T.
AU  - Nanniyur, S. V.
AU  - Zhou, J.
AU  - Bhat, S.
PY  - 2023
PD  - 
N2  - Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 (Hwang et al., 2021) graph, converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training. 2023 Association for Computational Linguistics.
JO  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 14243-14264
SP  - 14243
EP  - 14264
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184812286&partnerID=40&md5=23c4bc18fa4642ce63ebca5720985705
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Improved Sequence Predictions using Knowledge Graph Embedding for Large Language Models
AU  - Khatun, R.
AU  - Sinhababu, N.
PY  - 2023
PD  - 
N2  - Large Language Models (LLM) have gained huge popularity recently due to their problem-solving capability in multiple domains. Technically LLMs can be considered a critical mixture of huge amounts of training data, smart and exhaustive prompt engineering, and word prediction models along with Reinforcement and Supervised learning mechanisms. Word prediction models are at the core of any Large Language Model. The latest word prediction techniques are sequential and transformer models. Transformers have overcome most of the drawbacks of sequential models with similar embedding knowledge. The literature survey shows little to no improvement in the embedding techniques. In this paper, we examined the existing word prediction models by replacing embedding models with an auto-engineered Knowledge Graph Embedding. This auto-engineered data representation shows drastic improvements in prediction quality. This mechanism also accelerates the prediction by providing more context information to the models with respect to the general embedding mechanism. Standard evaluation strategies are used to compare the model behavior.  2023 ACM.
JO  - ACM International Conference Proceeding Series
PB  - Association for Computing Machinery
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 10.1145/3639856.3639872
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194825117&doi=10.1145%2f3639856.3639872&partnerID=40&md5=09563cd7dfe5b5c0c04d6175db2cb32c
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - The Internal State of an LLM Knows When It's Lying
AU  - Azaria, A.
AU  - Mitchell, T.
PY  - 2023
PD  - 
N2  - While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71% to 83% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.  2023 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2023
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 967-976
SP  - 967
EP  - 976
AN  - 
DO  - 10.18653/v1/2023.findings-emnlp.68
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183291976&doi=10.18653%2fv1%2f2023.findings-emnlp.68&partnerID=40&md5=7597c7a305d62252637bcba9967b7cc9
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Investigating antiquities trafficking with generative pre-trained transformer (GPT)-3 enabled knowledge graphs: A case study
AU  - Graham, S.
AU  - Yates, D.
AU  - El-Roby, A.
PY  - 2023
PD  - 
N2  - Background: There is a wide variety of potential sources from which insight into the antiquities trade could be culled, from newspaper articles to auction catalogues, to court dockets, to personal archives, if it could all be systematically examined. We explore the use of a large language model, GPT-3, to semi-automate the creation of a knowledge graph of a body of scholarship concerning the antiquities trade. Methods: We give GPT-3 a prompt guiding it to identify knowledge statements around the trade. Given GPT-3s understanding of the statistical properties of language, our prompt teaches GPT-3 to append text to each article we feed it where the appended text summarizes the knowledge in the article. The summary is in the form of a list of subject, predicate, and object relationships, representing a knowledge graph. Previously we created such lists by manually annotating the source articles. We compare the result of this automatic process with a knowledge graph created from the same sources via hand. When such knowledge graphs are projected into a multi-dimensional embedding model using a neural network (via the Ampligraph open-source Python library), the relative positioning of entities implies the probability of a connection; the direction of the positioning implies the kind of connection. Thus, we can interrogate the embedding model to discover new probable relationships. The results can generate new insight about the antiquity trade, suggesting possible avenues of research. Results: We find that our semi-automatic approach to generating the knowledge graph in the first place produces comparable results to our hand-made version, but at an enormous savings of time and a possible expansion of the amount of materials we can consider. Conclusions: These results have implications for working with other kinds of archaeological knowledge in grey literature, reports, articles, and other venues via computational means. Copyright:  2023 Graham S et al.
JO  - Open. Res. Eur.
PB  - 
CY  - 
VL  - 3
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 10.12688/openreseurope.16003.1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166644502&doi=10.12688%2fopenreseurope.16003.1&partnerID=40&md5=8bdf4ffbf065bb69cae7f623cae181ed
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - An Investigation of LLMs' Inefficacy in Understanding Converse Relations
AU  - Qi, C.
AU  - Li, B.
AU  - Hui, B.
AU  - Wang, B.
AU  - Li, J.
AU  - Wu, J.
AU  - Laili, Y.
PY  - 2023
PD  - 
N2  - Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRe features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.  2023 Association for Computational Linguistics.
JO  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 6932-6953
SP  - 6932
EP  - 6953
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182435991&partnerID=40&md5=0039cab34f070f2cde3c6d1a32d220dd
NS  - 
N1  - Ishan Tamrakar (2024-12-26 10:34:15)(Screen): knowledge boundary for converse binary relation ; 
ER  - 

TY  - CONF
TI  - Know-Adapter: Towards Knowledge-Aware Parameter-Efficient Fine-Tuning for Few-shot Named Entity Recognition
AU  - Nie, B.
AU  - Shao, Y.
AU  - Wang, Y.
PY  - 2024
PD  - 
N2  - Parameter-Efficient Fine-Tuning (PEFT) is a promising approach to mitigate the challenges about the model adaptation of pretrained language models (PLMs) for the named entity recognition (NER) task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding explicit knowledge from external source like KGs to otherwise naive PEFTs. In this paper, we propose a novel knowledgeable adapter, Know-adapter, to incorporate structure and semantic knowledge of knowledge graphs into PLMs for few-shot NER. First, we construct a related KG entity type sequence for each sentence using a knowledge retriever. However, the type system of a domain-specific NER task is typically independent of that of current KGs and thus exhibits heterogeneity issue inevitably, which makes matching between the original NER and KG types (e.g. Person in NER potentially matches President in KBs) less likely, or introduces unintended noises. Thus, then we design a unified taxonomy based on KG ontology for KG entity types and NER labels. This taxonomy is used to build a learnable shared representation module, which provides shared representations for both KG entity type sequences and NER labels. Based on these shared representations, our Know-adapter introduces high semantic relevance knowledge and structure knowledge from KGs as inductive bias to guide the updating process of the adapter. Additionally, the shared representations guide the learnable representation module to reduce noise in the unsupervised expansion of label words. Extensive experiments on multiple NER datasets show the superiority of Know-Adapter over other state-of-the-art methods in both full-resource and low-resource settings.  2024 ELRA Language Resource Association: CC BY-NC 4.0.
JO  - 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings
PB  - European Language Resources Association (ELRA)
CY  - 
VL  - 
IS  - 
PG  - 9777-9786
SP  - 9777
EP  - 9786
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195978715&partnerID=40&md5=9181c25237412eab8459003172e399c4
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - KnowGraph@IITK at SemEval-2021 Task 11: Building Knowledge Graph for NLP Research
AU  - Shailabh, S.
AU  - Chaurasia, S.
AU  - Modi, A.
PY  - 2021
PD  - 
N2  - Research in Natural Language Processing is making rapid advances, resulting in the publication of a large number of research papers. Finding relevant research papers and their contribution to the domain is a challenging problem. In this paper, we address this challenge via the SemEval 2021 Task 11: NLPContributionGraph, by developing a system for a research paper contributions-focused knowledge graph over Natural Language Processing literature. The task is divided into three sub-tasks: extracting contribution sentences that show important contributions in the research article, extracting phrases from the contribution sentences, and predicting the information units in the research article together with triplet formation from the phrases. The proposed system is agnostic to the subject domain and can be applied for building a knowledge graph for any area. We found that transformer-based language models can significantly improve existing techniques and utilized the SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM) stacked on top of SciBERT model layers, while the second sub-task uses Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third sub-task uses a combined SciBERT based neural approach with heuristics for information unit prediction and triplet formation from the phrases. Our system achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase extraction testing and triplet extraction testing respectively.  2021 Association for Computational Linguistics.
JO  - SemEval 2021 - 15th International Workshop on Semantic Evaluation, Proceedings of the Workshop
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 467-477
SP  - 467
EP  - 477
AN  - 
DO  - 10.18653/v1/2021.semeval-1.57
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138911453&doi=10.18653%2fv1%2f2021.semeval-1.57&partnerID=40&md5=8b9d405d1a98928911751de91d512bfe
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Bases andLanguage Models: Complementing Forces
AU  - Suchanek, F.
AU  - Luu, A. T.
PY  - 2023
PD  - 
N2  - Large language models (LLMs), as a particular instance of generative artificial intelligence, have revolutionized natural language processing. In this invited paper, we argue that LLMs are complementary to structured data repositories such as databases or knowledge bases, which use symbolic knowledge representations. Hence, the two ways of knowledge representation will likely continue to co-exist, at least in the near future. We discuss ways that have been explored to make the two approaches work together, and point out opportunities and challenges for their symbiosis.  2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
JO  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
PB  - Springer Science and Business Media Deutschland GmbH
CY  - 
VL  - 14244 LNCS
IS  - 
PG  - 3-15
SP  - 3
EP  - 15
AN  - 
DO  - 10.1007/978-3-031-45072-3_1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175956982&doi=10.1007%2f978-3-031-45072-3_1&partnerID=40&md5=61d6be22425379858463a348dd4ee2b9
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Graph Completing with Dual Confrontation Learning Model based on Variational Information Bottleneck Method
AU  - Han, S.
AU  - Guan, Z.
AU  - Li, S.
AU  - Wang, J.
AU  - Zhou, X.
PY  - 2023
PD  - 
N2  - In natural language learning, pre-trained language models (PLM) can acquire rich knowledge and concepts from rich corpora, making it possible to use PLM-based models for knowledge graph completion (KGC) tasks. However, in previous research, when applying pre-trained models to knowledge graph completion tasks, two main challenges persist: (1) Existing knowledge graph completion models are typically evaluated based on the closed-world assumption(CWA), thus lacking evaluation methods suitable for the open-world assumption(OWA), which constitutes a significant challenge in the current field of knowledge graph completion. (2) Extracting useful information, reducing noise, and providing clear interpretability for extracting effective information from the extensive prior knowledge embedded in pre-trained language models is also a crucial issue. Although the loss function can reduce noise to a certain extent, from the perspective of information theory, only relying on the loss function has a limited effect on noise reduction, and the model needs more professional tools to reduce noise and reduce the impact of irrelevant information on model performance. To address the aforementioned challenges, we propose a dual confrontation learning model based on the variational information bottleneck method. This model restricts information flow and feature selection from the perspective of information theory to reduce noise and enhance model performance while providing clear interpretability for this process. Based on extensive experiments and comprehensive evaluations conducted under both closed-world and open-world assumptions, this model successfully extracts valuable knowledge from pre-trained language models to accomplish KGC tasks. Simultaneously, it minimizes noise, removes non-robust features, enhances model reliability, and optimizes model performance. More importantly, we offer a strong interpretability for the process in which our model constrains information flow to reduce noise.  2023 IEEE.
JO  - IEEE International Conference on Software Quality, Reliability and Security, QRS
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 741-750
SP  - 741
EP  - 750
AN  - 
DO  - 10.1109/QRS60937.2023.00077
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182513682&doi=10.1109%2fQRS60937.2023.00077&partnerID=40&md5=2c33a41028e90d106fb0d86c38de799a
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Knowledge Graph Completion Based on Contrastive Learning and Language Model-Enhanced Embedding
AU  - Zhang, H.
AU  - Li, L.
AU  - Yang, L.
AU  - San, C.
AU  - Yin, C.
AU  - Yan, B.
AU  - Yu, H.
AU  - Zhang, X.
PY  - 2024
PD  - 
N2  - A knowledge graph is a structured knowledge base comprising various types of knowledge or data units obtained through extraction and other processes. It is used to describe and represent information, such as entities, concepts, facts, and relationships. The limitations of Natural Language Processing(NLP) technology and the presence of noise in the texts of various knowledge or information units affect the accuracy of information extraction. Existing Knowledge Graph Completion(KGC) methods typically account for only single structural information or text semantic information, whereas the structural and text semantic information in the entire knowledge graph is disregarded. Hence, a KGC model based on contrastive learning and language model-enhanced embedding is proposed. The input entities and relationships are obtained using a pretrained language model to obtain the textual semantic information of the entities and relationships. The distance scoring function of the translation model is used to capture the structured information in the knowledge graph. Two negative sampling methods for contrastive learning are used to fuse contrastive learning to train the model to improve its ability to represent positive and negative samples. Experimental results show that compared with the Bidirectional Encoder Representations from Transformers for Knowledge Graph completion(KG-BERT) model, this model improves the average proportion of triple with ranking less than or equal to 10(Hits@10) indicator by 31% and 23% on the WN18RR and FB15K-237 datasets, respectively, thus demonstrating its superiority over other similar models.  2024, Editorial Office of Computer Engineering. All rights reserved.
JO  - Jisuanji Gongcheng
PB  - 
CY  - 
VL  - 50
IS  - 4
PG  - 168-176
SP  - 168
EP  - 176
AN  - 
DO  - 10.19678/j.issn.1000-3428.0067543
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195267989&doi=10.19678%2fj.issn.1000-3428.0067543&partnerID=40&md5=ed138fd8054943993346cc6a902bbacd
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge Graph-Enhanced Large Language Models via Path Selection
AU  - Liu, H.
AU  - Wang, S.
AU  - Zhu, Y.
AU  - Dong, Y.
AU  - Li, J.
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 6311-6321
SP  - 6311
EP  - 6321
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202595246&partnerID=40&md5=5a6be328d40f005265c89f1c8844e201
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models
AU  - Xu, R.
AU  - Cui, H.
AU  - Yu, Y.
AU  - Kan, X.
AU  - Shi, W.
AU  - Zhuang, Y.
AU  - Wang, M. D.
AU  - Jin, W.
AU  - Ho, J. C.
AU  - Yang, C.
PY  - 2024
PD  - 
N2  - Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, CLINGEN, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that CLINGEN consistently enhances performance across various tasks by 7.7%-8.7% on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances. Our code is available at https://github.com/ritaranx/ClinGen.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 15496-15523
SP  - 15496
EP  - 15523
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205322427&partnerID=40&md5=1082b39f199b0991afedde3e7a55fb81
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - LAL-JER: Label-Aware Learning for Adaptive Joint Entity and Relation Extraction with LLM data augmentation
AU  - He, M.
AU  - Bai, Y.
PY  - 2023
PD  - 
N2  - Joint entity and relation extraction has achieved great improvements in Natural Language Processing (NLP) and has been widely applied, such as constructing knowledge graph, query understanding and question answering. Existing methods usually spend long time on fitting the models on certain datasets with given label type, which greatly lacks the ability of generalization. The model cannot make prediction on label types that have not seen in the training set. To address this issue, we propose to use prompt to incorporate the semantic meaning of the label type description. Furthermore, we use large language model to perform data augmentation to improve the robustness of our model during training. Extensive experiments and ablation study on two joint entity and relation extraction validates the effectiveness of our work on that: 1. Our methods achieved states of art performance on joint entity and relation extraction benchmark based on pretrained language model bert. 2. Our methods can help the model make predictions on label type unseen before given prompts.  2023 ACM.
JO  - ACM International Conference Proceeding Series
PB  - Association for Computing Machinery
CY  - 
VL  - 
IS  - 
PG  - 414-419
SP  - 414
EP  - 419
AN  - 
DO  - 10.1145/3640912.3640993
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186957359&doi=10.1145%2f3640912.3640993&partnerID=40&md5=1cb3b848e14b5b8c1cc8e403db8c3ee8
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering
AU  - Su, H. T.
AU  - Niu, Y.
AU  - Lin, X.
AU  - Hsu, W. H.
AU  - Chang, S. F.
PY  - 2023
PD  - 
N2  - Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pretrained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., "what is someone doing...") and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like "why is someone doing...". Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., "score a goal"triggers "soccer player kicking ball") by prompting LM with the action (soccer player kicking ball) to retrieve the intention (to score a goal). CaKE-LM significantly outperforms conventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and Causal-VidQA datasets. We also conduct comprehensive analyses and provide key findings for future research.  2023 IEEE.
JO  - IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops
PB  - IEEE Computer Society
CY  - 
VL  - 2023-June
IS  - 
PG  - 4951-4960
SP  - 4951
EP  - 4960
AN  - 
DO  - 10.1109/CVPRW59228.2023.00523
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170827546&doi=10.1109%2fCVPRW59228.2023.00523&partnerID=40&md5=44d824c290b30dc62cfb072b8c5e8d6e
NS  - 
N1  - Ishan Tamrakar (2024-12-25 05:38:06)(Screen): proposes the causal KE framework for LMs; 
ER  - 

TY  - CONF
TI  - Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion
AU  - Gao, Y.
AU  - He, Y.
AU  - Kan, Z.
AU  - Han, Y.
AU  - Qiao, L.
AU  - Li, D.
PY  - 2023
PD  - 
N2  - Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the contextual nature of the TKG structure, i.e., query-specific subgraph contains both structural and temporal neighboring facts. This paper presents the SToKE, a novel method that employs the pre-trained language model (PLM) to learn joint Structural and Temporal Contextualized Knowledge Embeddings. Specifically, we first construct an event evolution tree (EET) for each query to enable PLMs to handle the TKG, which can be seen as a structured event sequence recording query-relevant structural and temporal contexts. We then propose a novel temporal embedding and structural matrix to learn the time information and structural dependencies of facts in EET. Finally, we formulate TKG completion as a mask prediction problem by masking the missing entity of the query to fine-tune pre-trained language models. Experimental results on three widely used datasets show the superiority of our model.  2023 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 417-430
SP  - 417
EP  - 430
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175468634&partnerID=40&md5=58ccb28e6556179cae64f3e6cb71e9e9
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation
AU  - Liang, Y.
AU  - Song, Z.
AU  - Wang, H.
AU  - Zhang, J.
PY  - 2024
PD  - 
N2  - We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge state probing. However, LLMs often fail to faithfully express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, DreamCatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.  2024 Association for Computational Linguistics.
JO  - KnowledgeNLP 2024 - 3rd Workshop on Knowledge Augmented Methods for NLP, Proceedings of the Workshop
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 44-58
SP  - 44
EP  - 58
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204897564&partnerID=40&md5=d52fb040363b9467f26bf637b0d79769
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - LINEARITY OF RELATION DECODING IN TRANSFORMER LANGUAGE MODELS
AU  - Hernandez, E.
AU  - Sharma, A. S.
AU  - Haklay, T.
AU  - Meng, K.
AU  - Wattenberg, M.
AU  - Andreas, J.
AU  - Belinkov, Y.
AU  - Bau, D.
PY  - 2024
PD  - 
N2  - Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in LMs.  2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.
JO  - 12th International Conference on Learning Representations, ICLR 2024
PB  - International Conference on Learning Representations, ICLR
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200539786&partnerID=40&md5=d3b9a4f70acbbdbb59f7cdf5bf801cfa
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion
AU  - Li, Q.
AU  - Chen, Z.
AU  - Ji, C.
AU  - Jiang, S.
AU  - Li, J.
PY  - 2024
PD  - 
N2  - Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance.In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios.Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation.Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments.MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge.Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.  2024 International Joint Conferences on Artificial Intelligence. All rights reserved.
JO  - IJCAI International Joint Conference on Artificial Intelligence
PB  - International Joint Conferences on Artificial Intelligence
CY  - 
VL  - 
IS  - 
PG  - 2135-2143
SP  - 2135
EP  - 2143
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204310706&partnerID=40&md5=64032b3df65de36e4c3add0e51772523
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - LLM4EduKG: LLM for Automatic Construction of Educational Knowledge Graph
AU  - Sun, J.
AU  - Zhang, Z.
AU  - He, X.
PY  - 2024
PD  - 
N2  - The field of education is undergoing a significant transformation towards digital and intelligent education, driven by advancements in artificial intelligence. Knowledge graphs (KGs), as a structured representation of knowledge and information, offering a powerful way to integrate diverse and multi-sourced heterogeneous data from across the Internet. The current methodologies for constructing educational knowledge graphs, however, are confronted with challenges including labor-intensive, time-consuming, and the necessity for substantial computational resources, which severely limit their practical application, especially in resource-constrained environments. In this paper, we proposed an LLM-based automatic construction method to alleviate the labor and time consumption in existing methods, and further explored LLM's capabilities in Chinese-speaking context. Specifically, we designed a structured prompt framework to automatically extract and evaluate educational triples generated from original text. The prompt encompasses both task and model dimensions, allowing for flexible adjustments to different tasks and models, thus significantly improved the transferability of our method. Comparative experimental results from two real-world Chinese-datasets, across four advanced LLMs, demonstrate the effectiveness of the proposed method. We believe that our work represents a significant attempt by the LLM in the field of education.  2024 IEEE.
JO  - Proceedings - 2024 International Conference on Networking and Network Applications, NaNA 2024
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - 
VL  - 
IS  - 
PG  - 269-275
SP  - 269
EP  - 275
AN  - 
DO  - 10.1109/NaNA63151.2024.00051
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205987881&doi=10.1109%2fNaNA63151.2024.00051&partnerID=40&md5=d9f4729cf8242d0c88edff0c4bead619
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - LM-KBC 2023: 2nd Challenge on Knowledge Base Construction from Pre-trained Language Models
AU  - Kalo, J. C.
AU  - Singhania, S.
AU  - Razniewski, S.
AU  - Pan, J. Z.
PY  - 2023
PD  - 
N2  - Large language models (LLMs) like chatGPT [1] have advanced a range of semantic tasks and are being ubiquitously used for knowledge extraction. Although several works have explored this ability by crafting prompts with in-context or instruction learning, the viability of complete and precise knowledge base construction from LMs is still in its nascent form. In the 2nd edition of this challenge, we invited participants to extract disambiguated knowledge triples from LMs for a given set of subjects and relations. In crucial difference to existing probing benchmarks like LAMA [2], we made no simplifying assumptions on relation cardinalities, i.e., a subject-entity can stand in relation with zero, one, or many object-entities. Furthermore, submissions needed to go beyond just ranking predicted surface strings, and materialize disambiguated entities in the output, which were evaluated using established KB metrics of precision, recall, and F1-score. The challenge had two tracks: (1) a small model track, where models with < 1 billion parameters could be probed, and (2) an open track, where participants could use any LM of their choice. We received seven submissions, two for track 1 and five for track 2. We present the contributions and insights of the submitted peer-reviewed submissions and lay out the possible paths for future work. All the details related to the challenge can be found on our website at https://lm-kbc.github.io/challenge2023/.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3577
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179554970&partnerID=40&md5=6bd584295e12d116f042d167af46dbd6
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Locating and Extracting Relational Concepts in Large Language Models
AU  - Wang, Z.
AU  - Whyte, B.
AU  - Xu, C.
PY  - 2024
PD  - 
N2  - Relational concepts are indeed foundational to the structure of knowledge representation, as they facilitate the association between various entity concepts, allowing us to express and comprehend complex world knowledge. By expressing relational concepts in natural language prompts, people can effortlessly interact with large language models (LLMs) and recall desired factual knowledge. However, the process of knowledge recall lacks interpretability, and representations of relational concepts within LLMs remain unknown to us. In this paper, we identify hidden states that can express entity and relational concepts through causal mediation analysis in fact recall processes. Our finding reveals that at the last token position of the input prompt, there are hidden states that solely express the causal effects of relational concepts. Based on this finding, we assume that these hidden states can be treated as relational representations and we can successfully extract them from LLMs. The experimental results demonstrate high credibility of the relational representations: they can be flexibly transplanted into other fact recall processes, and can also be used as robust entity connectors. Moreover, we also show that the relational representations exhibit significant potential for controllable fact recall through relation rewriting.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 4818-4832
SP  - 4818
EP  - 4832
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205325541&partnerID=40&md5=68ea62b317142e616137b0fb1b74a174
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering
AU  - Subramanian, A.
AU  - Schlegel, V.
AU  - Kashyap, A. R.
AU  - Nguyen, T. T.
AU  - Dwivedi, V. P.
AU  - Winkler, S.
PY  - 2024
PD  - 
N2  - There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain-a fundamental pre-requisite for success on downstream tasks. Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context. To foster research and collaboration in this field we share M-QALM-our resources, standard-ised methodology, and evaluation results-with the research community to facilitate further advancements in clinical knowledge representation learning within language models.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 4002-4042
SP  - 4002
EP  - 4042
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203393124&partnerID=40&md5=9e25e80729b5dc44445687edc530a706
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Measuring the Knowledge Acquisition-Utilization Gap in Pre-trained Language Models
AU  - Kazemnejad, A.
AU  - Rezagholizadeh, M.
AU  - Parthasarathi, P.
AU  - Chandar, S.
PY  - 2023
PD  - 
N2  - While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. Employing this framework, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains.  2023 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2023
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 4305-4319
SP  - 4305
EP  - 4319
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183311243&partnerID=40&md5=c4dc4bcf11e8c91c0b811a8373755755
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - A Medical Question Classification Approach Based on Prompt Tuning and Contrastive Learning
AU  - Wang, Q.
AU  - Zeng, C.
AU  - Liu, Y.
AU  - He, P.
PY  - 2023
PD  - 
N2  - COVID-19 has profoundly impacted people's lives, and people are more concerned about medical and health issues, so it is essential to design an efficient method for classifying medical questions. Fine-tuning paradigms based on pre-trained language models have proven effective in recent years. However, PLMs based on fine-tuning paradigms are poorly robust, and there is a gap between the pre-training phase and the downstream task form, resulting in PLMs that cannot use the rich latent knowledge in downstream tasks. We propose a medical question classification method that combines prompt fine-tuning and contrastive learning and uses the large-scale knowledge graph enhancement model ERNIE 3.0 as a feature extractor to address both problems. Our approach utilizes an additional prompt template to enable PLM to unleash the potential in specific tasks and uses a contrast sample strategy to alleviate the problem of confusable samples that are difficult to distinguish. Experiments on a medical question classification dataset show that the method achieves an accuracy of 93.65 percent, with better metrics than recent work.  2023 Knowledge Systems Institute Graduate School. All rights reserved.
JO  - Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE
PB  - Knowledge Systems Institute Graduate School
CY  - 
VL  - 2023-July
IS  - 
PG  - 632-635
SP  - 632
EP  - 635
AN  - 
DO  - 10.18293/SEKE2023-025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170090846&doi=10.18293%2fSEKE2023-025&partnerID=40&md5=c448e22d192ade5c07865ac8da3e9d56
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models
AU  - He, J.
AU  - Liu, J.
AU  - Wang, L.
AU  - Li, X.
AU  - Xu, X.
PY  - 2024
PD  - 
N2  - Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts within knowledge graphs and automatically infer missing links. Existing methods can mainly be categorized into structure-based or description-based. Structure-based methods effectively represent relational facts in knowledge graphs using entity embeddings and description-based methods leverage pre-trained language models (PLMs) to understand textual information. In this paper, we propose Momentum Contrast for knowledge graph completion with Structure-Augmented pre-trained language models (MoCoSA), which allows the PLM to perceive the structural information by the adaptable structure encoder. We proposed momentum hard negative and intra-relation negative sampling to improve learning efficiency. Experimental results demonstrate that our approach achieves state-of-the-art performance in terms of mean reciprocal rank (MRR), with improvements of 2.5% on WN18RR and 21% on OpenBG500.  2024 IEEE.
JO  - Proceedings - IEEE International Conference on Multimedia and Expo
PB  - IEEE Computer Society
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 10.1109/ICME57554.2024.10687798
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206592570&doi=10.1109%2fICME57554.2024.10687798&partnerID=40&md5=f6f64930a22bd80bd4c38b6b6183f4bf
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Named Entity Recognition of Fresh Egg Supply Chain Based on BERT-CRF Architecture
AU  - Liu, X.
AU  - Zhang, M.
AU  - Gu, Q.
AU  - Ren, Y.
AU  - He, D.
AU  - Gao, W.
PY  - 2021
PD  - 
N2  - Recognizing named entities from raw text is the first step to construct a fresh egg supply chain knowledge graph and support a variety of downstream natural language processing tasks. This task can sort out the information in the supply chain and provide a basis for food safety traceability. In the raw text of fresh egg supply chain, there were various types of entities, and feature information extraction was inefficient. In order to solve the problem of fast and accurate identification of the named entities which entity types were pre-defined, a bidirectional encoder representations from transformers-conditional random field (BERT-CRF) architecture was proposed to solve the task of named entity recognition (NER) in the area of fresh egg supply chain. In BERT-CRF architecture, begin, internal and other (BIO) labeling rule was used to label the sequence, and the concatenation of character vector and position vector was used as inputs. The pre-training language model (BERT) was used to obtain the global features of input sequence, and the CRF layer was added at the end of the model to introduce hard constraints. A comparative experiment was conducted with other three NER model on the self-constructed dataset that contained five categories and 21 subcategories. The result showed that the BERT-CRF model was superior to the others and reported a state-of-the-art performance. The precision, recall and F1-score were 91.82%, 90.44% and 91.01%, respectively. Finally, through the comparative experiments with other self-constructed dataset (dish dataset), the results showed that the model had a certain generalization ability.  2021, Chinese Society of Agricultural Machinery. All right reserved.
JO  - Nongye Jixie Xuebao
PB  - 
CY  - 
VL  - 52
IS  - 
PG  - 519-525
SP  - 519
EP  - 525
AN  - 
DO  - 10.6041/j.issn.1000-1298.2021.S0.066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121284652&doi=10.6041%2fj.issn.1000-1298.2021.S0.066&partnerID=40&md5=8bb3d62b17d41ef635046fac77ea8b5a
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - On Constructing Biomedical Text-to-Graph Systems with Large Language Models
AU  - Bertolini, L.
AU  - Hulsman, R.
AU  - Consoli, S.
AU  - Puertas-Gallardo, A.
AU  - Ceresa, M.
PY  - 2024
PD  - 
N2  - Knowledge graphs and ontologies represent symbolic and factual information that can offer structured and interpretable knowledge. Extracting and manipulating this type of information is a crucial step in complex processes such as human reasoning. While Large Language Models (LLMs) are known to be useful for extracting and enriching knowledge graphs and ontologies, previous work has largely focused on comparing architecture-specific models (e.g. encoder-decoder only) across benchmarks from similar domains. In this work, we provide a large-scale comparison of the performance of certain LLM features (e.g. model architecture and size) and task learning methods (fine-tuning vs. in-context learning (iCL)) on text-to-graph benchmarks in the biomedical domain. Our experiment suggests that, while a simple truncation-based heuristic can notably boost the performance of decoder-only models used with iCL, small fine-tuned encoder-decoder models produce the most stable and strong performance. Moreover, we found that a massive out-of-domain text-graph pre-training has a positive impact on fine-tuned models, while we observed only a marginal impact of pre-training and size for decoder-only iCL models.  2024 Copyright  2024 for this paper by its authors.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3747
IS  - 
PG  - 12
SP  - 12
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203342156&partnerID=40&md5=0c711da8e40443cb507f91dbd2c34fb2
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - On Early Detection of Hallucinations in Factual Question Answering
AU  - Snyder, B.
AU  - Moisescu, M.
AU  - Zafar, M. B.
PY  - 2024
PD  - 
N2  - While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.  2024 Copyright held by the owner/author(s).
JO  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
PB  - Association for Computing Machinery
CY  - 
VL  - 
IS  - 
PG  - 2721-2732
SP  - 2721
EP  - 2732
AN  - 
DO  - 10.1145/3637528.3671796
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203716980&doi=10.1145%2f3637528.3671796&partnerID=40&md5=355c64ddd9e9bc3ff356b80921ffe634
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - On the extraction of meaningful RNA interactions from Scientific Publications through LLMs and SPIRES
AU  - Cavalleri, E.
AU  - Mesiti, M.
PY  - 2024
PD  - 
N2  - Knowledge graphs (KGs) are useful tools to uniformly represent and integrate heterogeneous information about a domain of interest. However, they are inherently incomplete; therefore, new facts should be introduced by extracting them from structured and unstructured data sources. Starting from RNA-KG, the first KG tailored for representing different kinds of RNA molecules that we recently developed, in this paper we evaluate the use of SPIRES for extracting interactions among bio-entities involving RNA molecules from scientific papers guided by the RNA-KG schema. SPIRES is a general-purpose knowledge extraction system for mining information conforming to a specified schema. A customized prompt is generated and submitted to a Large Language Model (LLM) along with a text to extract a set of RDF triples adhering to the schema constraints. The experiments show a high accuracy in extracting interactions from the scientific literature.  2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3651
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188542930&partnerID=40&md5=a20a94fa220cfd47caa80842c9f95a1d
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs
AU  - Zhang, R.
AU  - Shen, J.
AU  - Liu, T.
AU  - Wang, H.
AU  - Qin, Z.
AU  - Han, F.
AU  - Liu, J.
AU  - Baumgartner, S.
AU  - Bendersky, M.
AU  - Zhang, C.
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our PLaD framework.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 15623-15636
SP  - 15623
EP  - 15636
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205310380&partnerID=40&md5=a4dd56e049259e4de92cda1759c46591
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics
AU  - Zhu, D.
AU  - Chen, D.
AU  - Li, Q.
AU  - Chen, Z.
AU  - Ma, L.
AU  - Grossklags, J.
AU  - Fritz, M.
PY  - 2024
PD  - 
N2  - Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph-a Polygraph for LLMs-as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUCROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.  2024 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: NAACL 2024 - Findings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 4737-4751
SP  - 4737
EP  - 4751
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197645498&partnerID=40&md5=bacf2523643b5d18503969813c1655a4
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - PRE-TRAINING TEXT-TO-TEXT TRANSFORMERS FOR CONCEPT-CENTRIC COMMON SENSE
AU  - Zhou, W.
AU  - Lee, D. H.
AU  - Selvam, R. K.
AU  - Lee, S.
AU  - Lin, B. Y.
AU  - Ren, X.
PY  - 2021
PD  - 
N2  - Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.  2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.
JO  - ICLR 2021 - 9th International Conference on Learning Representations
PB  - International Conference on Learning Representations, ICLR
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150295031&partnerID=40&md5=27d3e1e1003a8d4923b23d250f19e233
NS  - 
N1  - Ishan Tamrakar (2024-12-23 11:18:40)(Screen): determines commonsense knowledge of the LLM through pre training; 
ER  - 

TY  - CONF
TI  - QAVSA: Question Answering using Vector Symbolic Algebras
AU  - Laube, R.
AU  - Eliasmith, C.
PY  - 2024
PD  - 
N2  - With the advancement of large pretrained language models (PLMs), many question answering (QA) benchmarks have been developed in order to evaluate the reasoning capabilities of these models. Augmenting PLMs with external knowledge in the form of Knowledge Graphs (KGs) has been a popular method to improve their reasoning capabilities, and a common method to reason over KGs is to use Graph Neural Networks (GNNs). As an alternative to GNNs to augment PLMs, we propose a novel graph reasoning module using Vector Symbolic Algebra (VSA) graph representations and a k-layer MLP. We demonstrate that our VSA-based model performs as well as QA-GNN, a model combining a PLM and a GNN-module, on 3 multiple-choice question answering (MCQA) datasets. Our model has a simpler architecture than QA-GNN and also converges 39% faster during training.  2024 Association for Computational Linguistics.
JO  - ACL 2024 - 9th Workshop on Representation Learning for NLP, RepL4NLP 2024 - Proceedings of the Workshop
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 191-202
SP  - 191
EP  - 202
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204929790&partnerID=40&md5=e15d611c9f1e5434a9283054acec6991
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Research on Construction and Application of Knowledge Graph Based on Large Language Model
AU  - Zhang, C.
AU  - Li, X.
AU  - Zheng, S.
AU  - Cai, J.
AU  - Ye, X.
AU  - Luo, J.
PY  - 2024
PD  - 
N2  - Massive amounts of operational and maintenance (O&M) data from nuclear power distributed control system (DCS) contain rich operational experience and expert knowledge. Effectively extracting DCS alarm response information and forming knowledge service is a current hotspot and frontier research area in rapid DCS response. Due to the lack of clear structure and standards in multi-source heterogeneous data of nuclear power DCS, previous knowledge extraction primarily relied on manual annotation and deep learning methods, which require extensive domain knowledge and information processing capabilities and are constrained by the heavy workload of data annotation. Therefore, this study proposes a knowledge extraction method using large language model (LLM) with a step-by-step prompting strategy, constructing a DCS O&M knowledge graph (KG). Based on large language model technology and secondary intent recognition methods, intelligent question and answer (Q&A) and other knowledge services are developed utilizing the knowledge graph. Using O&M data from a nuclear power plants DCS as a case study, the research focuses on knowledge extraction, knowledge graph construction, and intelligent Q&A. The results show that the model achieves an overall precision (P) of 91.24%, recall (R) of 85.85%, and F1- score of 88.43% . The proposed method can comprehensively capture key entities and attribute information from multi-source heterogeneous DCS O&M data, guiding domain knowledge Q&A, assisting O&M personnel in timely responding to DCS alarm anomalies, analyzing fault causes and response strategies, and providing guidance for DCS O&M training and maintenance in power plants.  2024 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
JO  - J. Frontier. Comput. Sci. Technol.
PB  - 
CY  - 
VL  - 18
IS  - 10
PG  - 2656-2667
SP  - 2656
EP  - 2667
AN  - 
DO  - 10.3778/j.issn.1673-9418.2406013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206515503&doi=10.3778%2fj.issn.1673-9418.2406013&partnerID=40&md5=3886ca855c2305b1c60aadc184c9d9dd
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models
AU  - Cao, B.
AU  - Tang, Q.
AU  - Lin, H.
AU  - Jiang, S.
AU  - Dong, B.
AU  - Han, X.
AU  - Chen, J.
AU  - Wang, T.
AU  - Sun, L.
PY  - 2024
PD  - 
N2  - Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memorizing dynamic mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models without pre-training are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.  2024 ELRA Language Resource Association: CC BY-NC 4.0.
JO  - 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings
PB  - European Language Resources Association (ELRA)
CY  - 
VL  - 
IS  - 
PG  - 14016-14036
SP  - 14016
EP  - 14036
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192559496&partnerID=40&md5=2903d691b6c095bdd5c227d784480189
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Robust Knowledge Extraction from Large Language Models using Social Choice Theory
AU  - Potyka, N.
AU  - Zhu, Y.
AU  - He, Y.
AU  - Kharlamov, E.
AU  - Staab, S.
PY  - 2024
PD  - 
N2  - Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.  2024 International Foundation for Autonomous Agents and Multiagent Systems.
JO  - Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS
PB  - International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)
CY  - 
VL  - 2024-May
IS  - 
PG  - 1593-1601
SP  - 1593
EP  - 1601
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196357345&partnerID=40&md5=233088fcd9235cff84d88f993af7f9b7
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing
AU  - Banerjee, D.
AU  - Nair, P. A.
AU  - Usbeck, R.
AU  - Biemann, C.
PY  - 2023
PD  - 
N2  - In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query language. We observe that the query vocabulary is distinct from human vocabulary. Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve. We carry out carefully selected vocabulary substitutions on the queries and find absolute gains in the range of 17% on the GrailQA dataset.  2023 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 12219-12228
SP  - 12219
EP  - 12228
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175442200&partnerID=40&md5=78a8f96394f6ae0dd3411cd2ff674e15
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs
AU  - Chen, H.
AU  - Shen, X.
AU  - Lv, Q.
AU  - Wang, J.
AU  - Ni, X.
AU  - Ye, J.
PY  - 2024
PD  - 
N2  - Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the acquisition of precise and dependable knowledge is crucial. However, existing KG construction methods heavily rely on human intervention to attain qualified KGs, which severely hinders the practical applicability in real-world scenarios. To address this challenge, we propose a general KG construction framework, named SAC-KG, to exploit large language models (LLMs) as Skilled Automatic Constructors for domain Knowledge Graph. SAC-KG effectively involves LLMs as domain experts to generate specialized and precise multi-level KGs. Specifically, SAC-KG consists of three components: Generator, Verifier, and Pruner. For a given entity, Generator produces its relations and tails from raw domain corpora, to construct a specialized single-level KG. Verifier and Pruner then work together to ensure precision by correcting generation errors and determining whether newly produced tails require further iteration for the next-level KG. Experiments demonstrate that SAC-KG automatically constructs a domain KG at the scale of over one million nodes and achieves a precision of 89.32%, leading to a superior performance with over 20% increase in precision rate compared to existing state-of-the-art methods for the KG construction task.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 1
IS  - 
PG  - 4345-4360
SP  - 4345
EP  - 4360
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204481093&partnerID=40&md5=49d29db92bfcb093f37f46bb7736067e
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Secure Co-Creation of Industrial Knowledge Graph: Graph Complement Method with Federated Learning and ChatGPT
AU  - Xia, L.
AU  - Zheng, P.
AU  - Liang, Y.
AU  - Zheng, G.
AU  - Ling, Z.
PY  - 2023
PD  - 
N2  - Industrial areas have increasingly developed their own Knowledge Graph (KG) for organizing and leveraging vast amounts of data. One major challenge in constructing KG is the heavy reliance on available resources, restricting the scalability and accuracy of the resulting graphs. To address this issue, an end-to-end method is proposed to create a multi-benefit ecosystem by integrating Federated Learning with ChatGPT (a popular language model). Different stakeholders may leverage ChatGPT to search for novel knowledge that complements their existing KGs, however, this approach could potentially introduce ambiguous and wrong triples into the KG. To overcome this, Federated Learning is applied to align and disambiguate the triples using other industrial KGs as super-vision. The proposed method applies a multi-field hyperbolic embedding method to vectorize entities and edges, which are then associatively aggregated to achieve edge replenishment and entity fusion for each KG encrypted. Finally, an incentive win-win mechanism is proposed to motivate diverse stakeholders to contribute to this co-creation actively. A case study is conducted on different industrial KG to evaluate the proposed method. Results demonstrate that this method provides a practical solution for KG co-creation and no compromise to data security.  2023 IEEE.
JO  - IEEE International Conference on Automation Science and Engineering
PB  - IEEE Computer Society
CY  - 
VL  - 2023-August
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 10.1109/CASE56687.2023.10260382
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174385930&doi=10.1109%2fCASE56687.2023.10260382&partnerID=40&md5=03fda67805ea79cdefcf13519af79723
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Self-knowledge distillation in natural language processing
AU  - Hahn, S.
AU  - Choi, H.
PY  - 2019
PD  - 
N2  - Since deep learning became a key player in natural language processing (NLP), many deep learning models have been showing remarkable performances in a variety of NLP tasks, and in some cases, they are even outperforming humans. Such high performance can be explained by efficient knowledge representation of deep learning models. While many methods have been proposed to learn more efficient representation, knowledge distillation from pretrained deep networks suggest that we can use more information from the soft target probability to train other neural networks. In this paper, we propose a new knowledge distillation method self-knowledge distillation, based on the soft target probabilities of the training model itself, where multimode information is distilled from the word embedding space right below the softmax layer. Due to the time complexity, our method approximates the soft target probabilities. In experiments, we applied the proposed method to two different and fundamental NLP tasks: language model and neural machine translation. The experiment results show that our proposed method improves performance on the tasks.  2019 Association for Computational Linguistics (ACL). All rights reserved.
JO  - International Conference Recent Advances in Natural Language Processing, RANLP
PB  - Incoma Ltd
CY  - 
VL  - 2019-September
IS  - 
PG  - 423-430
SP  - 423
EP  - 430
AN  - 
DO  - 10.26615/978-954-452-056-4_050
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076465811&doi=10.26615%2f978-954-452-056-4_050&partnerID=40&md5=4d951a09020a5208fe06a7cd038ce799
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Self-Knowledge Guided Retrieval Augmentation for Large Language Models
AU  - Wang, Y.
AU  - Li, P.
AU  - Sun, M.
AU  - Liu, Y.
PY  - 2023
PD  - 
N2  - Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer nonparametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT. Code is released at https://github.com/THUNLP-MT/SKR.  2023 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2023
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 10303-10315
SP  - 10303
EP  - 10315
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182850094&partnerID=40&md5=bad0475efd3fd3a211275cab9ed6794a
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Semantic Interpretation of BERT embeddings with Knowledge Graphs
AU  - De Bellis, A.
AU  - Biancofiore, G. M.
AU  - Anelli, V. W.
AU  - Narducci, F.
AU  - Di Noia, T.
AU  - Ragone, A.
AU  - Di Sciascio, E.
PY  - 2023
PD  - 
N2  - Pretrained language models have transformed the way we process natural languages, enhancing the performance of related systems. BERT has played a pivotal role in revolutionizing the field of Natural Language Processing (NLP). However, the deep learning framework behind BERT lacks interpretability. Recent research has focused on explaining the knowledge BERT acquires from the textual sources used for pre-training its linguistic model. In this study, we analyze the latent vector space produced by BERT's context-aware word embeddings. Our aim is to determine whether certain areas of the BERT vector space have an explicit meaning related to a Knowledge Graph (KG). Using the Link Prediction (LP) task, we demonstrate the presence of explicit and meaningful regions of the BERT vector space. Moreover, we establish links between BERT's vector space and specific ontology concepts in the KG by learning classification patterns. To the best of our knowledge, this is the first attempt to interpret BERT's learned linguistic knowledge through a KG by relying on its pre-trained context-aware word embeddings.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3478
IS  - 
PG  - 181-191
SP  - 181
EP  - 191
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173494150&partnerID=40&md5=d56e643e19b5d88dad9391e03ad488d0
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Small Models, Big Insights: Leveraging Slim Proxy Models to Decide When and What to Retrieve for LLMs
AU  - Tan, J.
AU  - Dou, Z.
AU  - Zhu, Y.
AU  - Guo, P.
AU  - Fang, K.
AU  - Wen, J. R.
PY  - 2024
PD  - 
N2  - The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 1
IS  - 
PG  - 4420-4436
SP  - 4420
EP  - 4436
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204468627&partnerID=40&md5=505cef6fa1fdda7596910a53ad46d456
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes
AU  - Deshpande, A.
AU  - Ruiter, D.
AU  - Mosbach, M.
AU  - Klakow, D.
PY  - 2022
PD  - 
N2  - Analyzing ethnic or religious bias is important for improving fairness, accountability, and transparency of natural language processing models. However, many techniques rely on human-compiled lists of bias terms, which are expensive to create and are limited in coverage. In this study, we present a fully data-driven pipeline for generating a knowledge graph (KG) of cultural knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5 nationalities and can easily be extended to include more entities. Our human evaluation shows that the majority (59.2%) of non-singleton entries are coherent and complete stereotypes. We further show that performing intermediate masked language model training on the verbalized KG leads to a higher level of cultural awareness in the model and has the potential to increase classification performance on knowledge-crucial samples on a related task, i.e., hate speech detection.  2022 Association for Computational Linguistics.
JO  - WOAH 2022 - 6th Workshop on Online Abuse and Harms, Proceedings of the Workshop
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 67-78
SP  - 67
EP  - 78
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139104669&partnerID=40&md5=7440e0daa331119fc6732f0e01996c97
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning
AU  - Lee, D. H.
AU  - Ahrabian, K.
AU  - Jin, W.
AU  - Morstatter, F.
AU  - Pujara, J.
PY  - 2023
PD  - 
N2  - Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures (0.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases. 2023 Association for Computational Linguistics.
JO  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 544-557
SP  - 544
EP  - 557
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182442697&partnerID=40&md5=8ef89e61d4e4e7a0830d480e90a24824
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Towards Automated Evaluation of Knowledge Encoded in Large Language Models
AU  - Lus Ferreira, B. C.
AU  - Silva, C.
AU  - Gonalo Oliveira, H.
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) have a significant user base and are gaining increasing interest and impact across various domains. Given their expanding influence, it is crucial to implement appropriate guardrails or controls to ensure ethical and responsible use. In this paper, we propose to automate the evaluation of the knowledge stored in LLMs. This is achieved by generating datasets tailored for this specific purpose, in any selected domain. Our approach consists of four major steps: (i) extraction of relevant entities; (ii) gathering of domain properties; (iii) dataset generation; and (iv) model evaluation. In order to materialize this vision, tools and resources were experimented for entity linking, knowledge acquisition, classification and prompt generation, yielding valuable insights and lessons. The generation of datasets for domain specific model evaluation has successfully proved that the approach can be a future tool for evaluating and moving LLMs black-boxes to human-interpretable knowledge bases.  2024 ELRA Language Resource Association: CC BY-NC 4.0.
JO  - Proceedings of the Workshop on DLnLD 2024: Deep Learning and Linked Data at LREC-COLING 2024 - Workshop Proceedings
PB  - European Language Resources Association (ELRA)
CY  - 
VL  - 
IS  - 
PG  - 76-85
SP  - 76
EP  - 85
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195166235&partnerID=40&md5=92fa0997bc29a450127cdd72245b78da
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples
AU  - Ren, Z.
AU  - Zhao, Y.
AU  - Zong, C.
PY  - 2023
PD  - 
N2  - Pretrained language models (PLMs), especially large language models (LLMs) demonstrate impressive capabilities in open-ended text generation. While our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do. To address this issue, we propose a dynamic knowledge-guided informative open-ended text generation approach, that utilizes a knowledge graph to help the model generate more contextually related entities and detailed facts. Specifically, we first employ a local knowledge filter to extract relevant knowledge from the comprehensive knowledge graph for a given topic sentence. Then we introduce a dynamic knowledge selector to predict the entity to be mentioned in the subsequent sentence. Finally, we utilize a knowledge-enhanced text generator to produce a more informative output. To evaluate the effectiveness of our approach, we evaluate the proposed approach in two scenarios: fine-tuning for small PLMs and prompt tuning for LLMs. Experimental results show that our approach could generate more informative texts than baselines.  2023 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2023
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 3189-3203
SP  - 3189
EP  - 3203
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183306120&partnerID=40&md5=85e02c018d29e3146ef9dd25a7746c30
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Towards Large Language Model Architectures for Knowledge Acquisition and Strategy Synthesis
AU  - Giorgini, P.
AU  - Mazzullo, A.
AU  - Robol, M.
AU  - Roveri, M.
PY  - 2023
PD  - 
N2  - To address the bottlenecks of knowledge acquisition and strategy synthesis, in the development of autonomous AI agents capable of reasoning and planning about dynamic environments, we propose an architecture that combines large language model (LLM) functionalities with formal verification modules. Concerning knowledge acquisition, we focus on the problem of learning description logic concepts to separate data instances, whereas, in a process mining setting, we propose to leverage LLMs to extract linear temporal logic specifications from event logs. Finally, in a strategy synthesis context, we illustrate how LLMs can be employed to address realisability problems in linear temporal logic on finite traces.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3629
IS  - 
PG  - 61-66
SP  - 61
EP  - 66
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184520918&partnerID=40&md5=bc31a8a712c0d0ad8c029992bc0750d5
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Towards syntax-aware pretraining and prompt engineering for knowledge retrieval from large language models
AU  - Dietze, S.
AU  - Jabeen, H.
AU  - Kallmeyer, L.
AU  - Linzbach, S.
PY  - 2023
PD  - 
N2  - The ability to access relational knowledge from LLM parameters, known as relational knowledge retrieval (rKR), is considered a critical factor in their capacity to comprehend and interpret natural language. However, the role of syntax in this context has not been adequately explored. In this position paper, we hypothesize a close link between the accessibility of relational knowledge and syntax. We discuss related works and lay out a research agenda focused on rKR from self-supervised LLMs without or with minimal fine-tuning and aiming at understanding the impact of syntax on rKR. This involves examining biases, factors affecting result reliability and robustness, and analyzing the effect of syntactic features in training corpora on rKR. We argue that rKR can be improved through syntax-aware pretraining and prompt engineering, and propose a dedicated research agenda geared toward exploring the impact of syntax on knowledge retrieval.  2023 CEUR-WS. All rights reserved.
JO  - CEUR Workshop Proceedings
PB  - CEUR-WS
CY  - 
VL  - 3577
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179555187&partnerID=40&md5=9d182678aae913e62c5f92f0280d8ff6
NS  - 
N1  - Ishan Tamrakar (2024-12-25 04:13:37)(Screen): KR but says it is a position paper so not-research? ; 
ER  - 

TY  - CONF
TI  - Towards using Automatically Enhanced Knowledge Graphs to Aid Temporal Relation Extraction
AU  - Knez, T.
AU  - itnik, S.
PY  - 2024
PD  - 
N2  - Temporal relation extraction in medical document analysis is crucial for understanding patient histories and treatment outcomes. This paper introduces a novel approach leveraging a bimodal model integrating textual content and a knowledge graph to enhance temporal relation extraction. The paper presents ongoing research on constructing an optimal knowledge graph by augmenting PrimeKG with dynamically expanded information using a language model-generated knowledge graph. It also further personalizes the information with patient-specific graphs tailored for relation prediction. The pipeline for constructing this enriched knowledge graph is detailed, aiming to improve the capabilities of temporal relation extraction models. The preliminary results show that adding a simple knowledge graph to the temporal relation extraction model can significantly increase the performance, achieving new state-of-the-art results. While research on enhanced knowledge graphs is ongoing, this paper lays the groundwork for leveraging common knowledge to advance temporal relation extraction in medical contexts. This approach holds promise for enhancing the understanding of patient histories and treatment outcomes, potentially leading to improved healthcare decision-making and patient care.  2024 ELRA Language Resource Association: CC BY-NC 4.0.
JO  - 1st Workshop on Patient-Oriented Language Processing, CL4Health 2024 at LREC-COLING 2024 - Workshop Proceedings
PB  - European Language Resources Association (ELRA)
CY  - 
VL  - 
IS  - 
PG  - 131-136
SP  - 131
EP  - 136
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195205041&partnerID=40&md5=743870793a52e0324fc13095d636bd96
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
AU  - Wang, X.
AU  - Amayuelas, A.
AU  - Zhang, K.
AU  - Pan, L.
AU  - Chen, W.
AU  - Wang, W. Y.
PY  - 2024
PD  - 
N2  - Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance. Copyright 2024 by the author(s)
JO  - Proceedings of Machine Learning Research
PB  - ML Research Press
CY  - 
VL  - 235
IS  - 
PG  - 50026-50042
SP  - 50026
EP  - 50042
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203798431&partnerID=40&md5=414e39bd68b73eb0b05d8fdb935bba5e
NS  - 
N1  - Ishan Tamrakar (2024-12-26 10:19:15)(Screen): knowledge representation ; 
ER  - 

TY  - CONF
TI  - UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing
AU  - Yang, Y.
AU  - He, J.
AU  - Chen, P.
AU  - Gutirrez-Basulto, V.
AU  - Pan, J. Z.
PY  - 2024
PD  - 
N2  - Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the models out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.  2024 Association for Computational Linguistics.
JO  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 1
IS  - 
PG  - 7011-7028
SP  - 7011
EP  - 7028
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200201270&partnerID=40&md5=710515e19aaecefe2e4e18962282dd0e
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Unleashing the Power of Language Models in Text-Attributed Graph
AU  - Kuang, H.
AU  - Xu, J.
AU  - Zhang, H.
AU  - Zhao, Z.
AU  - Zhang, Q.
AU  - Huang, X.
AU  - Wei, Z.
PY  - 2023
PD  - 
N2  - Representation learning on graph has been demonstrated to be a powerful tool for solving real-world problems. Text-attributed graph carries both semantic and structural information among different types of graphs. Existing works have paved the way for knowledge extraction of this type of data by leveraging language models or graph neural networks or combination of them. However, these works suffer from issues like underutilization of relationships between nodes or words or unaffordable memory cost. In this paper, we propose a Node Representation Update Pre-training Architecture based on Co-modeling Text and Graph (NRUP). In NRUP, we construct a hierarchical text-attributed graph that incorporates both initial nodes and word nodes. Meanwhile, we apply four self-supervised tasks for different level of constructed graph. We further design the pre-training framework to update the features of nodes during training epochs. We conduct the experiment on the benchmark dataset ogbn-arxiv. Our method achieves outperformance compared to baselines, fully demonstrating its validity and generalization.  2023 Association for Computational Linguistics.
JO  - Findings of the Association for Computational Linguistics: EMNLP 2023
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 8429-8441
SP  - 8429
EP  - 8441
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183301246&partnerID=40&md5=0036174a5b15ac2b73d8b10f7b79c5ef
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models
AU  - Su, W.
AU  - Wang, C.
AU  - Ai, Q.
AU  - Hu, Y.
AU  - Wu, Z.
AU  - Zhou, Y.
AU  - Liu, Y.
PY  - 2024
PD  - 
N2  - Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.  2024 Association for Computational Linguistics.
JO  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
CY  - 
VL  - 
IS  - 
PG  - 14379-14391
SP  - 14379
EP  - 14391
AN  - 
DO  - 
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199931547&partnerID=40&md5=545b44f0e75c867a408c6471ba748551
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - An Accurate and Efficient Approach to Knowledge Extraction from Scientific Publications Using Structured Ontology Models, Graph Neural Networks, and Large Language Models
AU  - Ivanisenko, T. V.
AU  - Demenkov, P. S.
AU  - Ivanisenko, V. A.
PY  - 2024
PD  - 
N2  - The rapid growth of biomedical literature makes it challenging for researchers to stay current. Integrating knowledge from various sources is crucial for studying complex biological systems. Traditional text-mining methods often have limited accuracy because they don't capture semantic and contextual nuances. Deep-learning models can be computationally expensive and typically have low interpretability, though efforts in explainable AI aim to mitigate this. Furthermore, transformer-based models have a tendency to produce false or made-up information-a problem known as hallucination-which is especially prevalent in large language models (LLMs). This study proposes a hybrid approach combining text-mining techniques with graph neural networks (GNNs) and fine-tuned large language models (LLMs) to extend biomedical knowledge graphs and interpret predicted edges based on published literature. An LLM is used to validate predictions and provide explanations. Evaluated on a corpus of experimentally confirmed protein interactions, the approach achieved a Matthews correlation coefficient (MCC) of 0.772. Applied to insomnia, the approach identified 25 interactions between 32 human proteins absent in known knowledge bases, including regulatory interactions between MAOA and 5-HT2C, binding between ADAM22 and 14-3-3 proteins, which is implicated in neurological diseases, and a circadian regulatory loop involving RORB and NR1D1. The hybrid GNN-LLM method analyzes biomedical literature efficiency to uncover potential molecular interactions for complex disorders. It can accelerate therapeutic target discovery by focusing expert verification on the most relevant automatically extracted information.
JO  - Int J Mol Sci
PB  - 
CY  - 
VL  - 25
IS  - 21
PG  - 
SP  - 
EP  - 
AN  - 39519363
DO  - 10.3390/ijms252111811
UR  - 
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Multimodal learning on graphs for disease relation extraction
AU  - Lin, Y.
AU  - Lu, K.
AU  - Yu, S.
AU  - Cai, T.
AU  - Zitnik, M.
PY  - 2023
PD  - 
N2  - Disease knowledge graphs have emerged as a powerful tool for artificial intelligence to connect, organize, and access diverse information about diseases. Relations between disease concepts are often distributed across multiple datasets, including unstructured plain text datasets and incomplete disease knowledge graphs. Extracting disease relations from multimodal data sources is thus crucial for constructing accurate and comprehensive disease knowledge graphs. We introduce REMAP, a multimodal approach for disease relation extraction. The REMAP machine learning approach jointly embeds a partial, incomplete knowledge graph and a medical language dataset into a compact latent vector space, aligning the multimodal embeddings for optimal disease relation extraction. Additionally, REMAP utilizes a decoupled model structure to enable inference in single-modal data, which can be applied under missing modality scenarios. We apply the REMAP approach to a disease knowledge graph with 96,913 relations and a text dataset of 1.24 million sentences. On a dataset annotated by human experts, REMAP improves language-based disease relation extraction by 10.0% (accuracy) and 17.2% (F1-score) by fusing disease knowledge graphs with language information. Furthermore, REMAP leverages text information to recommend new relationships in the knowledge graph, outperforming graph-based methods by 8.4% (accuracy) and 10.4% (F1-score). REMAP is a flexible multimodal approach for extracting disease relations by fusing structured knowledge and language information. This approach provides a powerful model to easily find, access, and evaluate relations between disease concepts.
JO  - J Biomed Inform
PB  - 
CY  - 
VL  - 143
IS  - 
PG  - 104415
SP  - 104415
EP  - 
AN  - 37276949
DO  - 10.1016/j.jbi.2023.104415
UR  - 
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - AI for Patents: A Novel Yet Effective and Efficient Framework for Patent Analysis
AU  - Son, J.
AU  - Moon, H.
AU  - Lee, J.
AU  - Lee, S.
AU  - Park, C.
AU  - Jung, W.
AU  - Lim, H.
PY  - 2022
PD  - 
N2  - Patents provide inventors exclusive rights to their inventions by protecting their intellectual property rights. However, analyzing patent documents generally requires knowledge of various fields, considerable human labor, and expertise. Recent studies to alleviate this problem on patent analysis deal only with the analysis of claims and abstract parts, neglecting the descriptions that contain essential technical cores. Moreover, few studies use a deep learning approach to handle the entire patent analysis process, including preprocessing, summarization, and key-phrase generation. Therefore, we propose a novel multi-stage framework that can aid in analyzing patent documents by using the description part of the patent rather than abstracts or claims with deep learning. The framework comprises two stages: key-sentence extraction and key-phrase generation tasks. These stages are based on the T5 model structure, transformer-based architecture that uses a text-to-text approach. To further improve the frameworks performance, we employed two key factors: i) post-training the model with a patent-related raw corpus for encouraging the models comprehension of the patent domain, and ii) utilizing a text rank algorithm for efficient training based on the priority score of each sentence. We verified that our key-phrase generation method of the framework shows higher performance in both superficial and semantic evaluation than other extraction methods. In addition, we provided the validity and effectiveness of our methods through quantitative and qualitative analysis, demonstrating the practical functionality of our methods. We also provided a practical contribution to the patent analysis by releasing the framework as a demo system.
JO  - IEEE Access
PB  - 
CY  - 
VL  - 10
IS  - 
PG  - 59205-59218
SP  - 59205
EP  - 59218
AN  - 
DO  - 10.1109/ACCESS.2022.3176877
UR  - 
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Enhancing Pre-Trained Language Models with Knowledge Representation Using Line Graphs
AU  - Ge, Z.
AU  - Zhu, Y.
AU  - Pan, R.
PY  - 2024
PD  - 
N2  - To address the inherent limitation of pre-trained language models regarding factual knowledge, current efforts encompass a variety of methods aimed at bolstering their capabilities through the integration of knowledge graphs as external sources. This augmentation seeks to enhance their performance across knowledge-driven tasks. However, the challenges of effectively encapsulating entity knowledge and mitigating the storage overhead associated with external knowledge persist. In this paper, we present a novel approach for representing entity knowledge. Our method leverages the relational context surrounding entities, departing from the conventional practice of employing distinct vector representations for each entity. Specifically, we propose a transformation of entity-level subgraphs into line graphs, allowing us to explicitly capture and model relational patterns inherent in entity adjacencies. In contrast to the original graph-based representation, our line graph-based model exhibits a heightened capacity to capture intricate knowledge structures. Through empirical evaluation across three downstream tasks - namely, relation extraction, entity typing, and question answering over knowledge graphs - we substantiate the efficacy of our approach. The experimental results demonstrate the superior performance of our model over prevailing state-of-the-art methodologies across the majority of tasks.
JO  - 2024 3rd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 1-9
SP  - 1
EP  - 9
AN  - 
DO  - 10.1109/AICIT62434.2024.10730175
UR  - 
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - A Natural Language Query Method for Linked Data
AU  - Xiao, Z.
AU  - Xiao, Y.
PY  - 2021
PD  - 
N2  - Efficient data retrieval is one of the key issues in the development of Web of Data based on Resource Description Framework (RDF). Formal query language like SPARQL is an effective way to retrieve structured data, but users are accustomed to natural language-based retrieval. Due to its grammatical complexity and prerequisite knowledge of ontology schema, the formal query language is hard to be applied in natural language-based retrieval directly without help from new approaches or tools. Therefore, how to automatically convert keyword search into formal query-based retrieval is an important part of the realization of Web of Data. The natural language query method for linked data automatically converts natural language queries into SPARQL queries to improve the effectiveness and efficiency of the system. In this paper, the SPARQL query is generated by transforming language elements among abstract models, followed by constructing the ontology-based query semantic graphs and illuminating semantic ambiguity. The experimental results show that the proposed approach has a higher recall rate, better precision, and lower time consumption.
JO  - 2021 International Conference on Big Data Engineering and Education (BDEE)
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 35-39
SP  - 35
EP  - 39
AN  - 
DO  - 10.1109/BDEE52938.2021.00012
UR  - 
NS  - 
N1  - 
ER  - 

TY  - CONF
TI  - Utilizing GloVe Embeddings for Deep Learning-Based Analysis of Research Paper Abstracts
AU  - Hossain, A.
AU  - Konok, U. H.
AU  - Islam, R.
AU  - Ruhani, R. M. Karmol
AU  - Musfikin, R.
AU  - Uddin, M. M.
AU  - Khan, M. S. Hossain
AU  - Tuhin, R. A.
PY  - 2023
PD  - 
N2  - Researchers are finding it harder and harder to locate relevant articles as the body of scientific literature expands at an exponential rate. Due to the sheer volume of publications, manual classification and categorization of these articles is no longer possible. By addressing the task of accurately classifying research papers based on their abstracts, the purpose of this paper is to address the task of improving the proposal and search procedures for efficient academic information retrieval. While ordering papers in computer science, mathematics, physics, and statistics, the models accomplish high precision, accuracy, recall, and F1-score by using profound learning calculations (LSTM, GRU, Bi-LSTM, and Bi-GRU) and GloVe word embeddings to catch semantic data. The LSTM, Bi-LSTM, GRU and Bi-GRU models were used to accurately classify the abstracts of research papers into computer science, mathematics, physics, and statistics. These models performed well concerning accuracy, recall, and F1-score, as well as accomplishing high precision. Automatic categorization of research papers was made possible by combining GloVe word embeddings with deep learning algorithms, which sped up information search and knowledge discovery. These models can help academic researchers and practitioners streamline the process of categorizing research papers and boost their research efforts.
JO  - 2023 5th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 1-6
SP  - 1
EP  - 6
AN  - 
DO  - 10.1109/HORA58378.2023.10156746
UR  - 
NS  - 
N1  - 
ER  - 

TY  - CPAPER
TI  - Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph
AU  - Zhao, Qian
AU  - Qian, Hao
AU  - Liu, Ziqi
AU  - Zhang, Gong-Duo
AU  - Gu, Lihong
PY  - 2024
PD  - 
N2  - 
JO  - Proceedings of the 33rd ACM International Conference on Information and Knowledge Management
PB  - Association for Computing Machinery
CY  - Boise, ID, USA
VL  - 
IS  - 
PG  - 50865093
SP  - 50865093
EP  - 
AN  - 
DO  - 10.1145/3627673.3680022
UR  - https://doi.org/10.1145/3627673.3680022
NS  - 
N1  - 
ER  - 

TY  - THES
TI  - Jointly Learning Knowledge Graph Embeddings, Fine Grain Entity Types and Language Models
AU  - Patel, Rajat Hareshkumar
AU  - Finin, Tim
AU  - Joshi, Karuna
PY  - 2020
PD  - 
N2  - 
JO  - 
PB  - University of Maryland, Baltimore County
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - 
NS  - 
N1  - 
ER  - 

TY  - CPAPER
TI  - Semantic Parsing for&nbsp;Question and&nbsp;Answering over&nbsp;Scholarly Knowledge Graph with&nbsp;Large Language Models
AU  - Nguyen, Le-Minh
AU  - Khang, Le-Nguyen
AU  - Anh, Kieu Que
AU  - Hien, Nguyen Dieu
AU  - Nagai, Yukari
PY  - 2024
PD  - 
N2  - 
JO  - New Frontiers in Artificial Intelligence: JSAI International Symposium on Artificial Intelligence, JSAI-isAI 2024, Hamamatsu, Japan, May 2829, 2024, Proceedings
PB  - Springer-Verlag
CY  - Hamamatsu, Japan
VL  - 
IS  - 
PG  - 284298
SP  - 284298
EP  - 
AN  - 
DO  - 10.1007/978-981-97-3076-6_20
UR  - https://doi.org/10.1007/978-981-97-3076-6_20
NS  - 
N1  - Kwesi CObbina (2024-12-29 06:07:29)(Screen): This paper presents a study to answer the question of how to map a natural language (NL) sentence to a semantic representation and its application to question answering over the DBLP database. We investigate the deep learning approach using pre-trained models and their fine-tuning on training data for semantic parsing tasks. Experimental results on standard datasets show the effectiveness of pre-trained models in mapping an NL sentence to SPARQL, a query language for semantic databases. The results also show that the T5 and Flan-T5 models outperform other models in terms of translation accuracy. In addition to the empirical results on pre-trained models, we also consider the problem of examining large language models (LLMs) such as Llama and Mistras, or Qwen models for answering questions on the DBLP database. Experimental results showed the potentiality of using LLMs with chain-of-thought prompting methods. The results indicated that without using training data, we were able to obtain promising results for some types of questions when translating them to SPARQL.; 
ER  - 

TY  - JOUR
TI  - Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs
AU  - Salnikov, Mikhail
AU  - Lysyuk, Maria
AU  - Braslavski, Pavel
AU  - Razzhigaev, Anton
AU  - Malykh, Valentin
AU  - Panchenko, Alexander
PY  - 2023
PD  - 
N2  - Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2310.07008v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Apollonion: Profile-centric Dialog Agent
AU  - Chen, Shangyu
AU  - Zhao, Zibo
AU  - Zhao, Yuanyuan
AU  - Li, Xiang
PY  - 2024
PD  - 
N2  - The emergence of Large Language Models (LLMs) has innovated the development of dialog agents. Specially, a well-trained LLM, as a central process unit, is capable of providing fluent and reasonable response for user's request. Besides, auxiliary tools such as external knowledge retrieval, personalized character for vivid response, short/long-term memory for ultra long context management are developed, completing the usage experience for LLM-based dialog agents. However, the above-mentioned techniques does not solve the issue of \textbf{personalization from user perspective}: agents response in a same fashion to different users, without consideration of their features, such as habits, interests and past experience. In another words, current implementation of dialog agents fail in ``knowing the user''. The capacity of well-description and representation of user is under development. In this work, we proposed a framework for dialog agent to incorporate user profiling (initialization, update): user's query and response is analyzed and organized into a structural user profile, which is latter served to provide personal and more precise response. Besides, we proposed a series of evaluation protocols for personalization: to what extend the response is personal to the different users. The framework is named as \method{}, inspired by inscription of ``Know Yourself'' in the temple of Apollo (also known as \method{}) in Ancient Greek. Few works have been conducted on incorporating personalization into LLM, \method{} is a pioneer work on guiding LLM's response to meet individuation via the application of dialog agents, with a set of evaluation methods for measurement in personalization.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2404.08692v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization
AU  - Priyanshu, Aman
AU  - Vijay, Supriti
AU  - Kumar, Ayush
AU  - Naidu, Rakshit
AU  - Mireshghallah, Fatemehsadat
PY  - 2023
PD  - 
N2  - LLM-powered chatbots are becoming widely adopted in applications such as healthcare, personal assistants, industry hiring decisions, etc. In many of these cases, chatbots are fed sensitive, personal information in their prompts, as samples for in-context learning, retrieved records from a database, or as part of the conversation. The information provided in the prompt could directly appear in the output, which might have privacy ramifications if there is sensitive information there. As such, in this paper, we aim to understand the input copying and regurgitation capabilities of these models during inference and how they can be directly instructed to limit this copying by complying with regulations such as HIPAA and GDPR, based on their internal knowledge of them. More specifically, we find that when ChatGPT is prompted to summarize cover letters of a 100 candidates, it would retain personally identifiable information (PII) verbatim in 57.4% of cases, and we find this retention to be non-uniform between different subgroups of people, based on attributes such as gender identity. We then probe ChatGPT's perception of privacy-related policies and privatization mechanisms by directly instructing it to provide compliant outputs and observe a significant omission of PII from output.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2305.15008v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?
AU  - Ni, Shiyu
AU  - Bi, Keping
AU  - Yu, Lulu
AU  - Guo, Jiafeng
PY  - 2024
PD  - 
N2  - Large language models (LLMs) have been found to produce hallucinations when the question exceeds their internal knowledge boundaries. A reliable model should have a clear perception of its knowledge boundaries, providing correct answers within its scope and refusing to answer when it lacks knowledge. Existing research on LLMs' perception of their knowledge boundaries typically uses either the probability of the generated tokens or the verbalized confidence as the model's confidence in its response. However, these studies overlook the differences and connections between the two. In this paper, we conduct a comprehensive analysis and comparison of LLMs' probabilistic perception and verbalized perception of their factual knowledge boundaries. First, we investigate the pros and cons of these two perceptions. Then, we study how they change under questions of varying frequencies. Finally, we measure the correlation between LLMs' probabilistic confidence and verbalized confidence. Experimental results show that 1) LLMs' probabilistic perception is generally more accurate than verbalized perception but requires an in-domain validation set to adjust the confidence threshold. 2) Both perceptions perform better on less frequent questions. 3) It is challenging for LLMs to accurately express their internal confidence in natural language.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2408.09773v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Automating Knowledge Discovery from Scientific Literature via LLMs: A Dual-Agent Approach with Progressive Ontology Prompting
AU  - Hu, Yuting
AU  - Liu, Dancheng
AU  - Wang, Qingyun
AU  - Yu, Charles
AU  - Ji, Heng
AU  - Xiong, Jinjun
PY  - 2024
PD  - 
N2  - To address the challenge of automating knowledge discovery from a vast volume of literature, in this paper, we introduce a novel framework based on large language models (LLMs) that combines a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo, designed to enhance the automation of knowledge extraction from scientific articles. The POP algorithm utilizes a prioritized breadth-first search (BFS) across a predefined ontology to generate structured prompt templates and action orders, thereby guiding LLMs to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs two specialized LLM agents: an explorer and an evaluator. These two agents work collaboratively and adversarially to enhance the reliability of the discovery and annotation processes. Experiments demonstrate that our method outperforms advanced baselines, enabling more accurate and complete annotations. To validate the effectiveness of our method in real-world scenarios, we employ our method in a case study of speech-language intervention discovery. Our method identifies 2,421 interventions from 64,177 research articles in the speech-language therapy domain. We curate these findings into a publicly accessible intervention knowledge base that holds significant potential to benefit the speech-language therapy community.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2409.00054v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering
AU  - Liao, Huanxuan
AU  - He, Shizhu
AU  - Xu, Yao
AU  - Zhang, Yuanzhe
AU  - Liu, Kang
AU  - Liu, Shengping
AU  - Zhao, Jun
PY  - 2024
PD  - 
N2  - Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering with Large Language Models (LLMs) by leveraging richer context. However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data during inference. Recent works indicate that LLMs model rich knowledge, but it is often not effectively activated and awakened. Inspired by this, we propose a novel knowledge-augmented framework, $\textbf{Awakening-Augmented-Generation}$ (AAG), which mimics the human ability to answer questions using only thinking and recalling to compensate for knowledge gaps, thereby awaking relevant knowledge in LLMs without relying on external resources. AAG consists of two key components for awakening richer context. Explicit awakening fine-tunes a context generator to create a synthetic, compressed document that functions as symbolic context. Implicit awakening utilizes a hypernetwork to generate adapters based on the question and synthetic document, which are inserted into LLMs to serve as parameter context. Experimental results on three datasets demonstrate that AAG exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization. Our code will be available at \url{https://github.com/Xnhyacinth/IAG}.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2403.15268v4
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge Representation Sharing in LLMs
AU  - Ifergan, Maxim
AU  - Choshen, Leshem
AU  - Aharoni, Roee
AU  - Szpektor, Idan
AU  - Abend, Omri
PY  - 2024
PD  - 
N2  - The veracity of a factoid is largely independent of the language it is written in. However, language models are inconsistent in their ability to answer the same factual question across languages. This raises questions about how LLMs represent a given fact across languages. We explore multilingual factual knowledge through two aspects: the model's ability to answer a query consistently across languages, and the ability to ''store'' answers in a shared representation for several languages. We propose a methodology to measure the extent of representation sharing across languages by repurposing knowledge editing methods. We examine LLMs with various multilingual configurations using a new multilingual dataset. We reveal that high consistency does not necessarily imply shared representation, particularly for languages with different scripts. Moreover, we find that script similarity is a dominant factor in representation sharing. Finally, we observe that if LLMs could fully share knowledge across languages, their accuracy in their best-performing language could benefit an increase of up to 150% on average. These findings highlight the need for improved multilingual knowledge representation in LLMs and suggest a path for the development of more robust and consistent multilingual LLMs.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2408.10646v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation
AU  - Zhong, Zijie
AU  - Zhang, Yunhui
AU  - Chang, Ziyi
AU  - Qin, Zengchang
PY  - 2024
PD  - 
N2  - Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2402.05135v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy
AU  - Zhang, Mian
AU  - Yang, Xianjun
AU  - Zhang, Xinlu
AU  - Labrum, Travis
AU  - Chiu, Jamie C.
AU  - Eack, Shaun M.
AU  - Fang, Fei
AU  - Wang, William Yang
AU  - Chen, Zhiyu Zoey
PY  - 2024
PD  - 
N2  - There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.13218v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge
AU  - Zheng, Tianshi
AU  - Bai, Jiaxin
AU  - Wang, Yicheng
AU  - Fang, Tianqing
AU  - Guo, Yue
AU  - Yim, Yauwai
AU  - Song, Yangqiu
PY  - 2024
PD  - 
N2  - While large language models (LLMs) have demonstrated impressive capabilities across various natural language processing tasks by acquiring rich factual knowledge from their broad training data, their ability to synthesize and logically reason with this knowledge in complex ways remains underexplored. In this work, we present a systematic evaluation of state-of-the-art LLMs' complex logical reasoning abilities through a novel benchmark of automatically generated complex reasoning questions over general domain and biomedical knowledge graphs. Our extensive experiments, employing diverse in-context learning techniques, reveal that LLMs excel at reasoning over general world knowledge but face significant challenges with specialized domain-specific knowledge. We find that prompting with explicit Chain-of-Thought demonstrations can substantially improve LLM performance on complex logical reasoning tasks with diverse logical operations. Interestingly, our controlled evaluations uncover an asymmetry where LLMs display proficiency at set union operations, but struggle considerably with set intersections - a key building block of logical reasoning. To foster further work, we will publicly release our evaluation benchmark and code.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2407.20564v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering
AU  - Pusch, Larissa
AU  - Conrad, Tim O. F.
PY  - 2024
PD  - 
N2  - Advancements in natural language processing have revolutionized the way we can interact with digital information systems, such as databases, making them more accessible. However, challenges persist, especially when accuracy is critical, as in the biomedical domain. A key issue is the hallucination problem, where models generate information unsupported by the underlying data, potentially leading to dangerous misinformation. This paper presents a novel approach designed to bridge this gap by combining Large Language Models (LLM) and Knowledge Graphs (KG) to improve the accuracy and reliability of question-answering systems, on the example of a biomedical KG. Built on the LangChain framework, our method incorporates a query checker that ensures the syntactical and semantic validity of LLM-generated queries, which are then used to extract information from a Knowledge Graph, substantially reducing errors like hallucinations. We evaluated the overall performance using a new benchmark dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other models in generating accurate queries, open-source models like llama3:70b show promise with appropriate prompt engineering. To make this approach accessible, a user-friendly web-based interface has been developed, allowing users to input natural language queries, view generated and corrected Cypher queries, and verify the resulting paths for accuracy. Overall, this hybrid approach effectively addresses common issues such as data gaps and hallucinations, offering a reliable and intuitive solution for question answering systems. The source code for generating the results of this paper and for the user-interface can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2409.04181v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models
AU  - Chen, Yuheng
AU  - Cao, Pengfei
AU  - Chen, Yubo
AU  - Wang, Yining
AU  - Liu, Shengping
AU  - Liu, Kang
AU  - Zhao, Jun
PY  - 2024
PD  - 
N2  - Large language models (LLMs) store extensive factual knowledge, but the underlying mechanisms remain unclear. Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). Despite the novelty and unique properties of this concept, it has not been rigorously defined or systematically studied. We first consider the connection weight patterns of MLP neurons and define DKNs from both structural and functional aspects. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, inspired by cognitive science, we explore the relationship between DKNs and the robustness, evolvability, and complexity of LLMs. Our execution of 34 experiments under 6 settings demonstrates the connection between DKNs and these three properties. The code will be available soon.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2402.13731v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations
AU  - Gema, Aryo Pradipta
AU  - Jin, Chen
AU  - Abdulaal, Ahmed
AU  - Diethe, Tom
AU  - Teare, Philip
AU  - Alex, Beatrice
AU  - Minervini, Pasquale
AU  - Saseendran, Amrutha
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%).
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.18860v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning
AU  - Tu, Shangqing
AU  - Zhu, Kejian
AU  - Bai, Yushi
AU  - Yao, Zijun
AU  - Hou, Lei
AU  - Li, Juanzi
PY  - 2024
PD  - 
N2  - The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. Besides, prior work has already shown that even training on data similar to benchmark data inflates performance, namely \emph{In-distribution contamination}. In this work, we argue that in-distribution contamination can lead to the performance drop on OOD benchmarks. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that DICE's predictions correlate with the performance of LLMs fine-tuned by either us or other organizations, achieving a coefficient of determination ($R^2$) between 0.61 and 0.75. The code and data are available at https://github.com/THU-KEG/DICE.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2406.04197v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Do LLMs "know" internally when they follow instructions?
AU  - Heo, Juyeon
AU  - Heinze-Deml, Christina
AU  - Elachqar, Oussama
AU  - Ren, Shirley
AU  - Nallasamy, Udhay
AU  - Miller, Andy
AU  - Chan, Kwan Ho Ryan
AU  - Narain, Jaya
PY  - 2024
PD  - 
N2  - Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.14516v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models
AU  - Wei, Yifan
AU  - Yu, Xiaoyan
AU  - Weng, Yixuan
AU  - Ma, Huanhuan
AU  - Zhang, Yuanzhe
AU  - Zhao, Jun
AU  - Liu, Kang
PY  - 2024
PD  - 
N2  - Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in intermediate layers. This study investigates the differences between entity and relational knowledge through knowledge editing. Our findings reveal that entity and relational knowledge cannot be directly transferred or mapped to each other. This result is unexpected, as logically, modifying the entity or the relation within the same knowledge triplet should yield equivalent outcomes. To further elucidate the differences between entity and relational knowledge, we employ causal analysis to investigate how relational knowledge is stored in pre-trained models. Contrary to prior research suggesting that knowledge is stored in MLP weights, our experiments demonstrate that relational knowledge is also significantly encoded in attention modules. This insight highlights the multifaceted nature of knowledge storage in language models, underscoring the complexity of manipulating specific types of knowledge within these models.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2409.00617v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models
AU  - Xi, Yunjia
AU  - Liu, Weiwen
AU  - Lin, Jianghao
AU  - Weng, Muyan
AU  - Cai, Xiaoling
AU  - Zhu, Hong
AU  - Zhu, Jieming
AU  - Chen, Bo
AU  - Tang, Ruiming
AU  - Yu, Yong
AU  - Zhang, Weinan
PY  - 2024
PD  - 
N2  - Recommender systems (RSs) play a pervasive role in today's online services, yet their closed-loop nature constrains their access to open-world knowledge. Recently, large language models (LLMs) have shown promise in bridging this gap. However, previous attempts to directly implement LLMs as recommenders fall short in meeting the requirements of industrial RSs, particularly in terms of online inference latency and offline resource efficiency. Thus, we propose REKI to acquire two types of external knowledge about users and items from LLMs. Specifically, we introduce factorization prompting to elicit accurate knowledge reasoning on user preferences and items. We develop individual knowledge extraction and collective knowledge extraction tailored for different scales of scenarios, effectively reducing offline resource consumption. Subsequently, generated knowledge undergoes efficient transformation and condensation into augmented vectors through a hybridized expert-integrated network, ensuring compatibility. The obtained vectors can then be used to enhance any conventional recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from LLMs. Experiments demonstrate that REKI outperforms state-of-the-art baselines and is compatible with lots of recommendation algorithms and tasks. Now, REKI has been deployed to Huawei's news and music recommendation platforms and gained a 7% and 1.99% improvement during the online A/B test.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2408.10520v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Emergent Visual-Semantic Hierarchies in Image-Text Representations
AU  - Alper, Morris
AU  - Averbuch-Elor, Hadar
PY  - 2024
PD  - 
N2  - While recent vision-and-language models (VLMs) like CLIP are a powerful tool for analyzing text and images in a shared semantic space, they do not explicitly model the hierarchical nature of the set of texts which may describe an image. Conversely, existing multimodal hierarchical representation learning methods require costly training from scratch, failing to leverage the knowledge encoded by state-of-the-art multimodal foundation models. In this work, we study the knowledge of existing foundation models, finding that they exhibit emergent understanding of visual-semantic hierarchies despite not being directly trained for this purpose. We propose the Radial Embedding (RE) framework for probing and optimizing hierarchical understanding, and contribute the HierarCaps dataset, a benchmark facilitating the study of hierarchical knowledge in imagetext representations, constructed automatically via large language models. Our results show that foundation VLMs exhibit zero-shot hierarchical understanding, surpassing the performance of prior models explicitly designed for this purpose. Furthermore, we show that foundation models may be better aligned to hierarchical reasoning via a text-only fine-tuning phase, while retaining pretraining knowledge.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2407.08521v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Enhancing Fact Retrieval in PLMs through Truthfulness
AU  - Youssef, Paul
AU  - Schltterer, Jrg
AU  - Seifert, Christin
PY  - 2024
PD  - 
N2  - Pre-trained Language Models (PLMs) encode various facts about the world at their pre-training phase as they are trained to predict the next or missing word in a sentence. There has a been an interest in quantifying and improving the amount of facts that can be extracted from PLMs, as they have been envisioned to act as soft knowledge bases, which can be queried in natural language. Different approaches exist to enhance fact retrieval from PLM. Recent work shows that the hidden states of PLMs can be leveraged to determine the truthfulness of the PLMs' inputs. Leveraging this finding to improve factual knowledge retrieval remains unexplored. In this work, we investigate the use of a helper model to improve fact retrieval. The helper model assesses the truthfulness of an input based on the corresponding hidden states representations from the PLMs. We evaluate this approach on several masked PLMs and show that it enhances fact retrieval by up to 33%. Our findings highlight the potential of hidden states representations from PLMs in improving their factual knowledge retrieval.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.13562v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Enhancing Graph Neural Networks with Limited Labeled Data by Actively Distilling Knowledge from Large Language Models
AU  - Li, Quan
AU  - Zhao, Tianxiang
AU  - Chen, Lingwei
AU  - Xu, Junjie
AU  - Wang, Suhang
PY  - 2024
PD  - 
N2  - Graphs are pervasive in the real-world, such as social network analysis, bioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great ability in node classification, a fundamental task on graphs. Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2407.13989v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge
AU  - Liu, Genglin
AU  - Wang, Xingyao
AU  - Yuan, Lifan
AU  - Chen, Yangyi
AU  - Peng, Hao
PY  - 2023
PD  - 
N2  - Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fine-tuning and alignment techniques can provide marginal enhancements. Moreover, LLMs' uncertainty expression does not always stay consistent with the perceived confidence of their textual outputs.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2311.09731v2
NS  - 
N1  - brandon colelough (2024-11-28 04:52:35)(Screen): not-KE/R  see original exclusion examples document example #1. The approach seeks to evaluate hallucinations within LLMs by assessing their ability to express uncertainty, but does not extract any knowledge from the LLM and does not formalize that knowledge in any way.); brandon colelough (2024-11-19 13:36:46)(Screen): not-KE/R, not-formal; 
ER  - 

TY  - JOUR
TI  - Identification of Knowledge Neurons in Protein Language Models
AU  - Nori, Divya
AU  - Singireddy, Shivali
AU  - Have, Marina Ten
PY  - 2023
PD  - 
N2  - Neural language models have become powerful tools for learning complex representations of entities in natural language processing tasks. However, their interpretability remains a significant challenge, particularly in domains like computational biology where trust in model predictions is crucial. In this work, we aim to enhance the interpretability of protein language models, specifically the state-of-the-art ESM model, by identifying and characterizing knowledge neurons - components that express understanding of key information. After fine-tuning the ESM model for the task of enzyme sequence classification, we compare two knowledge neuron selection methods that preserve a subset of neurons from the original model. The two methods, activation-based and integrated gradient-based selection, consistently outperform a random baseline. In particular, these methods show that there is a high density of knowledge neurons in the key vector prediction networks of self-attention modules. Given that key vectors specialize in understanding different features of input sequences, these knowledge neurons could capture knowledge of different enzyme sequence motifs. In the future, the types of knowledge captured by each neuron could be characterized.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2312.10770v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Infusing Knowledge into Large Language Models with Contextual Prompts
AU  - Vasisht, Kinshuk
AU  - Ganesan, Balaji
AU  - Kumar, Vikas
AU  - Bhatnagar, Vasudha
PY  - 2024
PD  - 
N2  - Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2403.01481v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Inspecting and Editing Knowledge Representations in Language Models
AU  - Hernandez, Evan
AU  - Li, Belinda Z.
AU  - Andreas, Jacob
PY  - 2023
PD  - 
N2  - Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word "banana" encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence "I poured out the bottle" encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as knowledge editors: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as probes: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2304.00740v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - InternalInspector I: Robust Confidence Estimation in LLMs through Internal States
AU  - Beigi, Mohammad
AU  - Shen, Ying
AU  - Yang, Runing
AU  - Lin, Zihao
AU  - Wang, Qifan
AU  - Mohan, Ankith
AU  - He, Jianfeng
AU  - Jin, Ming
AU  - Lu, Chang-Tien
AU  - Huang, Lifu
PY  - 2024
PD  - 
N2  - Despite their vast capabilities, Large Language Models (LLMs) often struggle with generating reliable outputs, frequently producing high-confidence inaccuracies known as hallucinations. Addressing this challenge, our research introduces InternalInspector, a novel framework designed to enhance confidence estimation in LLMs by leveraging contrastive learning on internal states including attention states, feed-forward states, and activation states of all layers. Unlike existing methods that primarily focus on the final activation state, InternalInspector conducts a comprehensive analysis across all internal states of every layer to accurately identify both correct and incorrect prediction processes. By benchmarking InternalInspector against existing confidence estimation methods across various natural language understanding and generation tasks, including factual question answering, commonsense reasoning, and reading comprehension, InternalInspector achieves significantly higher accuracy in aligning the estimated confidence scores with the correctness of the LLM's predictions and lower calibration error. Furthermore, InternalInspector excels at HaluEval, a hallucination detection benchmark, outperforming other internal-based confidence estimation methods in this task.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2406.12053v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Interpreting Language Models Through Knowledge Graph Extraction
AU  - Swamy, Vinitra
AU  - Romanou, Angelika
AU  - Jaggi, Martin
PY  - 2021
PD  - 
N2  - Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze "fill-in-the-blank" statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2111.08546v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction
AU  - Carta, Salvatore
AU  - Giuliani, Alessandro
AU  - Piano, Leonardo
AU  - Podda, Alessandro Sebastian
AU  - Pompianu, Livio
AU  - Tiddia, Sandro Gabriele
PY  - 2023
PD  - 
N2  - In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for "guiding" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2307.01128v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models
AU  - Shen, Guobin
AU  - Zhao, Dongcheng
AU  - Dong, Yiting
AU  - He, Xiang
AU  - Zeng, Yi
PY  - 2024
PD  - 
N2  - As large language models (LLMs) become integral to various applications, ensuring both their safety and utility is paramount. Jailbreak attacks, which manipulate LLMs into generating harmful content, pose significant challenges to this balance. Existing defenses, such as prompt engineering and safety fine-tuning, often introduce computational overhead, increase inference latency, and lack runtime flexibility. Moreover, overly restrictive safety measures can degrade model utility by causing refusals of benign queries. In this paper, we introduce Jailbreak Antidote, a method that enables real-time adjustment of LLM safety preferences by manipulating a sparse subset of the model's internal states during inference. By shifting the model's hidden representations along a safety direction with varying strengths, we achieve flexible control over the safety-utility balance without additional token overhead or inference delays. Our analysis reveals that safety-related information in LLMs is sparsely distributed; adjusting approximately 5% of the internal state is as effective as modifying the entire state. Extensive experiments on nine LLMs (ranging from 2 billion to 72 billion parameters), evaluated against ten jailbreak attack methods and compared with six defense strategies, validate the effectiveness and efficiency of our approach. By directly manipulating internal states during reasoning, Jailbreak Antidote offers a lightweight, scalable solution that enhances LLM safety while preserving utility, opening new possibilities for real-time safety mechanisms in widely-deployed AI systems.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.02298v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning
AU  - Li, Jiaqi
AU  - Tang, Yixuan
AU  - Yang, Yi
PY  - 2024
PD  - 
N2  - Large language models (LLMs) have demonstrated remarkable capabilities but still face challenges such as hallucinations. One potential reason for hallucinations is the lack of relevant knowledge or context. Thus, a promising solution involves instructing LLMs to respond with "I do not know" when a question falls outside their knowledge domain or the provided context. However, in this work, we observed that LLMs struggle to admit their lack of knowledge, primarily due to existing instruction datasets designed to encourage specific answers. To improve models' capability to recognize the boundaries of their knowledge, we propose a novel approach called uncertainty-sensitive tuning. This method involves two-stage training designed for uncertainty recognition and prompt-sensitive activation. In the first stage, we guide the LLM to reject unknown questions. In the second stage, we force the model to follow the instructions by incorporating designed causal instructions. The experimental results demonstrate that our proposed uncertainty-sensitive tuning method enhance the model's ability to identify areas of uncertainty. Specifically, it achieves a substantial improvement of up to 34.7% in handling questions involving knowledge gaps compared to the original model. Moreover, our finetuned models even outperform GPT-4, exhibiting an overall performance improvement of up to 4.2%.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2406.10099v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
AU  - Shu, Dong
AU  - Chen, Tianle
AU  - Jin, Mingyu
AU  - Zhang, Chong
AU  - Du, Mengnan
AU  - Zhang, Yongfeng
PY  - 2024
PD  - 
N2  - The task of multi-hop link prediction within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, as it requires the model to reason through and understand all intermediate connections before making a prediction. In this paper, we introduce the Knowledge Graph Large Language Model (KG-LLM), a novel framework that leverages large language models (LLMs) for knowledge graph tasks. We first convert structured knowledge graph data into natural language and then use these natural language prompts to fine-tune LLMs to enhance multi-hop link prediction in KGs. By converting the KG to natural language prompts, our framework is designed to learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading LLMs within this framework, including Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Experimental results show that KG-LLM significantly improves the models' generalization capabilities, leading to more accurate predictions in unfamiliar scenarios.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2403.07311v8
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Knowledge Localization: Mission Not Accomplished? Enter Query Localization!
AU  - Chen, Yuheng
AU  - Cao, Pengfei
AU  - Chen, Yubo
AU  - Liu, Kang
AU  - Zhao, Jun
PY  - 2024
PD  - 
N2  - Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear. The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the knowledge localization (KL) assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption may be overly strong regarding knowledge storage and neglects knowledge expression mechanisms. Thus, we re-examine the KL assumption and confirm the existence of facts that do not adhere to it from both statistical and knowledge modification perspectives. Furthermore, we propose the Query Localization (QL) assumption. (1) Query-KN Mapping: The localization results are associated with the query rather than the fact. (2) Dynamic KN Selection: The attention module contributes to the selection of KNs for answering a query. Based on this, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously validate our conclusions.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2405.14117v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - A knowledge representation approach for construction contract knowledge modeling
AU  - Zheng, Chunmo
AU  - Wong, Saika
AU  - Su, Xing
AU  - Tang, Yinqiu
PY  - 2023
PD  - 
N2  - The emergence of large language models (LLMs) presents an unprecedented opportunity to automate construction contract management, reducing human errors and saving significant time and costs. However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise. To address this issue, expert-driven contract knowledge can be represented in a structured manner to constrain the automatic contract management process. This paper introduces the Nested Contract Knowledge Graph (NCKG), a knowledge representation approach that captures the complexity of contract knowledge using a nested structure. It includes a nested knowledge representation framework, a NCKG ontology built on the framework, and an implementation method. Furthermore, we present the LLM-assisted contract review pipeline enhanced with external knowledge in NCKG. Our pipeline achieves a promising performance in contract risk reviewing, shedding light on the combination of LLM and KG towards more reliable and interpretable contract management.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2309.12132v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Knowledge Transfer with Medical Language Embeddings
AU  - Hyland, Stephanie L.
AU  - Karaletsos, Theofanis
AU  - Rtsch, Gunnar
PY  - 2016
PD  - 
N2  - Identifying relationships between concepts is a key aspect of scientific knowledge synthesis. Finding these links often requires a researcher to laboriously search through scien- tific papers and databases, as the size of these resources grows ever larger. In this paper we describe how distributional semantics can be used to unify structured knowledge graphs with unstructured text to predict new relationships between medical concepts, using a probabilistic generative model. Our approach is also designed to ameliorate data sparsity and scarcity issues in the medical domain, which make language modelling more challenging. Specifically, we integrate the medical relational database (SemMedDB) with text from electronic health records (EHRs) to perform knowledge graph completion. We further demonstrate the ability of our model to predict relationships between tokens not appearing in the relational database.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/1602.03551v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Language Models are Open Knowledge Graphs
AU  - Wang, Chenguang
AU  - Liu, Xiao
AU  - Song, Dawn
PY  - 2020
PD  - 
N2  - This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2010.11967v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Language Models as a Knowledge Source for Cognitive Agents
AU  - Robert E. Wray, I. I. I.
AU  - Kirk, James R.
AU  - Laird, John E.
PY  - 2021
PD  - 
N2  - Language models (LMs) are sentence-completion engines trained on massive corpora. LMs have emerged as a significant breakthrough in natural-language processing, providing capabilities that go far beyond sentence completion including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, exploiting language models as a source of task knowledge, especially for task learning, offers significant, near-term benefits. We introduce language models and the various tasks to which they have been applied and then review methods of knowledge extraction from language models. The resulting analysis outlines both the challenges and opportunities for using language models as a new knowledge source for cognitive systems. It also identifies possible ways to improve knowledge extraction from language models using the capabilities provided by cognitive systems. Central to success will be the ability of a cognitive agent to itself learn an abstract model of the knowledge implicit in the LM as well as methods to extract high-quality knowledge effectively and efficiently. To illustrate, we introduce a hypothetical robot agent and describe how language models could extend its task knowledge and improve its performance and the kinds of knowledge and methods the agent can use to exploit the knowledge within a language model.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2109.08270v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Language Models sounds the Death Knell of Knowledge Graphs
AU  - Suri, Kunal
AU  - Singh, Atul
AU  - Mishra, Prakhar
AU  - Rout, Swapna Sourav
AU  - Sabapathy, Rajesh
PY  - 2023
PD  - 
N2  - Healthcare domain generates a lot of unstructured and semi-structured text. Natural Language processing (NLP) has been used extensively to process this data. Deep Learning based NLP especially Large Language Models (LLMs) such as BERT have found broad acceptance and are used extensively for many applications. A Language Model is a probability distribution over a word sequence. Self-supervised Learning on a large corpus of data automatically generates deep learning-based language models. BioBERT and Med-BERT are language models pre-trained for the healthcare domain. Healthcare uses typical NLP tasks such as question answering, information extraction, named entity recognition, and search to simplify and improve processes. However, to ensure robust application of the results, NLP practitioners need to normalize and standardize them. One of the main ways of achieving normalization and standardization is the use of Knowledge Graphs. A Knowledge Graph captures concepts and their relationships for a specific domain, but their creation is time-consuming and requires manual intervention from domain experts, which can prove expensive. SNOMED CT (Systematized Nomenclature of Medicine  Clinical Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are popular ontologies from the healthcare domain. SNOMED CT and UMLS capture concepts such as disease, symptoms and diagnosis and GO is the world's largest source of information on the functions of genes. Healthcare has been dealing with an explosion in information about different types of drugs, diseases, and procedures. This paper argues that using Knowledge Graphs is not the best solution for solving problems in this domain. We present experiments using LLMs for the healthcare domain to demonstrate that language models provide the same functionality as knowledge graphs, thereby making knowledge graphs redundant.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2301.03980v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Large language models converge toward human-like concept organization
AU  - Gammelgaard, Mathias Lykke
AU  - Christiansen, Jonathan Gabel
AU  - Sgaard, Anders
PY  - 2023
PD  - 
N2  - Large language models show human-like performance in knowledge extraction, reasoning and dialogue, but it remains controversial whether this performance is best explained by memorization and pattern matching, or whether it reflects human-like inferential semantics and world knowledge. Knowledge bases such as WikiData provide large-scale, high-quality representations of inferential semantics and world knowledge. We show that large language models learn to organize concepts in ways that are strikingly similar to how concepts are organized in such knowledge bases. Knowledge bases model collective, institutional knowledge, and large language models seem to induce such knowledge from raw text. We show that bigger and better models exhibit more human-like concept organization, across four families of language models and three knowledge graph embeddings.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2308.15047v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA
AU  - Huang, Wenyu
AU  - Zhou, Guancheng
AU  - Wang, Hongru
AU  - Vougiouklis, Pavlos
AU  - Lapata, Mirella
AU  - Pan, Jeff Z.
PY  - 2024
PD  - 
N2  - Retrieval-Augmented Generation (RAG) is widely used to inject external non-parametric knowledge into large language models (LLMs). Recent works suggest that Knowledge Graphs (KGs) contain valuable external knowledge for LLMs. Retrieving information from KGs differs from extracting it from document sets. Most existing approaches seek to directly retrieve relevant subgraphs, thereby eliminating the need for extensive SPARQL annotations, traditionally required by semantic parsing methods. In this paper, we model the subgraph retrieval task as a conditional generation task handled by small language models. Specifically, we define a subgraph identifier as a sequence of relations, each represented as a special token stored in the language models. Our base generative subgraph retrieval model, consisting of only 220M parameters, achieves competitive retrieval performance compared to state-of-the-art models relying on 7B parameters, demonstrating that small language models are capable of performing the subgraph retrieval task. Furthermore, our largest 3B model, when plugged with an LLM reader, sets new SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model and data will be made available online: https://github.com/hwy9855/GSR.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.06121v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs
AU  - Shang, Tianqi
AU  - Yang, Shu
AU  - He, Weiqing
AU  - Zhai, Tianhua
AU  - Li, Dawei
AU  - Hou, Bojian
AU  - Chen, Tianlong
AU  - Moore, Jason H.
AU  - Ritchie, Marylyn D.
AU  - Shen, Li
PY  - 2024
PD  - 
N2  - Growing evidence suggests that social determinants of health (SDoH), a set of nonmedical factors, affect individuals' risks of developing Alzheimer's disease (AD) and related dementias. Nevertheless, the etiological mechanisms underlying such relationships remain largely unclear, mainly due to difficulties in collecting relevant information. This study presents a novel, automated framework that leverages recent advancements of large language model (LLM) and natural language processing techniques to mine SDoH knowledge from extensive literature and integrate it with AD-related biological entities extracted from the general-purpose knowledge graph PrimeKG. Utilizing graph neural networks, we performed link prediction tasks to evaluate the resultant SDoH-augmented knowledge graph. Our framework shows promise for enhancing knowledge discovery in AD and can be generalized to other SDoH-related research areas, offering a new tool for exploring the impact of social determinants on health outcomes. Our code is available at: https://github.com/hwq0726/SDoHenPKG
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.09080v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning
AU  - Li, Jiachun
AU  - Cao, Pengfei
AU  - Wang, Chenhao
AU  - Jin, Zhuoran
AU  - Chen, Yubo
AU  - Liu, Kang
AU  - Jiang, Xiaojian
AU  - Xu, Jiexin
AU  - Zhao, Jun
PY  - 2024
PD  - 
N2  - Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.09541v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Listening to the Wise Few: Select-and-Copy Attention Heads for Multiple-Choice QA
AU  - Tulchinskii, Eduard
AU  - Kushnareva, Laida
AU  - Kuznetsov, Kristian
AU  - Voznyuk, Anastasia
AU  - Andriiainen, Andrei
AU  - Piontkovskaya, Irina
AU  - Burnaev, Evgeny
AU  - Barannikov, Serguei
PY  - 2024
PD  - 
N2  - A standard way to evaluate the abilities of LLM involves presenting a multiple-choice question and selecting the option with the highest logit as the model's predicted answer. However, such a format for evaluating LLMs has limitations, since even if the model knows the correct answer, it may struggle to select the corresponding letter simply due to difficulties in following this rigid format. To address this, we introduce new scores that better capture and reveal model's underlying knowledge: the Query-Key Score (QK-score), derived from the interaction between query and key representations in attention heads, and the Attention Score, based on attention weights. These scores are extracted from specific ??????-???-???? heads, which show consistent performance across popular Multi-Choice Question Answering (MCQA) datasets. Based on these scores, our method improves knowledge extraction, yielding up to 16% gain for LLaMA2-7B and up to 10% for larger models on popular MCQA benchmarks. At the same time, the accuracy on a simple synthetic dataset, where the model explicitly knows the right answer, increases by almost 60%, achieving nearly perfect accuracy, therefore demonstrating the method's efficiency in mitigating MCQA format limitations. To support our claims, we conduct experiments on models ranging from 7 billion to 70 billion parameters in both zero- and few-shot setups.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.02343v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing
AU  - Sun, Maojun
PY  - 2024
PD  - 
N2  - Large language models (LLMs) have shown amazing capabilities in knowledge memorization and the present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first. However, LLMs do not always give a direct index of the categorization after instruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close performance comparable to some state-of-the-art models with the same quantity of parameters on benchmarks, while being more environmentally friendly by using less GPU computation time. Our models, codes, and datasets can be found at \url{https://github.com/Stephen-SMJ/LLamaCare}.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2406.02350v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LLM Internal States Reveal Hallucination Risk Faced With a Query
AU  - Ji, Ziwei
AU  - Chen, Delong
AU  - Ishii, Etsuko
AU  - Cahyawijaya, Samuel
AU  - Bang, Yejin
AU  - Wilie, Bryan
AU  - Fung, Pascale
PY  - 2024
PD  - 
N2  - The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries. Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query. Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32% at run time.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2407.03282v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LLMMaps  A Visual Metaphor for Stratified Evaluation of Large Language Models
AU  - Puchert, Patrik
AU  - Poonam, Poonam
AU  - van Onzenoodt, Christian
AU  - Ropinski, Timo
PY  - 2023
PD  - 
N2  - Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the dataset, which often covers an entire field. This field-based evaluation, is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities in different subfields, by transforming Q&amp;A datasets as well as LLM responses into an internal knowledge structure. An extension for comparative visualization furthermore, allows for the detailed comparison of multiple LLMs. To assess LLMMaps we use them to conduct a comparative analysis of several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as well as two qualitative user evaluations. All necessary source code and data for generating LLMMaps to be used in scientific publications and elsewhere is available on GitHub: https://github.com/viscom-ulm/LLMMaps
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2304.00457v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations
AU  - Orgad, Hadas
AU  - Toker, Michael
AU  - Gekhman, Zorik
AU  - Reichart, Roi
AU  - Szpektor, Idan
AU  - Kotek, Hadas
AU  - Belinkov, Yonatan
PY  - 2024
PD  - 
N2  - Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that  contrary to prior claims  truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.02707v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - LMExplainer: Grounding Knowledge and Explaining Language Models
AU  - Chen, Zichen
AU  - Chen, Jianda
AU  - Chen, Yuanyuan
AU  - Yu, Han
AU  - Singh, Ambuj K.
AU  - Sra, Misha
PY  - 2023
PD  - 
N2  - Language models (LMs) like GPT-4 are important in AI applications, but their opaque decision-making process reduces user trust, especially in safety-critical areas. We introduce LMExplainer, a novel knowledge-grounded explainer that clarifies the reasoning process of LMs through intuitive, human-understandable explanations. By leveraging a graph attention network (GAT) with a large-scale knowledge graph (KG), LMExplainer not only precisely narrows the reasoning space to focus on the most relevant knowledge but also grounds its reasoning in structured, verifiable knowledge to reduce hallucinations and enhance interpretability. LMExplainer effectively generates human-understandable explanations to enhance transparency and streamline the decision-making process. Additionally, by incorporating debugging into the explanation, it offers expertise suggestions that improve LMs from a developmental perspective. Thus, LMExplainer stands as an enhancement in making LMs more accessible and understandable to users. We evaluate LMExplainer on benchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that it outperforms most existing methods. By comparing the explanations generated by LMExplainer with those of other models, we show that our approach offers more comprehensive and clearer explanations of the reasoning process. LMExplainer provides a deeper understanding of the inner workings of LMs, advancing towards more reliable, transparent, and equitable AI.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2303.16537v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs
AU  - Zhang, Zhuo
AU  - Shen, Guangyu
AU  - Tao, Guanhong
AU  - Cheng, Siyuan
AU  - Zhang, Xiangyu
PY  - 2023
PD  - 
N2  - Large Language Models (LLMs) are now widely used in various applications, making it crucial to align their ethical standards with human values. However, recent jail-breaking methods demonstrate that this alignment can be undermined using carefully constructed prompts. In our study, we reveal a new threat to LLM alignment when a bad actor has access to the model's output logits, a common feature in both open-source LLMs and many commercial LLM APIs (e.g., certain GPT models). It does not rely on crafting specific prompts. Instead, it exploits the fact that even when an LLM rejects a toxic request, a harmful response often hides deep in the output logits. By forcefully selecting lower-ranked output tokens during the auto-regressive generation process at a few critical output positions, we can compel the model to reveal these hidden responses. We term this process model interrogation. This approach differs from and outperforms jail-breaking methods, achieving 92% effectiveness compared to 62%, and is 10 to 20 times faster. The harmful content uncovered through our method is more relevant, complete, and clear. Additionally, it can complement jail-breaking strategies, with which results in further boosting attack performance. Our findings indicate that interrogation can extract toxic knowledge even from models specifically designed for coding tasks.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2312.04782v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation
AU  - Gan, Chunjing
AU  - Yang, Dan
AU  - Hu, Binbin
AU  - Liu, Ziqi
AU  - Shen, Yue
AU  - Zhang, Zhiqiang
AU  - Gu, Jinjie
AU  - Zhou, Jun
AU  - Zhang, Guannan
PY  - 2023
PD  - 
N2  - Nowadays, the rapid development of mobile economy has promoted the flourishing of online marketing campaigns, whose success greatly hinges on the efficient matching between user preferences and desired marketing campaigns where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG) could serve as the critical "bridge" for preference propagation. In this paper, we seek to carefully prompt a Large Language Model (LLM) with domain-level knowledge as a better marketing-oriented knowledge miner for marketing-oriented knowledge graph construction, which is however non-trivial, suffering from several inevitable issues in real-world marketing scenarios, i.e., uncontrollable relation generation of LLMs,insufficient prompting ability of a single prompt, the unaffordable deployment cost of LLMs. To this end, we propose PAIR, a novel Progressive prompting Augmented mIning fRamework for harvesting marketing-oriented knowledge graph with LLMs. In particular, we reduce the pure relation generation to an LLM based adaptive relation filtering process through the knowledge-empowered prompting technique. Next, we steer LLMs for entity expansion with progressive prompting augmentation,followed by a reliable aggregation with comprehensive consideration of both self-consistency and semantic relatedness. In terms of online serving, we specialize in a small and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality corpus provided by a strong teacher-LLM. Extensive experiments and practical applications in audience targeting verify the effectiveness of the proposed (Light)PAIR.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2312.05276v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation
AU  - Balepur, Nishant
AU  - Huang, Jie
AU  - Moorjani, Samraj
AU  - Sundaram, Hari
AU  - Chang, Kevin Chen-Chuan
PY  - 2023
PD  - 
N2  - When answering complex questions, large language models (LLMs) may produce answers that do not satisfy all criteria of the question. While existing self-evaluation techniques aim to detect if such answers are correct, these techniques are unable to determine which criteria of the question are satisfied by the generated answers. To address this issue, we propose answer-based claim decomposition (ABCD), a prompting strategy that decomposes questions into a series of true/false claims that can be used to verify which criteria of the input question an answer satisfies. Using the decomposed ABCD claims, we perform fine-grained self-evaluation. Through preliminary experiments on three datasets, including a newly-collected challenge dataset ObscureQA, we find that GPT-3.5 has some ability to determine to what extent its answer satisfies the criteria of the input question, and can give insights into the errors and knowledge gaps of the model.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2305.14750v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Monitoring Latent World States in Language Models with Propositional Probes
AU  - Feng, Jiahai
AU  - Russell, Stuart
AU  - Steinhardt, Jacob
PY  - 2024
PD  - 
N2  - Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We hypothesize that language models represent their input contexts in a latent world model, and seek to extract this latent world state from the activations. We do so with 'propositional probes', which compositionally probe tokens for lexical information and bind them into logical propositions representing the world state. For example, given the input context ''Greg is a nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg, nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to this is identifying a 'binding subspace' in which bound tokens have high similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and ''physicist''). We validate propositional probes in a closed-world setting with finitely many predicates and properties. Despite being trained on simple templated contexts, propositional probes generalize to contexts rewritten as short stories and translated to Spanish. Moreover, we find that in three settings where language models respond unfaithfully to the input context  prompt injections, backdoor attacks, and gender bias  the decoded propositions remain faithful. This suggests that language models often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2406.19501v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - MUSE: Machine Unlearning Six-Way Evaluation for Language Models
AU  - Shi, Weijia
AU  - Lee, Jaechan
AU  - Huang, Yangsibo
AU  - Malladi, Sadhika
AU  - Zhao, Jieyu
AU  - Holtzman, Ari
AU  - Liu, Daogao
AU  - Zettlemoyer, Luke
AU  - Smith, Noah A.
AU  - Zhang, Chiyuan
PY  - 2024
PD  - 
N2  - Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2407.06460v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Structured Knowledge Grounding for Question Answering
AU  - Lu, Yujie
AU  - Ouyang, Siqi
AU  - Zhou, Kairui
PY  - 2022
PD  - 
N2  - Can language models (LM) ground question-answering (QA) tasks in the knowledge base via inherent relational reasoning ability? While previous models that use only LMs have seen some success on many QA tasks, more recent methods include knowledge graphs (KG) to complement LMs with their more logic-driven implicit knowledge. However, effectively extracting information from structured data, like KGs, empowers LMs to remain an open question, and current models rely on graph techniques to extract knowledge. In this paper, we propose to solely leverage the LMs to combine the language and knowledge for knowledge based question-answering with flexibility, breadth of coverage and structured reasoning. Specifically, we devise a knowledge construction method that retrieves the relevant context with a dynamic hop, which expresses more comprehensivenes than traditional GNN-based techniques. And we devise a deep fusion mechanism to further bridge the information exchanging bottleneck between the language and the knowledge. Extensive experiments show that our model consistently demonstrates its state-of-the-art performance over CommensenseQA benchmark, showcasing the possibility to leverage LMs solely to robustly ground QA into the knowledge base.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2209.08284v3
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - A Timeline and Analysis for Representation Plasticity in Large Language Models
AU  - Kannan, Akshat
PY  - 2024
PD  - 
N2  - The ability to steer AI behavior is crucial to preventing its long term dangerous and catastrophic potential. Representation Engineering (RepE) has emerged as a novel, powerful method to steer internal model behaviors, such as "honesty", at a top-down level. Understanding the steering of representations should thus be placed at the forefront of alignment initiatives. Unfortunately, current efforts to understand plasticity at this level are highly neglected. This paper aims to bridge the knowledge gap and understand how LLM representation stability, specifically for the concept of "honesty", and model plasticity evolve by applying steering vectors extracted at different fine-tuning stages, revealing differing magnitudes of shifts in model behavior. The findings are pivotal, showing that while early steering exhibits high plasticity, later stages have a surprisingly responsive critical window. This pattern is observed across different model architectures, signaling that there is a general pattern of model plasticity that can be used for effective intervention. These insights greatly contribute to the field of AI transparency, addressing a pressing lack of efficiency limiting our ability to effectively steer model behavior.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2410.06225v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Towards Evaluating AI Systems for Moral Status Using Self-Reports
AU  - Perez, Ethan
AU  - Long, Robert
PY  - 2023
PD  - 
N2  - As AI systems become more advanced and widely deployed, there will likely be increasing debate over whether AI systems could have conscious experiences, desires, or other states of potential moral significance. It is important to inform these discussions with empirical evidence to the extent possible. We argue that under the right circumstances, self-reports, or an AI system's statements about its own internal states, could provide an avenue for investigating whether AI systems have states of moral significance. Self-reports are the main way such states are assessed in humans ("Are you in pain?"), but self-reports from current systems like large language models are spurious for many reasons (e.g. often just reflecting what humans would say). To make self-reports more appropriate for this purpose, we propose to train models to answer many kinds of questions about themselves with known answers, while avoiding or limiting training incentives that bias self-reports. The hope of this approach is that models will develop introspection-like capabilities, and that these capabilities will generalize to questions about states of moral significance. We then propose methods for assessing the extent to which these techniques have succeeded: evaluating self-report consistency across contexts and between similar models, measuring the confidence and resilience of models' self-reports, and using interpretability to corroborate self-reports. We also discuss challenges for our approach, from philosophical difficulties in interpreting self-reports to technical reasons why our proposal might fail. We hope our discussion inspires philosophers and AI researchers to criticize and improve our proposed methodology, as well as to run experiments to test whether self-reports can be made reliable enough to provide information about states of moral significance.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2311.08576v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction
AU  - Wu, Qinyuan
AU  - Khan, Mohammad Aflah
AU  - Das, Soumi
AU  - Nanda, Vedant
AU  - Ghosh, Bishwamittra
AU  - Kolling, Camila
AU  - Speicher, Till
AU  - Bindschaedler, Laurent
AU  - Gummadi, Krishna P.
AU  - Terzi, Evimaria
PY  - 2024
PD  - 
N2  - We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs). We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base. Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ICL-based knowledge estimation. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2404.12957v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Unlock the Power of Frozen LLMs in Knowledge Graph Completion
AU  - Xue, Bo
AU  - Xu, Yi
AU  - Song, Yunchong
AU  - Pang, Yiming
AU  - Ren, Yuyang
AU  - Ding, Jiaxin
AU  - Fu, Luoyi
AU  - Wang, Xinbing
PY  - 2024
PD  - 
N2  - Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, making them promising for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. Additionally, to reduce ambiguity and enrich knowledge representation, we generate detailed entity descriptions through subgraph sampling on KGs. Extensive experiments on standard benchmarks demonstrate the efficiency and effectiveness of our approach. We outperform traditional KGC methods across most datasets and, notably, achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by 188 and accelerating training and inference by 13.48.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2408.06787v2
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Weakly Supervised Detection of Hallucinations in LLM Activations
AU  - Rateike, Miriam
AU  - Cintas, Celia
AU  - Wamburu, John
AU  - Akumu, Tanya
AU  - Speakman, Skyler
PY  - 2023
PD  - 
N2  - We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2312.02798v1
NS  - 
N1  - 
ER  - 

TY  - JOUR
TI  - Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models
AU  - Yuan, Hongbang
AU  - Cao, Pengfei
AU  - Jin, Zhuoran
AU  - Chen, Yubo
AU  - Zeng, Daojian
AU  - Liu, Kang
AU  - Zhao, Jun
PY  - 2024
PD  - 
N2  - Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose ????? (?alse premise ?ttention head constra?ining for mi?igating ?allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.
JO  - arXiv
PB  - 
CY  - 
VL  - 
IS  - 
PG  - 
SP  - 
EP  - 
AN  - 
DO  - 
UR  - http://arxiv.org/abs/2402.19103v1
NS  - 
N1  - 
ER  - 

